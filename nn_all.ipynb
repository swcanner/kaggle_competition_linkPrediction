{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/iupui-link-prediction/train_edges.csv\n",
      "/kaggle/input/iupui-link-prediction/features.csv\n",
      "/kaggle/input/iupui-link-prediction/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, LeakyReLU, ReLU\n",
    "from keras.utils import np_utils,  to_categorical\n",
    "from keras import optimizers\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"/kaggle/input/iupui-link-prediction/train_edges.csv\";\n",
    "sample_sub = \"/kaggle/input/iupui-link-prediction/sample_submission.csv\"\n",
    "feat_file = \"/kaggle/input/iupui-link-prediction/features.csv\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_index(G):\n",
    "    \n",
    "    s = list(G.nodes);\n",
    "    num_nodes = max(s)+1;\n",
    "    \n",
    "    #Store it into a matrix for self preservation\n",
    "    #num_nodes = G.number_of_nodes();\n",
    "    common_neigh = np.zeros((num_nodes,num_nodes));\n",
    "    salton = common_neigh.copy();\n",
    "    jaccard = common_neigh.copy();\n",
    "    sorensen = common_neigh.copy();\n",
    "    hub_promoted = common_neigh.copy();\n",
    "    hub_depressed = common_neigh.copy();\n",
    "    leicht = common_neigh.copy();\n",
    "    pref = common_neigh.copy();\n",
    "    adamic = common_neigh.copy();\n",
    "    resource = common_neigh.copy();\n",
    "    \n",
    "    for k in s:\n",
    "        i = s.index(k);\n",
    "        if (i % 100 == 0):\n",
    "            print(i)\n",
    "        for l in s:\n",
    "            j = s.index(l);\n",
    "            if (i != j):\n",
    "                deg_i = G.degree[i];\n",
    "                deg_j = G.degree[j];\n",
    "                min_deg = deg_i;\n",
    "                max_deg = deg_j;\n",
    "                if (deg_j < min_deg):\n",
    "                    min_deg = deg_j;\n",
    "                    max_deg = deg_i;\n",
    "\n",
    "                com = sorted(nx.common_neighbors(G,i,j));\n",
    "                num_com = len(com);\n",
    "                common_neigh[i,j] = num_com;\n",
    "                preff = deg_i * deg_j;\n",
    "                \n",
    "                ada = 0;\n",
    "                res = 0;\n",
    "                \n",
    "                for k in com:\n",
    "                    ada += 1 / math.log(G.degree[k]);\n",
    "                    res += 1 / (G.degree[k]);\n",
    "                    \n",
    "                pref[i,j] = preff;\n",
    "                adamic[i,j] = ada;\n",
    "                resource[i,j] = res;\n",
    "\n",
    "                if (num_com > 0):\n",
    "                    salt = num_com / math.sqrt(deg_i * deg_j);\n",
    "                    jac = num_com / (deg_i + deg_j - num_com);\n",
    "                    sor = 2 * num_com / (deg_i + deg_j);\n",
    "                    hub_d = num_com / min_deg;\n",
    "                    hub_p = num_com / max_deg;\n",
    "                    lec = num_com / (deg_i * deg_j);\n",
    "\n",
    "                    salton[i,j] = salt;\n",
    "                    jaccard[i,j] = jac;\n",
    "                    sorensen[i,j] = sor;\n",
    "                    hub_promoted[i,j] = hub_p;\n",
    "                    hub_depressed[i,j] = hub_d;\n",
    "                    leicht[i,j] = lec;\n",
    "                \n",
    "    \n",
    "    return(common_neigh,salton,jaccard,sorensen,hub_promoted,hub_depressed,leicht,pref,adamic,resource);\n",
    "\n",
    "def calc_index_nodes(k,l,s,G):\n",
    "    \n",
    "    i = s.index(k);\n",
    "    j = s.index(l);\n",
    "    \n",
    "    deg_i = G.degree[i];\n",
    "    deg_j = G.degree[j];\n",
    "    min_deg = deg_i;\n",
    "    max_deg = deg_j;\n",
    "    if (deg_j < min_deg):\n",
    "        min_deg = deg_j;\n",
    "        max_deg = deg_i;\n",
    "\n",
    "    com = sorted(nx.common_neighbors(G,i,j));\n",
    "    num_com = len(com);\n",
    "    pref = deg_i * deg_j;\n",
    "\n",
    "    ada = 0;\n",
    "    res = 0;\n",
    "\n",
    "    for k in com:\n",
    "        ada += 1 / math.log(G.degree[k]);\n",
    "        res += 1 / (G.degree[k]);\n",
    "\n",
    "    salt = 0;\n",
    "    jac = 0;\n",
    "    sor = 0;\n",
    "    hub_d = 0;\n",
    "    hub_p = 0;\n",
    "    lec = 0;\n",
    "        \n",
    "    if (num_com > 0):\n",
    "        salt = num_com / math.sqrt(deg_i * deg_j);\n",
    "        jac = num_com / (deg_i + deg_j - num_com);\n",
    "        sor = 2 * num_com / (deg_i + deg_j);\n",
    "        hub_d = num_com / min_deg;\n",
    "        hub_p = num_com / max_deg;\n",
    "        lec = num_com / (deg_i * deg_j);                \n",
    "    \n",
    "    return(num_com,salt,jac,sor,hub_p,hub_d,lec,pref,ada,res);\n",
    "\n",
    "def calc_common_neigh(G):\n",
    "    #Store it into a matrix for self preservation\n",
    "    num_nodes = G.number_of_nodes();\n",
    "    common_neigh = np.zeros((num_nodes,num_nodes));\n",
    "    \n",
    "    for i in range(num_nodes):\n",
    "        if (i % 100 == 0):\n",
    "            print(i)\n",
    "        for j in range(num_nodes):\n",
    "            if (i != j):\n",
    "                num_com = len(sorted(nx.common_neighbors(G,i,j)));\n",
    "                common_neigh[i,j] = num_com\n",
    "    \n",
    "    return(common_neigh);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#Load the input files\n",
    "import csv\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "with open(train_file) as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in spamreader:\n",
    "        if row[1]=='1':\n",
    "            edge= row[0].split('-')\n",
    "            #print(int(edge[0]),int(edge[1]))\n",
    "            G.add_edge(int(edge[0]),int(edge[1]))\n",
    "            \n",
    "#s = sorted(G.nodes);\n",
    "s = G.nodes\n",
    "s = list(s)\n",
    "\n",
    "adj = nx.adjacency_matrix(G);\n",
    "print(adj[146,1356])\n",
    "print(adj[s.index(146),s.index(1356)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2708\n"
     ]
    }
   ],
   "source": [
    "# next let's read in the possible edges we need to classify from the test file\n",
    "#FUCK THE WAY MOHLER DID THIS\n",
    "#It assumes all nodes are connected so fuck\n",
    "#But we will let it stand...\n",
    "\n",
    "Gsub = nx.Graph()\n",
    "\n",
    "with open(sample_sub) as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in spamreader:\n",
    "        if row[1]=='1' or row[1]=='0':\n",
    "            edge= row[0].split('-')\n",
    "            #print(edge[0],edge[1]);\n",
    "            Gsub.add_edge(int(edge[0]),int(edge[1]))\n",
    "            \n",
    "G.add_nodes_from(Gsub.nodes)\n",
    "print(G.number_of_nodes());\n",
    "s = list(G.nodes);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select only the training data\n",
    "train_df = pd.read_csv(train_file);\n",
    "train_df = train_df.values;\n",
    "sz = np.shape(train_df);\n",
    "train_data = np.zeros((sz[0],3));\n",
    "\n",
    "for i in range(sz[0]):\n",
    "    edge = train_df[i,0];\n",
    "    edge = edge.split('-');\n",
    "    train_data[i,0] = int(edge[0])\n",
    "    train_data[i,1] = int(edge[1]);\n",
    "    train_data[i,2] = int(train_df[i,1]);\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 462    0    0 ...    0    0    0]\n",
      " [1911    0    0 ...    0    0    0]\n",
      " [2002    0    0 ...    0    0    0]\n",
      " ...\n",
      " [2372    0    0 ...    0    0    0]\n",
      " [ 955    0    0 ...    0    0    0]\n",
      " [ 376    0    0 ...    0    0    0]]\n",
      "(2708, 1434)\n",
      "17\n",
      "482\n",
      "22\n",
      "1928\n"
     ]
    }
   ],
   "source": [
    "#Load the feature file\n",
    "feat = pd.read_csv(feat_file,header=None);\n",
    "feat = feat.values;\n",
    "print(feat)\n",
    "print(feat.shape)\n",
    "\n",
    "#feat = feat.sort(axis=0);\n",
    "#print(feat)\n",
    "\n",
    "def feat_find(x,feat):\n",
    "    y = feat[feat[:,0]==x][0][1:1433];\n",
    "    return y;\n",
    "\n",
    "y = feat_find(1911,feat)\n",
    "print(np.sum(y))\n",
    "print(np.sum(feat[0,:]))\n",
    "y = feat_find(2002,feat)\n",
    "print(np.sum(y))\n",
    "print(np.sum(feat[1,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24 18 18 ... 14 23  2]\n"
     ]
    }
   ],
   "source": [
    "#Run K-means on the raw data\n",
    "km_k_1 = 25;\n",
    "km_k_2 = 10;\n",
    "feat_data = feat[:,1:1433];\n",
    "feat_ind = feat[:,0];\n",
    "feat_ind = feat_ind.tolist()\n",
    "\n",
    "km_1 = KMeans(n_clusters=km_k_1, random_state=0).fit(feat_data)\n",
    "km_1_lab = km_1.labels_;\n",
    "km_2 = KMeans(n_clusters=km_k_2, random_state=0).fit(feat_data)\n",
    "km_2_lab = km_2.labels_;\n",
    "\n",
    "print(km_1.labels_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28.61206924 27.26813732 25.87107944 25.15121366 23.54645545 21.39462921\n",
      " 19.90420152 19.31085509 18.89314444 18.3319261  18.04349181 17.80966185\n",
      " 17.01715495 16.85405204 16.71814399]\n",
      "(2708, 15)\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components = 15);\n",
    "feat_pca = pca.fit_transform(feat_data)\n",
    "print(pca.singular_values_)\n",
    "print(np.shape(feat_pca))\n",
    "#print(feat_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57.98869193 28.06106211 26.38354815 25.24221479 23.83808358 22.14033389\n",
      " 21.06337249 19.41407071 18.98746452 18.35584903 18.10183269 17.88867761\n",
      " 17.6055499  17.10510647 16.93320787 16.80631596 16.6921565  16.30243008\n",
      " 15.89373125 15.82896717 15.76646632 15.55468424 15.40369103 15.28760085\n",
      " 15.16219174 15.04766382 14.89578657 14.71406277 14.53643533 14.48211861\n",
      " 14.41763954 14.21069345 14.09894907 14.01399209 13.96777078 13.8923858\n",
      " 13.79994404 13.71466344 13.64534233 13.54503971 13.52705845 13.4570214\n",
      " 13.32382803 13.21392131 13.17450335 13.14832187 13.09845302 13.00476802\n",
      " 12.90182898 12.81273372 12.77422095 12.70430809 12.59631492 12.56330314\n",
      " 12.52468224 12.4473607  12.41800849 12.36871239 12.21073195 12.15752586\n",
      " 12.13269288 12.06438148 12.02922357 11.93615278 11.85921974 11.8480262\n",
      " 11.81961144 11.77329098 11.69113592 11.64075408 11.61774024 11.60468142\n",
      " 11.49157561 11.43416323 11.38265364 11.35332112 11.29960403 11.27701037\n",
      " 11.20746452 11.1285104  11.11567122 11.08802154 11.04544539 10.94741722\n",
      " 10.93051004]\n",
      "(2708, 85)\n"
     ]
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components = 85, n_iter=20, random_state=42);\n",
    "feat_svd = svd.fit_transform(feat_data)\n",
    "print(svd.singular_values_)\n",
    "print(np.shape(feat_svd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7388\n",
      "7388\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "(14776, 294)\n",
      "[[ 0.          0.          0.         ...  0.27500408 -0.50795508\n",
      "   0.        ]\n",
      " [ 2.          0.10166571  0.01538462 ... -0.30014728  0.23763707\n",
      "   1.        ]\n",
      " [ 5.          0.30429031  0.14705882 ...  0.27261364 -0.10893412\n",
      "   1.        ]\n",
      " ...\n",
      " [ 0.          0.          0.         ... -0.02024884 -0.05162364\n",
      "   1.        ]\n",
      " [ 0.          0.          0.         ...  0.10292158  0.07433107\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.20397512 -0.45180778\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Now I need to make this into something that I can feed into a neural net like nothing\n",
    "num_nodes = max(s)+1;\n",
    "adj = nx.adjacency_matrix(G);\n",
    "adj = adj.todense();\n",
    "\n",
    "sz_train = np.shape(train_df);\n",
    "print(sz_train[0])\n",
    "sz_pca = np.shape(feat_pca);\n",
    "sz_svd = np.shape(feat_svd);\n",
    "datum = np.zeros((sz_train[0],24+km_k_1*2+km_k_2*2+sz_pca[1]*2+sz_svd[1]*2));\n",
    "datum_1 = datum.copy();\n",
    "ind = 0;\n",
    "\n",
    "print(sz_train[0])\n",
    "\n",
    "eps = .0001;\n",
    "\n",
    "for ind in range(sz_train[0]):\n",
    "    \n",
    "    if (ind % 1000 == 0):\n",
    "        print(ind)\n",
    "\n",
    "    k = train_data[ind,0];\n",
    "    l = train_data[ind,1];\n",
    "    i = s.index(k);\n",
    "    j = s.index(l);\n",
    "    \n",
    "    com, salt, jac, sor, hub_p, hub_d, lec, pref, res, ada = calc_index_nodes(k,l,s,G);\n",
    "    \n",
    "    datum[ind,0] = com;\n",
    "    datum[ind,1] = salt;\n",
    "    datum[ind,2] = jac;\n",
    "    datum[ind,3] = sor;\n",
    "    datum[ind,4] = hub_p;\n",
    "    datum[ind,5] = hub_d;\n",
    "    datum[ind,6] = lec;\n",
    "    datum[ind,7] = pref;\n",
    "    datum[ind,8] = res;\n",
    "    datum[ind,9] = ada;\n",
    "\n",
    "    deg_i = G.degree[i];\n",
    "    deg_j = G.degree[j];\n",
    "    min_deg = deg_i;\n",
    "    max_deg = deg_j;\n",
    "    if (min_deg > max_deg):\n",
    "        max_deg = deg_j;\n",
    "        min_deg = deg_i;\n",
    "    datum[ind,10] = min_deg;\n",
    "    datum[ind,11] = max_deg;\n",
    "\n",
    "    #print(ind,k,l)\n",
    "    y1 = feat_find(k,feat);\n",
    "    y2 = feat_find(l,feat);\n",
    "\n",
    "    dp = np.dot(y1,y2);\n",
    "    n1 = np.linalg.norm(y1);\n",
    "    n2 = np.linalg.norm(y2);\n",
    "    dp_norm = dp / (n1*n2);\n",
    "\n",
    "    c_ang = math.acos(dp_norm);\n",
    "\n",
    "    datum[ind,12] = dp;\n",
    "    datum[ind,13] = dp_norm;\n",
    "    datum[ind,14] = c_ang;\n",
    "\n",
    "    #Covariance stuff\n",
    "    #Use the cov and inv cov\n",
    "\n",
    "    #Covariance of the features\n",
    "    c_cov = np.cov(y1,y2);\n",
    "    new_cov = c_cov.copy();\n",
    "\n",
    "    for i in range(np.shape(new_cov)[0]):\n",
    "        new_cov[i,i] += eps\n",
    "\n",
    "    new_cov = np.linalg.inv(new_cov);\n",
    "\n",
    "    c_cov = c_cov.flatten();\n",
    "    new_cov = new_cov.flatten();\n",
    "\n",
    "    datum[ind,15:19] = c_cov;\n",
    "    datum[ind,19:23] = new_cov;\n",
    "    \n",
    "    #Kmeans\n",
    "    i_f_ind = feat_ind.index(k);\n",
    "    j_f_ind = feat_ind.index(l);\n",
    "    \n",
    "    k_c1 = km_1_lab[i_f_ind];\n",
    "    k_c2 = km_1_lab[j_f_ind];\n",
    "    \n",
    "    \n",
    "    datum_1[ind,:23] = datum[ind,:23];\n",
    "    \n",
    "    datum[ind,23+k_c1] = 1;\n",
    "    datum[ind,23+km_k_1+k_c2] = 1;\n",
    "    datum_1[ind,23+k_c2] = 1;\n",
    "    datum_1[ind,23+km_k_1+k_c1] = 1;\n",
    "    \n",
    "    \n",
    "    k_c1 = km_2_lab[i_f_ind];\n",
    "    k_c2 = km_2_lab[j_f_ind];\n",
    "    datum[ind,23+km_k_1*2+k_c1] = 1;\n",
    "    datum[ind,23+km_k_1*2+km_k_2+k_c2] = 1;\n",
    "    datum_1[ind,23+k_c2] = 1;\n",
    "    datum_1[ind,23+km_k_1*2+km_k_2+k_c1] = 1;\n",
    "\n",
    "    #PCA\n",
    "    p_c1 = feat_pca[i_f_ind,:];\n",
    "    p_c2 = feat_pca[j_f_ind,:];\n",
    "    new_st = 23+km_k_1*2+km_k_2*2;\n",
    "    \n",
    "    datum[ind,new_st:new_st+sz_pca[1]] = p_c1;\n",
    "    datum[ind,new_st+sz_pca[1]:new_st+sz_pca[1]*2] = p_c2;\n",
    "    datum_1[ind,new_st:new_st+sz_pca[1]] = p_c2;\n",
    "    datum_1[ind,new_st+sz_pca[1]:new_st+sz_pca[1]*2] = p_c1;\n",
    "    \n",
    "    #SVD\n",
    "    s_c1 = feat_svd[i_f_ind,:];\n",
    "    s_c2 = feat_svd[j_f_ind,:];\n",
    "    #print(s_c1)\n",
    "    new_st = 23+km_k_1*2+km_k_2*2+sz_pca[1]*2;\n",
    "    #print(new_st,new_st+sz_svd[1])\n",
    "    \n",
    "    datum[ind,new_st:new_st+sz_svd[1]] = s_c1;\n",
    "    datum[ind,new_st+sz_svd[1]:new_st+sz_svd[1]*2] = s_c2;\n",
    "    datum_1[ind,new_st:new_st+sz_svd[1]] = s_c2;\n",
    "    datum_1[ind,new_st+sz_svd[1]:new_st+sz_svd[1]*2] = s_c1;\n",
    "    \n",
    "    \n",
    "    datum[ind,-1] = train_data[ind,2];\n",
    "    datum_1[ind,-1] = train_data[ind,2];\n",
    "        \n",
    "\n",
    "#Combine our datums\n",
    "datum = np.concatenate((datum,datum_1),axis=0);    \n",
    "    \n",
    "print(np.shape(datum))\n",
    "print(datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29552, 294)\n",
      "[[ 0.          0.          0.         ...  0.27500408 -0.50795508\n",
      "   0.        ]\n",
      " [ 2.          0.10166571  0.01538462 ... -0.30014728  0.23763707\n",
      "   1.        ]\n",
      " [ 5.          0.30429031  0.14705882 ...  0.27261364 -0.10893412\n",
      "   1.        ]\n",
      " ...\n",
      " [ 0.          0.          0.         ... -0.01971337 -0.04987134\n",
      "   1.        ]\n",
      " [ 0.          0.          0.         ...  0.10569838  0.07462603\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.20288791 -0.46614556\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "noisey_datum = datum.copy();\n",
    "sz_dat = np.shape(datum);\n",
    "num_entry = sz_dat[0] * (sz_dat[1]-1);\n",
    "noisey = np.random.normal(1,.03,num_entry);\n",
    "noisey = noisey.reshape(-1,sz_dat[1]-1);\n",
    "noisey_datum[:,:-1] *= noisey;\n",
    "#print(noisey_datum)\n",
    "datum = np.concatenate((datum,noisey_datum),axis=0); \n",
    "print(np.shape(datum));\n",
    "print(datum);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "[[ 0.          0.          0.         ...  0.2661687  -0.01524116\n",
      "   0.02393302]\n",
      " [ 0.          0.          0.         ...  0.21524577  0.00919083\n",
      "  -0.03208832]\n",
      " [ 0.          0.          0.         ... -0.02950152 -0.02547247\n",
      "  -0.12677891]\n",
      " ...\n",
      " [ 0.          0.          0.         ... -0.01306529  0.01784848\n",
      "  -0.04614251]\n",
      " [ 0.          0.          0.         ... -0.09300129 -0.00743396\n",
      "   0.32274806]\n",
      " [ 0.          0.          0.         ...  0.0078797   0.17753173\n",
      "   0.08620801]]\n"
     ]
    }
   ],
   "source": [
    "#Get test data\n",
    "test_df = pd.read_csv(sample_sub);\n",
    "test_df = test_df.values;\n",
    "test_sz = np.shape(test_df);\n",
    "test_inp = np.zeros((test_sz[0],3));\n",
    "\n",
    "for i in range(test_sz[0]):\n",
    "    edge = test_df[i,0];\n",
    "    edge = edge.split('-');\n",
    "    test_inp[i,0] = int(edge[0])\n",
    "    test_inp[i,1] = int(edge[1]);\n",
    "    test_inp[i,2] = int(test_inp[i,1]);\n",
    "    \n",
    "#print(train_data)\n",
    "\n",
    "num_nodes = max(s)+1;\n",
    "\n",
    "test_datum = np.zeros((test_sz[0],23+km_k_1*2+km_k_2*2+sz_pca[1]*2+sz_svd[1]*2));\n",
    "test_datum_1 = test_datum.copy();\n",
    "ind = 0;\n",
    "\n",
    "for ind in range(test_sz[0]):\n",
    "    \n",
    "    if (ind % 1000 == 0):\n",
    "        print(ind)\n",
    "\n",
    "    k = int(test_inp[ind,0]);\n",
    "    l = int(test_inp[ind,1]);\n",
    "    i = s.index(k);\n",
    "    j = s.index(l);\n",
    "    \n",
    "    com, salt, jac, sor, hub_p, hub_d, lec, pref, res, ada = calc_index_nodes(k,l,s,G);\n",
    "    \n",
    "    test_datum[ind,0] = com;\n",
    "    test_datum[ind,1] = salt;\n",
    "    test_datum[ind,2] = jac;\n",
    "    test_datum[ind,3] = sor;\n",
    "    test_datum[ind,4] = hub_p;\n",
    "    test_datum[ind,5] = hub_d;\n",
    "    test_datum[ind,6] = lec;\n",
    "    test_datum[ind,7] = pref;\n",
    "    test_datum[ind,8] = res;\n",
    "    test_datum[ind,9] = ada;\n",
    "\n",
    "    deg_i = G.degree[i];\n",
    "    deg_j = G.degree[j];\n",
    "    min_deg = deg_i;\n",
    "    max_deg = deg_j;\n",
    "    if (min_deg > max_deg):\n",
    "        max_deg = deg_j;\n",
    "        min_deg = deg_i;\n",
    "    \n",
    "    test_datum[ind,10] = min_deg;\n",
    "    test_datum[ind,11] = max_deg;\n",
    "    \n",
    "    y1 = feat_find(k,feat);\n",
    "    y2 = feat_find(l,feat);\n",
    "\n",
    "    dp = np.dot(y1,y2);\n",
    "    n1 = np.linalg.norm(y1);\n",
    "    n2 = np.linalg.norm(y2);\n",
    "    dp_norm = dp / (n1*n2);\n",
    "\n",
    "    c_ang = math.acos(dp_norm);\n",
    "\n",
    "    test_datum[ind,12] = dp;\n",
    "    test_datum[ind,13] = dp_norm;\n",
    "    test_datum[ind,14] = c_ang;\n",
    "    \n",
    "    #Covariance of the features\n",
    "    c_cov = np.cov(y1,y2);\n",
    "    new_cov = c_cov.copy();\n",
    "\n",
    "    for i in range(np.shape(new_cov)[0]):\n",
    "        new_cov[i,i] += eps\n",
    "\n",
    "    new_cov = np.linalg.inv(new_cov);\n",
    "\n",
    "    c_cov = c_cov.flatten();\n",
    "    new_cov = new_cov.flatten();\n",
    "\n",
    "    test_datum[ind,15:19] = c_cov;\n",
    "    test_datum[ind,19:23] = new_cov;\n",
    "    \n",
    "    \n",
    "    #Kmeans\n",
    "    i_f_ind = feat_ind.index(k);\n",
    "    j_f_ind = feat_ind.index(l);\n",
    "    \n",
    "    k_c1 = km_1_lab[i_f_ind];\n",
    "    k_c2 = km_1_lab[j_f_ind];\n",
    "    \n",
    "    test_datum_1[ind,:23] = test_datum[ind,:23];\n",
    "    \n",
    "    test_datum[ind,23+k_c1] = 1;\n",
    "    test_datum[ind,23+km_k_1+k_c2] = 1;\n",
    "    \n",
    "    test_datum_1[ind,23+k_c2] = 1;\n",
    "    test_datum_1[ind,23+km_k_1+k_c1] = 1;\n",
    "    \n",
    "    k_c1 = km_2_lab[i_f_ind];\n",
    "    k_c2 = km_2_lab[j_f_ind];\n",
    "    \n",
    "    test_datum[ind,23+km_k_1*2+k_c1] = 1;\n",
    "    test_datum[ind,23+km_k_1*2+km_k_2+k_c2] = 1;\n",
    "    \n",
    "    test_datum_1[ind,23+k_c2] = 1;\n",
    "    test_datum_1[ind,23+km_k_1*2+km_k_2+k_c1] = 1;\n",
    "    \n",
    "    #PCA\n",
    "    p_c1 = feat_pca[i_f_ind,:];\n",
    "    p_c2 = feat_pca[j_f_ind,:];\n",
    "    new_st = 23+km_k_1*2+km_k_2*2;\n",
    "    \n",
    "    test_datum[ind,new_st:new_st+sz_pca[1]] = p_c1;\n",
    "    test_datum[ind,new_st+sz_pca[1]:new_st+sz_pca[1]*2] = p_c2;\n",
    "    test_datum_1[ind,new_st:new_st+sz_pca[1]] = p_c2;\n",
    "    test_datum_1[ind,new_st+sz_pca[1]:new_st+sz_pca[1]*2] = p_c1;\n",
    "    \n",
    "    #SVD\n",
    "    s_c1 = feat_svd[i_f_ind,:];\n",
    "    s_c2 = feat_svd[j_f_ind,:];\n",
    "    #print(s_c1)\n",
    "    new_st = 23+km_k_1*2+km_k_2*2+sz_pca[1]*2;\n",
    "    #print(new_st,new_st+sz_svd[1])\n",
    "    \n",
    "    test_datum[ind,new_st:new_st+sz_svd[1]] = s_c1;\n",
    "    test_datum[ind,new_st+sz_svd[1]:new_st+sz_svd[1]*2] = s_c2;\n",
    "    test_datum_1[ind,new_st:new_st+sz_svd[1]] = s_c2;\n",
    "    test_datum_1[ind,new_st+sz_svd[1]:new_st+sz_svd[1]*2] = s_c1;\n",
    "\n",
    "\n",
    "print(test_datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(datum);\n",
    "train_df.to_csv(\"train.csv\",header=None,index=None)\n",
    "test_df = pd.DataFrame(test_datum);\n",
    "test_df.to_csv(\"test.csv\",header=None,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization\n",
    "max_col = np.amax(datum,axis=0);\n",
    "datum /= max_col;\n",
    "test_datum /= max_col[:-1];\n",
    "test_datum_1 /= max_col[:-1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I now have the training and testing data\n",
    "#Separate the training data into train and test\n",
    "Y = datum[:,-1];\n",
    "X = datum[:,:-1];\n",
    "\n",
    "sz_X = np.shape(X);\n",
    "\n",
    "#\"\"\"\n",
    "X = X.reshape(-1,sz_X[1],1);\n",
    "test_datum = test_datum.reshape(-1,sz_X[1],1);\n",
    "test_datum_1 = test_datum_1.reshape(-1,sz_X[1],1);\n",
    "#\"\"\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size = .25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 293, 512)          1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 293, 512)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 293, 512)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 150016)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               38404352  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 38,466,209\n",
      "Trainable params: 38,466,209\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#\"\"\"\n",
    "#create convolution neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(sz_X[1],1)))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.33))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.33))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.33))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.33))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.33))\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.33))\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary();\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22164 samples, validate on 7388 samples\n",
      "Epoch 1/150\n",
      "22164/22164 [==============================] - 18s 792us/step - loss: 0.1908 - accuracy: 0.7237 - val_loss: 0.1741 - val_accuracy: 0.7474\n",
      "Epoch 2/150\n",
      "22164/22164 [==============================] - 16s 706us/step - loss: 0.1723 - accuracy: 0.7554 - val_loss: 0.1555 - val_accuracy: 0.7814\n",
      "Epoch 3/150\n",
      "22164/22164 [==============================] - 16s 720us/step - loss: 0.1571 - accuracy: 0.7801 - val_loss: 0.1488 - val_accuracy: 0.7952\n",
      "Epoch 4/150\n",
      "22164/22164 [==============================] - 16s 713us/step - loss: 0.1423 - accuracy: 0.8016 - val_loss: 0.1406 - val_accuracy: 0.8131\n",
      "Epoch 5/150\n",
      "22164/22164 [==============================] - 16s 700us/step - loss: 0.1307 - accuracy: 0.8207 - val_loss: 0.1345 - val_accuracy: 0.8228\n",
      "Epoch 6/150\n",
      "22164/22164 [==============================] - 16s 711us/step - loss: 0.1208 - accuracy: 0.8373 - val_loss: 0.1270 - val_accuracy: 0.8265\n",
      "Epoch 7/150\n",
      "22164/22164 [==============================] - 16s 716us/step - loss: 0.1097 - accuracy: 0.8523 - val_loss: 0.1198 - val_accuracy: 0.8376\n",
      "Epoch 8/150\n",
      "22164/22164 [==============================] - 16s 717us/step - loss: 0.1000 - accuracy: 0.8673 - val_loss: 0.1144 - val_accuracy: 0.8487\n",
      "Epoch 9/150\n",
      "22164/22164 [==============================] - 16s 701us/step - loss: 0.0929 - accuracy: 0.8767 - val_loss: 0.1071 - val_accuracy: 0.8648\n",
      "Epoch 10/150\n",
      "22164/22164 [==============================] - 16s 703us/step - loss: 0.0843 - accuracy: 0.8917 - val_loss: 0.1021 - val_accuracy: 0.8657\n",
      "Epoch 11/150\n",
      "22164/22164 [==============================] - 16s 721us/step - loss: 0.0804 - accuracy: 0.8972 - val_loss: 0.0994 - val_accuracy: 0.8711\n",
      "Epoch 12/150\n",
      "22164/22164 [==============================] - 16s 707us/step - loss: 0.0757 - accuracy: 0.9039 - val_loss: 0.0950 - val_accuracy: 0.8757\n",
      "Epoch 13/150\n",
      "22164/22164 [==============================] - 16s 701us/step - loss: 0.0703 - accuracy: 0.9088 - val_loss: 0.0929 - val_accuracy: 0.8818\n",
      "Epoch 14/150\n",
      "22164/22164 [==============================] - 16s 706us/step - loss: 0.0660 - accuracy: 0.9171 - val_loss: 0.0900 - val_accuracy: 0.8864\n",
      "Epoch 15/150\n",
      "22164/22164 [==============================] - 16s 723us/step - loss: 0.0637 - accuracy: 0.9206 - val_loss: 0.0850 - val_accuracy: 0.8946\n",
      "Epoch 16/150\n",
      "22164/22164 [==============================] - 16s 704us/step - loss: 0.0624 - accuracy: 0.9216 - val_loss: 0.0870 - val_accuracy: 0.8933\n",
      "Epoch 17/150\n",
      "22164/22164 [==============================] - 16s 699us/step - loss: 0.0574 - accuracy: 0.9294 - val_loss: 0.0852 - val_accuracy: 0.8939\n",
      "Epoch 18/150\n",
      "22164/22164 [==============================] - 16s 705us/step - loss: 0.0562 - accuracy: 0.9288 - val_loss: 0.0773 - val_accuracy: 0.9061\n",
      "Epoch 19/150\n",
      "22164/22164 [==============================] - 16s 727us/step - loss: 0.0520 - accuracy: 0.9356 - val_loss: 0.0816 - val_accuracy: 0.9006\n",
      "Epoch 20/150\n",
      "22164/22164 [==============================] - 16s 715us/step - loss: 0.0213 - accuracy: 0.9751 - val_loss: 0.0556 - val_accuracy: 0.9404\n",
      "Epoch 92/150\n",
      "22164/22164 [==============================] - 16s 705us/step - loss: 0.0191 - accuracy: 0.9782 - val_loss: 0.0525 - val_accuracy: 0.9419\n",
      "Epoch 93/150\n",
      "22164/22164 [==============================] - 16s 701us/step - loss: 0.0208 - accuracy: 0.9758 - val_loss: 0.0562 - val_accuracy: 0.9388\n",
      "Epoch 94/150\n",
      "22164/22164 [==============================] - 15s 692us/step - loss: 0.0215 - accuracy: 0.9753 - val_loss: 0.0567 - val_accuracy: 0.9385\n",
      "Epoch 95/150\n",
      "22164/22164 [==============================] - 16s 713us/step - loss: 0.0224 - accuracy: 0.9745 - val_loss: 0.0548 - val_accuracy: 0.9396\n",
      "Epoch 96/150\n",
      "22164/22164 [==============================] - 16s 708us/step - loss: 0.0219 - accuracy: 0.9734 - val_loss: 0.0557 - val_accuracy: 0.9371\n",
      "Epoch 97/150\n",
      "22164/22164 [==============================] - 15s 698us/step - loss: 0.0201 - accuracy: 0.9769 - val_loss: 0.0555 - val_accuracy: 0.9375\n",
      "Epoch 98/150\n",
      "22164/22164 [==============================] - 15s 694us/step - loss: 0.0218 - accuracy: 0.9748 - val_loss: 0.0584 - val_accuracy: 0.9372\n",
      "Epoch 99/150\n",
      "22164/22164 [==============================] - 16s 709us/step - loss: 0.0212 - accuracy: 0.9753 - val_loss: 0.0546 - val_accuracy: 0.9398\n",
      "Epoch 100/150\n",
      "22164/22164 [==============================] - 16s 716us/step - loss: 0.0207 - accuracy: 0.9757 - val_loss: 0.0546 - val_accuracy: 0.9392\n",
      "Epoch 101/150\n",
      "22164/22164 [==============================] - 15s 696us/step - loss: 0.0190 - accuracy: 0.9778 - val_loss: 0.0565 - val_accuracy: 0.9391\n",
      "Epoch 102/150\n",
      "22164/22164 [==============================] - 15s 699us/step - loss: 0.0225 - accuracy: 0.9742 - val_loss: 0.0578 - val_accuracy: 0.9387\n",
      "Epoch 103/150\n",
      "22164/22164 [==============================] - 16s 710us/step - loss: 0.0209 - accuracy: 0.9755 - val_loss: 0.0596 - val_accuracy: 0.9362\n",
      "Epoch 104/150\n",
      "22164/22164 [==============================] - 15s 698us/step - loss: 0.0191 - accuracy: 0.9784 - val_loss: 0.0538 - val_accuracy: 0.9417\n",
      "Epoch 105/150\n",
      "22164/22164 [==============================] - 15s 696us/step - loss: 0.0196 - accuracy: 0.9774 - val_loss: 0.0580 - val_accuracy: 0.9373\n",
      "Epoch 106/150\n",
      "22164/22164 [==============================] - 15s 697us/step - loss: 0.0209 - accuracy: 0.9767 - val_loss: 0.0561 - val_accuracy: 0.9398\n",
      "Epoch 107/150\n",
      "22164/22164 [==============================] - 16s 718us/step - loss: 0.0231 - accuracy: 0.9735 - val_loss: 0.0590 - val_accuracy: 0.9375\n",
      "Epoch 108/150\n",
      "22164/22164 [==============================] - 15s 696us/step - loss: 0.0214 - accuracy: 0.9749 - val_loss: 0.0544 - val_accuracy: 0.9414\n",
      "Epoch 109/150\n",
      "22164/22164 [==============================] - 15s 692us/step - loss: 0.0225 - accuracy: 0.9741 - val_loss: 0.0645 - val_accuracy: 0.9306\n",
      "Epoch 110/150\n",
      "22164/22164 [==============================] - 15s 698us/step - loss: 0.0216 - accuracy: 0.9749 - val_loss: 0.0546 - val_accuracy: 0.9402\n",
      "Epoch 111/150\n",
      "22164/22164 [==============================] - 16s 715us/step - loss: 0.0206 - accuracy: 0.9760 - val_loss: 0.0531 - val_accuracy: 0.9403\n",
      "Epoch 112/150\n",
      "22164/22164 [==============================] - 15s 698us/step - loss: 0.0207 - accuracy: 0.9751 - val_loss: 0.0530 - val_accuracy: 0.9409\n",
      "Epoch 113/150\n",
      "22164/22164 [==============================] - 15s 691us/step - loss: 0.0200 - accuracy: 0.9769 - val_loss: 0.0531 - val_accuracy: 0.9427\n",
      "Epoch 114/150\n",
      "22164/22164 [==============================] - 15s 696us/step - loss: 0.0207 - accuracy: 0.9765 - val_loss: 0.0534 - val_accuracy: 0.9411\n",
      "Epoch 115/150\n",
      "22164/22164 [==============================] - 16s 715us/step - loss: 0.0189 - accuracy: 0.9783 - val_loss: 0.0541 - val_accuracy: 0.9409\n",
      "Epoch 116/150\n",
      "22164/22164 [==============================] - 15s 697us/step - loss: 0.0185 - accuracy: 0.9791 - val_loss: 0.0542 - val_accuracy: 0.9425\n",
      "Epoch 117/150\n",
      "22164/22164 [==============================] - 15s 697us/step - loss: 0.0199 - accuracy: 0.9768 - val_loss: 0.0573 - val_accuracy: 0.9350\n",
      "Epoch 118/150\n",
      "22164/22164 [==============================] - 16s 707us/step - loss: 0.0201 - accuracy: 0.9764 - val_loss: 0.0532 - val_accuracy: 0.9425\n",
      "Epoch 119/150\n",
      "22164/22164 [==============================] - 16s 714us/step - loss: 0.0193 - accuracy: 0.9772 - val_loss: 0.0585 - val_accuracy: 0.9352\n",
      "Epoch 120/150\n",
      "22164/22164 [==============================] - 16s 702us/step - loss: 0.0207 - accuracy: 0.9762 - val_loss: 0.0553 - val_accuracy: 0.9396\n",
      "Epoch 121/150\n",
      "22164/22164 [==============================] - 15s 696us/step - loss: 0.0183 - accuracy: 0.9789 - val_loss: 0.0551 - val_accuracy: 0.9404\n",
      "Epoch 122/150\n",
      "22164/22164 [==============================] - 16s 710us/step - loss: 0.0202 - accuracy: 0.9763 - val_loss: 0.0546 - val_accuracy: 0.9391\n",
      "Epoch 123/150\n",
      "22164/22164 [==============================] - 16s 704us/step - loss: 0.0201 - accuracy: 0.9774 - val_loss: 0.0556 - val_accuracy: 0.9390\n",
      "Epoch 124/150\n",
      "22164/22164 [==============================] - 15s 695us/step - loss: 0.0222 - accuracy: 0.9746 - val_loss: 0.0564 - val_accuracy: 0.9390\n",
      "Epoch 125/150\n",
      "22164/22164 [==============================] - 15s 695us/step - loss: 0.0204 - accuracy: 0.9763 - val_loss: 0.0529 - val_accuracy: 0.9419\n",
      "Epoch 126/150\n",
      "22164/22164 [==============================] - 16s 710us/step - loss: 0.0183 - accuracy: 0.9797 - val_loss: 0.0535 - val_accuracy: 0.9430\n",
      "Epoch 127/150\n",
      " 8850/22164 [==========>...................] - ETA: 8s - loss: 0.0249 - accuracy: 0.9714"
     ]
    }
   ],
   "source": [
    "#\"\"\"\n",
    "#Compile model\n",
    "sgd = optimizers.SGD(lr=.01);\n",
    "model.compile(loss='MSE', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train,y_train, shuffle=True,\n",
    "          batch_size=25,epochs=150,verbose=1,\n",
    "          validation_data=(X_test,y_test))\n",
    "\n",
    "Y_out_0 = model.predict(test_datum)\n",
    "Y_out_1 = model.predict(test_datum_1);\n",
    "\n",
    "Y_out = ( Y_out_0 + Y_out_1 ) / 2;\n",
    "\n",
    "print(Y_out)\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.        ]\n",
      " [0.99996734]\n",
      " [0.        ]\n",
      " ...\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.        ]]\n"
     ]
    }
   ],
   "source": [
    "Y_out_0 = model.predict(test_datum)\n",
    "Y_out_1 = model.predict(test_datum_1);\n",
    "\n",
    "Y_out = ( Y_out_0 + Y_out_1 ) / 2;\n",
    "\n",
    "print(Y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmax_num = 0;\\nmax_acc = 0;\\n#Create and train the random forest classifier\\nfor i in [20,30,40,50,60,70,80]:\\n#for i in [5]:\\n    rf_model = RandomForestClassifier(max_depth=i,n_estimators=2500);\\n    rf_model.fit(X_train,y_train);\\n    Y_pred = rf_model.predict(X_test);\\n    a = accuracy_score(y_test,Y_pred);\\n    print(i,a);\\n    if (a > max_acc):\\n        max_acc = a;\\n        max_num = i;\\n\\n#We choose XXXX RF since best accuracy\\n#print(np.mean(Y_pred))\\n#print(np.mean(y_test))\\n#We choose max_depth of 5 as our model\\nrf_model = RandomForestClassifier(max_depth=95,n_estimators=10000);\\nrf_model.fit(X_train,y_train);\\nY_pred = rf_model.predict(X_test);\\na = accuracy_score(y_test,Y_pred);\\nprint(i,a);\\n\\nY_out_0 = rf_model.predict_proba(test_datum)\\nY_out_1 = rf_model.predict_proba(test_datum_1);\\n\\nY_out = ( Y_out_0 + Y_out_1 ) / 2;\\nY_out = Y_out[:,1]\\n\\nprint(Y_out)\\nprint(np.mean(Y_out))\\n#'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "max_num = 0;\n",
    "max_acc = 0;\n",
    "#Create and train the random forest classifier\n",
    "for i in [20,30,40,50,60,70,80]:\n",
    "#for i in [5]:\n",
    "    rf_model = RandomForestClassifier(max_depth=i,n_estimators=2500);\n",
    "    rf_model.fit(X_train,y_train);\n",
    "    Y_pred = rf_model.predict(X_test);\n",
    "    a = accuracy_score(y_test,Y_pred);\n",
    "    print(i,a);\n",
    "    if (a > max_acc):\n",
    "        max_acc = a;\n",
    "        max_num = i;\n",
    "\n",
    "#We choose XXXX RF since best accuracy\n",
    "#print(np.mean(Y_pred))\n",
    "#print(np.mean(y_test))\n",
    "#We choose max_depth of 5 as our model\n",
    "rf_model = RandomForestClassifier(max_depth=95,n_estimators=10000);\n",
    "rf_model.fit(X_train,y_train);\n",
    "Y_pred = rf_model.predict(X_test);\n",
    "a = accuracy_score(y_test,Y_pred);\n",
    "print(i,a);\n",
    "\n",
    "Y_out_0 = rf_model.predict_proba(test_datum)\n",
    "Y_out_1 = rf_model.predict_proba(test_datum_1);\n",
    "\n",
    "Y_out = ( Y_out_0 + Y_out_1 ) / 2;\n",
    "Y_out = Y_out[:,1]\n",
    "\n",
    "print(Y_out)\n",
    "print(np.mean(Y_out))\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#Create an SVM\\n\\n#Create a simple one\\nlin_svc = svm.SVC(kernel='linear');\\nlin_svc.fit(X_train,y_train);\\nY_lin = lin_svc.predict(X_test);\\nprint(accuracy_score(Y_lin,y_test))\\n\\n#RBF Kernel\\nrbf_svc = svm.SVC(kernel='rbf');\\nrbf_svc.fit(X_train,y_train);\\nY_rbf = rbf_svc.predict(X_test);\\nprint(accuracy_score(Y_rbf,y_test))\\n\\nY_lin_1 = lin_svc.predict_proba(test_datum);\\nY_lin_2 = lin_svc.predict_proba(test_datum_1);\\nY_out = (Y_lin_1 + Y_lin_2) / 2;\\n#Y_rbf = rbf_svc.predict(test_datum);\\n#Y_out = Y_lin.copy();\\n#\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#Create an SVM\n",
    "\n",
    "#Create a simple one\n",
    "lin_svc = svm.SVC(kernel='linear');\n",
    "lin_svc.fit(X_train,y_train);\n",
    "Y_lin = lin_svc.predict(X_test);\n",
    "print(accuracy_score(Y_lin,y_test))\n",
    "\n",
    "#RBF Kernel\n",
    "rbf_svc = svm.SVC(kernel='rbf');\n",
    "rbf_svc.fit(X_train,y_train);\n",
    "Y_rbf = rbf_svc.predict(X_test);\n",
    "print(accuracy_score(Y_rbf,y_test))\n",
    "\n",
    "Y_lin_1 = lin_svc.predict_proba(test_datum);\n",
    "Y_lin_2 = lin_svc.predict_proba(test_datum_1);\n",
    "Y_out = (Y_lin_1 + Y_lin_2) / 2;\n",
    "#Y_rbf = rbf_svc.predict(test_datum);\n",
    "#Y_out = Y_lin.copy();\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output outputs for stacking\n",
    "y0_df = pd.DataFrame(Y_out);\n",
    "y1_df = pd.DataFrame(Y_out_1);\n",
    "y0_df.to_csv(\"Y0.csv\",header=None,index=None);\n",
    "y1_df.to_csv(\"Y1.csv\",header=None,index=None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "# here is a simple algorithm to get you started...for each possible edge i-j in the test set\n",
    "# we will count the number of nodes in the training graph they have in common...if they have\n",
    "# one or more \"mutual friends\" then we will connect them (this will score about .67 acc on kaggle)\n",
    "\n",
    "\n",
    "cutoff = .485;\n",
    "\n",
    "alfa = 0;\n",
    "with open('/kaggle/working/sub.csv', 'w') as csvfile:\n",
    "    fieldnames = ['edge', 'label']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    with open(sample_sub) as csvfile2:\n",
    "        reader = csv.reader(csvfile2, delimiter=',')\n",
    "        for row in reader:\n",
    "            if row[1]=='1' or row[1]=='0':\n",
    "                edge= row[0].split('-')\n",
    "\n",
    "                i=int(edge[0])\n",
    "                j=int(edge[1])\n",
    "                \n",
    "                # here networkx has a common_neighbors function\n",
    "                num_com_neigh=len(sorted(nx.common_neighbors(G, i, j)))\n",
    "                y=0\n",
    "                #if num_com_neigh>0:\n",
    "                #    y=1\n",
    "\n",
    "                edg_out=str(i)+\"-\"+str(j) \n",
    "                if (Y_out[alfa] > cutoff):\n",
    "                    y = 1\n",
    "                alfa += 1;\n",
    "                writer.writerow({'edge': edg_out, 'label': y})\n",
    "                #print(edg_out, num_com_neigh, y)\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.mean(Y_out > .45))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

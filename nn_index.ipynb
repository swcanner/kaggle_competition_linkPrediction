{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/iupui-link-prediction/features.csv\n",
      "/kaggle/input/iupui-link-prediction/sample_submission.csv\n",
      "/kaggle/input/iupui-link-prediction/train_edges.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, LeakyReLU, ReLU\n",
    "from keras.utils import np_utils,  to_categorical\n",
    "from keras import optimizers\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"/kaggle/input/iupui-link-prediction/train_edges.csv\";\n",
    "sample_sub = \"/kaggle/input/iupui-link-prediction/sample_submission.csv\"\n",
    "test_file = \"/kaggle/input/iupui-link-prediction/features.csv\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_index(G):\n",
    "    \n",
    "    s = list(G.nodes);\n",
    "    num_nodes = max(s)+1;\n",
    "    \n",
    "    #Store it into a matrix for self preservation\n",
    "    #num_nodes = G.number_of_nodes();\n",
    "    common_neigh = np.zeros((num_nodes,num_nodes));\n",
    "    salton = common_neigh.copy();\n",
    "    jaccard = common_neigh.copy();\n",
    "    sorensen = common_neigh.copy();\n",
    "    hub_promoted = common_neigh.copy();\n",
    "    hub_depressed = common_neigh.copy();\n",
    "    leicht = common_neigh.copy();\n",
    "    pref = common_neigh.copy();\n",
    "    adamic = common_neigh.copy();\n",
    "    resource = common_neigh.copy();\n",
    "    \n",
    "    for k in s:\n",
    "        i = s.index(k);\n",
    "        if (i % 100 == 0):\n",
    "            print(i)\n",
    "        for l in s:\n",
    "            j = s.index(l);\n",
    "            if (i != j):\n",
    "                deg_i = G.degree[i];\n",
    "                deg_j = G.degree[j];\n",
    "                min_deg = deg_i;\n",
    "                max_deg = deg_j;\n",
    "                if (deg_j < min_deg):\n",
    "                    min_deg = deg_j;\n",
    "                    max_deg = deg_i;\n",
    "\n",
    "                com = sorted(nx.common_neighbors(G,i,j));\n",
    "                num_com = len(com);\n",
    "                common_neigh[i,j] = num_com;\n",
    "                preff = deg_i * deg_j;\n",
    "                \n",
    "                ada = 0;\n",
    "                res = 0;\n",
    "                \n",
    "                for k in com:\n",
    "                    ada += 1 / math.log(G.degree[k]);\n",
    "                    res += 1 / (G.degree[k]);\n",
    "                    \n",
    "                pref[i,j] = preff;\n",
    "                adamic[i,j] = ada;\n",
    "                resource[i,j] = res;\n",
    "\n",
    "                if (num_com > 0):\n",
    "                    salt = num_com / math.sqrt(deg_i * deg_j);\n",
    "                    jac = num_com / (deg_i + deg_j - num_com);\n",
    "                    sor = 2 * num_com / (deg_i + deg_j);\n",
    "                    hub_d = num_com / min_deg;\n",
    "                    hub_p = num_com / max_deg;\n",
    "                    lec = num_com / (deg_i * deg_j);\n",
    "\n",
    "                    salton[i,j] = salt;\n",
    "                    jaccard[i,j] = jac;\n",
    "                    sorensen[i,j] = sor;\n",
    "                    hub_promoted[i,j] = hub_p;\n",
    "                    hub_depressed[i,j] = hub_d;\n",
    "                    leicht[i,j] = lec;\n",
    "                \n",
    "    \n",
    "    return(common_neigh,salton,jaccard,sorensen,hub_promoted,hub_depressed,leicht,pref,adamic,resource);\n",
    "\n",
    "def calc_common_neigh(G):\n",
    "    #Store it into a matrix for self preservation\n",
    "    num_nodes = G.number_of_nodes();\n",
    "    common_neigh = np.zeros((num_nodes,num_nodes));\n",
    "    \n",
    "    for i in range(num_nodes):\n",
    "        if (i % 100 == 0):\n",
    "            print(i)\n",
    "        for j in range(num_nodes):\n",
    "            if (i != j):\n",
    "                num_com = len(sorted(nx.common_neighbors(G,i,j)));\n",
    "                common_neigh[i,j] = num_com\n",
    "    \n",
    "    return(common_neigh);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#Load the input files\n",
    "import csv\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "with open(train_file) as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in spamreader:\n",
    "        if row[1]=='1':\n",
    "            edge= row[0].split('-')\n",
    "            #print(int(edge[0]),int(edge[1]))\n",
    "            G.add_edge(int(edge[0]),int(edge[1]))\n",
    "            \n",
    "#s = sorted(G.nodes);\n",
    "s = G.nodes\n",
    "s = list(s)\n",
    "\n",
    "adj = nx.adjacency_matrix(G);\n",
    "print(adj[146,1356])\n",
    "print(adj[s.index(146),s.index(1356)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(s)\n",
    "#print(sorted(s))\n",
    "#print(type(s))\n",
    "#print(list(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2506\n"
     ]
    }
   ],
   "source": [
    "#Node info\n",
    "print(G.number_of_nodes());\n",
    "#nx.draw(G,node_size = 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2708\n"
     ]
    }
   ],
   "source": [
    "# next let's read in the possible edges we need to classify from the test file\n",
    "#FUCK THE WAY MOHLER DID THIS\n",
    "#It assumes all nodes are connected so fuck\n",
    "#But we will let it stand...\n",
    "\n",
    "Gsub = nx.Graph()\n",
    "\n",
    "with open(sample_sub) as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in spamreader:\n",
    "        if row[1]=='1' or row[1]=='0':\n",
    "            edge= row[0].split('-')\n",
    "            #print(edge[0],edge[1]);\n",
    "            Gsub.add_edge(int(edge[0]),int(edge[1]))\n",
    "            \n",
    "G.add_nodes_from(Gsub.nodes)\n",
    "print(G.number_of_nodes());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sorted(G.nodes))\n",
    "s = list(G.nodes);\n",
    "#print(s)\n",
    "#a = 0;\n",
    "#for i in range(len(s)-1):\n",
    "#    if(s[i+1] - s[i] != 1):\n",
    "#        a += 1;\n",
    "#print(a)\n",
    "#print(G.degree[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n"
     ]
    }
   ],
   "source": [
    "#n = calc_common_neigh(G)\n",
    "com,salt,jac,sor,hub_p,hub_d,lec,pref,ada,res = calc_index(G);\n",
    "\n",
    "\n",
    "#print(com,salt,jac,sor,hub_p,hub_d);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select only the training data\n",
    "train_df = pd.read_csv(train_file);\n",
    "train_df = train_df.values;\n",
    "sz = np.shape(train_df);\n",
    "train_data = np.zeros((sz[0],3));\n",
    "\n",
    "for i in range(sz[0]):\n",
    "    edge = train_df[i,0];\n",
    "    edge = edge.split('-');\n",
    "    train_data[i,0] = int(edge[0])\n",
    "    train_data[i,1] = int(edge[1]);\n",
    "    train_data[i,2] = int(train_df[i,1]);\n",
    "    \n",
    "#print(train_data)\n",
    "\n",
    "#Create a matrix of the data\n",
    "data_exist = np.zeros((len(s),len(s)));\n",
    "for i in range(sz[0]):\n",
    "    \n",
    "    j = int(train_data[i,0]);\n",
    "    k = int(train_data[i,1]);\n",
    "    j = s.index(j);\n",
    "    k = s.index(k);\n",
    "    \n",
    "    data_exist[j,k] = 1;\n",
    "    data_exist[k,j] = 1;\n",
    "\n",
    "def datum_in(i,j,df):\n",
    "    sz = np.shape(df);\n",
    "    for k in range(sz[0]):\n",
    "        if (df[k,0] == i):\n",
    "            if (df[k,1] == j):\n",
    "                return True;\n",
    "        if (df[k,0] == j):\n",
    "            if (df[k,1] == i):\n",
    "                return True;\n",
    "        \n",
    "    return False;\n",
    "\n",
    "def datum_in_mat(i,j,s,dat):\n",
    "    \n",
    "    if (data_exist[i,j] != 0):\n",
    "        return True;\n",
    "    else:\n",
    "        return False;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "(7359, 13)\n",
      "[[2.00000000e+00 1.01665714e-01 1.53846154e-02 ... 1.29000000e+02\n",
      "  3.00000000e+00 1.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 1.29000000e+02\n",
      "  2.20000000e+01 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 1.29000000e+02\n",
      "  2.00000000e+00 1.00000000e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 3.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 1.00000000e+00\n",
      "  3.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "#Now I need to make this into something that I can feed into a neural net like nothing\n",
    "num_nodes = max(s)+1;\n",
    "adj = nx.adjacency_matrix(G);\n",
    "adj = adj.todense();\n",
    "\n",
    "sz_train = np.shape(train_df);\n",
    "datum = np.zeros((sz_train[0],13));\n",
    "ind = 0;\n",
    "\n",
    "for k in s:\n",
    "    i = s.index(k);\n",
    "    if (i % 100 == 0):\n",
    "            print(i)\n",
    "    for l in s:\n",
    "        j = s.index(l);\n",
    "        if (j > i):\n",
    "            if (datum_in_mat(i,j,s,data_exist)):\n",
    "\n",
    "                datum[ind,0] = com[i,j];\n",
    "                datum[ind,1] = salt[i,j];\n",
    "                datum[ind,2] = jac[i,j];\n",
    "                datum[ind,3] = sor[i,j];\n",
    "                datum[ind,4] = hub_p[i,j];\n",
    "                datum[ind,5] = hub_d[i,j];\n",
    "                datum[ind,6] = lec[i,j];\n",
    "                datum[ind,7] = pref[i,j];\n",
    "                datum[ind,8] = res[i,j];\n",
    "                datum[ind,9] = ada[i,j];\n",
    "\n",
    "                deg_i = G.degree[i];\n",
    "                deg_j = G.degree[j];\n",
    "                min_deg = deg_i;\n",
    "                max_deg = deg_j;\n",
    "                if (min_deg > max_deg):\n",
    "                    max_deg = deg_j;\n",
    "                    min_deg = deg_i;\n",
    "                datum[ind,10] = min_deg;\n",
    "                datum[ind,11] = max_deg;\n",
    "\n",
    "                #ind_i = s.index(i);\n",
    "                #ind_j = s.index(j);\n",
    "\n",
    "                datum[ind,12] = adj[i,j];\n",
    "                ind += 1;\n",
    "        \n",
    "#remove the all zero rows\n",
    "datum = datum[~np.all(datum == 0, axis=1)];\n",
    "#Append an all zero row\n",
    "oof = np.zeros((1,13));\n",
    "datum = np.append(datum,oof);\n",
    "datum = datum.reshape(-1,13);\n",
    "print(np.shape(datum))\n",
    "print(datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#print(adj)\n",
    "\n",
    "\n",
    "print(adj[2250,985])\n",
    "print(adj[s.index(146),s.index(1356)])\n",
    "print(adj[s.index(1356),s.index(146)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 3. 3.]\n",
      " [0. 0. 0. ... 0. 4. 2.]\n",
      " [0. 0. 0. ... 0. 2. 3.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Get test data\n",
    "test_df = pd.read_csv(sample_sub);\n",
    "test_df = test_df.values;\n",
    "test_sz = np.shape(test_df);\n",
    "test_inp = np.zeros((test_sz[0],3));\n",
    "\n",
    "for i in range(test_sz[0]):\n",
    "    edge = test_df[i,0];\n",
    "    edge = edge.split('-');\n",
    "    test_inp[i,0] = int(edge[0])\n",
    "    test_inp[i,1] = int(edge[1]);\n",
    "    test_inp[i,2] = int(test_inp[i,1]);\n",
    "    \n",
    "#print(train_data)\n",
    "\n",
    "num_nodes = max(s)+1;\n",
    "\n",
    "sz_test = np.shape(train_df);\n",
    "test_datum = np.zeros((sz_test[0],12));\n",
    "ind = 0;\n",
    "\n",
    "for ind in range(test_sz[0]):\n",
    "\n",
    "    i = int(test_inp[ind,0]);\n",
    "    j = int(test_inp[ind,1]);\n",
    "    i = s.index(i);\n",
    "    j = s.index(j);\n",
    "    \n",
    "    test_datum[ind,0] = com[i,j];\n",
    "    test_datum[ind,1] = salt[i,j];\n",
    "    test_datum[ind,2] = jac[i,j];\n",
    "    test_datum[ind,3] = sor[i,j];\n",
    "    test_datum[ind,4] = hub_p[i,j];\n",
    "    test_datum[ind,5] = hub_d[i,j];\n",
    "    test_datum[ind,6] = lec[i,j];\n",
    "    test_datum[ind,7] = pref[i,j];\n",
    "    test_datum[ind,8] = res[i,j];\n",
    "    test_datum[ind,9] = ada[i,j];\n",
    "\n",
    "    deg_i = G.degree[i];\n",
    "    deg_j = G.degree[j];\n",
    "    min_deg = deg_i;\n",
    "    max_deg = deg_j;\n",
    "    if (min_deg > max_deg):\n",
    "        max_deg = deg_j;\n",
    "        min_deg = deg_i;\n",
    "    \n",
    "    test_datum[ind,10] = min_deg;\n",
    "    test_datum[ind,11] = max_deg;\n",
    "\n",
    "print(test_datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization\n",
    "max_col = np.amax(datum,axis=0);\n",
    "datum /= max_col;\n",
    "test_datum /= max_col[:-1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I now have the training and testing data\n",
    "#Separate the training data into train and test\n",
    "Y = datum[:,-1];\n",
    "X = datum[:,:-1];\n",
    "\n",
    "X = X.reshape(-1,12,1);\n",
    "test_datum = test_datum.reshape(-1,12,1);\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size = .2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Create and train the random forest classifier\\nfor i in [3,4,5,6,7,8]:\\n    rf_model = RandomForestClassifier(max_depth=i,n_estimators=2500);\\n    rf_model.fit(X_train,y_train);\\n    Y_pred = rf_model.predict(X_test);\\n    a = accuracy_score(y_test,Y_pred);\\n    print(i,a);\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#Create and train the random forest classifier\n",
    "for i in [3,4,5,6,7,8]:\n",
    "    rf_model = RandomForestClassifier(max_depth=i,n_estimators=2500);\n",
    "    rf_model.fit(X_train,y_train);\n",
    "    Y_pred = rf_model.predict(X_test);\n",
    "    a = accuracy_score(y_test,Y_pred);\n",
    "    print(i,a);\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 12, 512)           1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 12, 512)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 512)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6144)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               1573120   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,618,465\n",
      "Trainable params: 1,618,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create convolution neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(12,1)))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.33))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.33))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.33))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.33))\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.33))\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile model\n",
    "#sgd = optimizers.SGD(lr=.01);\n",
    "model.compile(loss='MSE', \n",
    "              optimizer='adagrad', \n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5887 samples, validate on 1472 samples\n",
      "Epoch 1/250\n",
      "5887/5887 [==============================] - 8s 1ms/step - loss: 0.2502 - accuracy: 0.5081 - val_loss: 0.2473 - val_accuracy: 0.5020\n",
      "Epoch 2/250\n",
      "5887/5887 [==============================] - 6s 987us/step - loss: 0.2489 - accuracy: 0.5290 - val_loss: 0.2454 - val_accuracy: 0.5503\n",
      "Epoch 3/250\n",
      "5887/5887 [==============================] - 6s 995us/step - loss: 0.2471 - accuracy: 0.5373 - val_loss: 0.2454 - val_accuracy: 0.5408\n",
      "Epoch 4/250\n",
      "5887/5887 [==============================] - 6s 1ms/step - loss: 0.2467 - accuracy: 0.5427 - val_loss: 0.2453 - val_accuracy: 0.5435\n",
      "Epoch 5/250\n",
      "5887/5887 [==============================] - 6s 1ms/step - loss: 0.2467 - accuracy: 0.5471 - val_loss: 0.2445 - val_accuracy: 0.5503\n",
      "Epoch 6/250\n",
      "5887/5887 [==============================] - 6s 986us/step - loss: 0.2460 - accuracy: 0.5466 - val_loss: 0.2448 - val_accuracy: 0.5401\n",
      "Epoch 7/250\n",
      "5887/5887 [==============================] - 6s 1ms/step - loss: 0.2457 - accuracy: 0.5487 - val_loss: 0.2459 - val_accuracy: 0.5394\n",
      "Epoch 8/250\n",
      "5887/5887 [==============================] - 6s 981us/step - loss: 0.2459 - accuracy: 0.5475 - val_loss: 0.2435 - val_accuracy: 0.5618\n",
      "Epoch 9/250\n",
      "5887/5887 [==============================] - 7s 1ms/step - loss: 0.2442 - accuracy: 0.5531 - val_loss: 0.2439 - val_accuracy: 0.5489\n",
      "Epoch 10/250\n",
      "5887/5887 [==============================] - 6s 1ms/step - loss: 0.2451 - accuracy: 0.5519 - val_loss: 0.2429 - val_accuracy: 0.5679\n",
      "Epoch 11/250\n",
      "5887/5887 [==============================] - 6s 1ms/step - loss: 0.2441 - accuracy: 0.5595 - val_loss: 0.2428 - val_accuracy: 0.5605\n",
      "Epoch 12/250\n",
      "5887/5887 [==============================] - 6s 992us/step - loss: 0.2445 - accuracy: 0.5570 - val_loss: 0.2422 - val_accuracy: 0.5754\n",
      "Epoch 13/250\n",
      "5887/5887 [==============================] - 6s 1ms/step - loss: 0.2446 - accuracy: 0.5604 - val_loss: 0.2422 - val_accuracy: 0.5686\n",
      "Epoch 14/250\n",
      "5887/5887 [==============================] - 6s 1ms/step - loss: 0.2430 - accuracy: 0.5624 - val_loss: 0.2416 - val_accuracy: 0.5774\n",
      "Epoch 15/250\n",
      "5887/5887 [==============================] - 6s 980us/step - loss: 0.2440 - accuracy: 0.5577 - val_loss: 0.2424 - val_accuracy: 0.5686\n",
      "Epoch 16/250\n",
      "5887/5887 [==============================] - 6s 990us/step - loss: 0.2434 - accuracy: 0.5616 - val_loss: 0.2420 - val_accuracy: 0.5679\n",
      "Epoch 17/250\n",
      "5887/5887 [==============================] - 6s 966us/step - loss: 0.2439 - accuracy: 0.5602 - val_loss: 0.2415 - val_accuracy: 0.5768\n",
      "Epoch 18/250\n",
      "5887/5887 [==============================] - 6s 1ms/step - loss: 0.2436 - accuracy: 0.5636 - val_loss: 0.2412 - val_accuracy: 0.5788\n",
      "Epoch 19/250\n",
      "5887/5887 [==============================] - 6s 1ms/step - loss: 0.2433 - accuracy: 0.5691 - val_loss: 0.2410 - val_accuracy: 0.5788\n",
      "Epoch 20/250\n",
      "5887/5887 [==============================] - 6s 1ms/step - loss: 0.2438 - accuracy: 0.5651 - val_loss: 0.2417 - val_accuracy: 0.5734\n",
      "Epoch 21/250\n",
      "5887/5887 [==============================] - 6s 1ms/step - loss: 0.2436 - accuracy: 0.5617 - val_loss: 0.2413 - val_accuracy: 0.5747\n",
      "Epoch 22/250\n",
      "5887/5887 [==============================] - 6s 998us/step - loss: 0.2437 - accuracy: 0.5575 - val_loss: 0.2409 - val_accuracy: 0.5747\n",
      "Epoch 23/250\n",
      "5887/5887 [==============================] - 6s 958us/step - loss: 0.2433 - accuracy: 0.5624 - val_loss: 0.2409 - val_accuracy: 0.5781\n",
      "Epoch 24/250\n",
      "5887/5887 [==============================] - 6s 1ms/step - loss: 0.2447 - accuracy: 0.5670 - val_loss: 0.2412 - val_accuracy: 0.5774\n",
      "Epoch 25/250\n",
      "5887/5887 [==============================] - 6s 973us/step - loss: 0.2432 - accuracy: 0.5655 - val_loss: 0.2418 - val_accuracy: 0.5727\n",
      "Epoch 26/250\n",
      "5887/5887 [==============================] - 6s 974us/step - loss: 0.2431 - accuracy: 0.5658 - val_loss: 0.2411 - val_accuracy: 0.5822\n",
      "Epoch 27/250\n",
      "5887/5887 [==============================] - 6s 983us/step - loss: 0.2438 - accuracy: 0.5665 - val_loss: 0.2411 - val_accuracy: 0.5849\n",
      "Epoch 28/250\n",
      "5887/5887 [==============================] - 6s 996us/step - loss: 0.2429 - accuracy: 0.5699 - val_loss: 0.2408 - val_accuracy: 0.5795\n",
      "Epoch 29/250\n",
      "5887/5887 [==============================] - 6s 1ms/step - loss: 0.2428 - accuracy: 0.5699 - val_loss: 0.2409 - val_accuracy: 0.5761\n",
      "Epoch 30/250\n",
      "5887/5887 [==============================] - 7s 1ms/step - loss: 0.2433 - accuracy: 0.5665 - val_loss: 0.2407 - val_accuracy: 0.5774\n",
      "Epoch 31/250\n",
      "5887/5887 [==============================] - 6s 1ms/step - loss: 0.2429 - accuracy: 0.5650 - val_loss: 0.2404 - val_accuracy: 0.5754\n",
      "Epoch 32/250\n",
      "5887/5887 [==============================] - 6s 966us/step - loss: 0.2427 - accuracy: 0.5655 - val_loss: 0.2409 - val_accuracy: 0.5836\n",
      "Epoch 33/250\n",
      "5887/5887 [==============================] - 6s 976us/step - loss: 0.2433 - accuracy: 0.5668 - val_loss: 0.2406 - val_accuracy: 0.5829\n",
      "Epoch 34/250\n",
      "5887/5887 [==============================] - 6s 958us/step - loss: 0.2427 - accuracy: 0.5694 - val_loss: 0.2405 - val_accuracy: 0.5754\n",
      "Epoch 35/250\n",
      "5887/5887 [==============================] - 6s 995us/step - loss: 0.2436 - accuracy: 0.5680 - val_loss: 0.2405 - val_accuracy: 0.5740\n",
      "Epoch 36/250\n",
      "5887/5887 [==============================] - 6s 982us/step - loss: 0.2425 - accuracy: 0.5721 - val_loss: 0.2408 - val_accuracy: 0.5836\n",
      "Epoch 37/250\n",
      "5887/5887 [==============================] - 6s 967us/step - loss: 0.2425 - accuracy: 0.5657 - val_loss: 0.2408 - val_accuracy: 0.5720\n",
      "Epoch 38/250\n",
      "5887/5887 [==============================] - 6s 939us/step - loss: 0.2420 - accuracy: 0.5657 - val_loss: 0.2402 - val_accuracy: 0.5802\n",
      "Epoch 39/250\n",
      "5887/5887 [==============================] - 6s 969us/step - loss: 0.2430 - accuracy: 0.5629 - val_loss: 0.2413 - val_accuracy: 0.5707\n",
      "Epoch 40/250\n",
      "5887/5887 [==============================] - 6s 1ms/step - loss: 0.2431 - accuracy: 0.5655 - val_loss: 0.2408 - val_accuracy: 0.5768\n",
      "Epoch 41/250\n",
      "5887/5887 [==============================] - 6s 1ms/step - loss: 0.2427 - accuracy: 0.5691 - val_loss: 0.2404 - val_accuracy: 0.5815\n",
      "Epoch 42/250\n",
      "5887/5887 [==============================] - 6s 971us/step - loss: 0.2425 - accuracy: 0.5699 - val_loss: 0.2403 - val_accuracy: 0.5802\n",
      "Epoch 43/250\n",
      "5887/5887 [==============================] - 6s 941us/step - loss: 0.2427 - accuracy: 0.5723 - val_loss: 0.2401 - val_accuracy: 0.5734\n",
      "Epoch 44/250\n",
      "5887/5887 [==============================] - 6s 943us/step - loss: 0.2420 - accuracy: 0.5687 - val_loss: 0.2401 - val_accuracy: 0.5761\n",
      "Epoch 45/250\n",
      "5887/5887 [==============================] - 6s 948us/step - loss: 0.2431 - accuracy: 0.5643 - val_loss: 0.2404 - val_accuracy: 0.5761\n",
      "Epoch 46/250\n",
      "5887/5887 [==============================] - 6s 968us/step - loss: 0.2417 - accuracy: 0.5679 - val_loss: 0.2400 - val_accuracy: 0.5747\n",
      "Epoch 47/250\n",
      "5887/5887 [==============================] - 6s 949us/step - loss: 0.2422 - accuracy: 0.5682 - val_loss: 0.2400 - val_accuracy: 0.5829\n",
      "Epoch 48/250\n",
      "5887/5887 [==============================] - 6s 976us/step - loss: 0.2426 - accuracy: 0.5723 - val_loss: 0.2400 - val_accuracy: 0.5727\n",
      "Epoch 49/250\n",
      "5887/5887 [==============================] - 6s 937us/step - loss: 0.2421 - accuracy: 0.5714 - val_loss: 0.2400 - val_accuracy: 0.5761\n",
      "Epoch 66/250\n",
      "5887/5887 [==============================] - 6s 976us/step - loss: 0.2418 - accuracy: 0.5714 - val_loss: 0.2405 - val_accuracy: 0.5727\n",
      "Epoch 67/250\n",
      "5887/5887 [==============================] - 5s 924us/step - loss: 0.2415 - accuracy: 0.5713 - val_loss: 0.2399 - val_accuracy: 0.5815\n",
      "Epoch 68/250\n",
      "5887/5887 [==============================] - 6s 941us/step - loss: 0.2417 - accuracy: 0.5699 - val_loss: 0.2398 - val_accuracy: 0.5707\n",
      "Epoch 69/250\n",
      "5887/5887 [==============================] - 6s 940us/step - loss: 0.2419 - accuracy: 0.5721 - val_loss: 0.2398 - val_accuracy: 0.5802\n",
      "Epoch 70/250\n",
      "5887/5887 [==============================] - 5s 927us/step - loss: 0.2419 - accuracy: 0.5667 - val_loss: 0.2398 - val_accuracy: 0.5720\n",
      "Epoch 71/250\n",
      "5887/5887 [==============================] - 6s 1ms/step - loss: 0.2414 - accuracy: 0.5706 - val_loss: 0.2397 - val_accuracy: 0.5808\n",
      "Epoch 72/250\n",
      "5887/5887 [==============================] - 6s 1ms/step - loss: 0.2420 - accuracy: 0.5719 - val_loss: 0.2399 - val_accuracy: 0.5808\n",
      "Epoch 73/250\n",
      "5887/5887 [==============================] - 6s 972us/step - loss: 0.2416 - accuracy: 0.5684 - val_loss: 0.2399 - val_accuracy: 0.5808\n",
      "Epoch 74/250\n",
      "5887/5887 [==============================] - 5s 932us/step - loss: 0.2421 - accuracy: 0.5762 - val_loss: 0.2397 - val_accuracy: 0.5747\n",
      "Epoch 75/250\n",
      "5887/5887 [==============================] - 6s 945us/step - loss: 0.2418 - accuracy: 0.5721 - val_loss: 0.2399 - val_accuracy: 0.5829\n",
      "Epoch 76/250\n",
      "5887/5887 [==============================] - 5s 924us/step - loss: 0.2412 - accuracy: 0.5743 - val_loss: 0.2401 - val_accuracy: 0.5781\n",
      "Epoch 77/250\n",
      "5887/5887 [==============================] - 6s 950us/step - loss: 0.2420 - accuracy: 0.5711 - val_loss: 0.2399 - val_accuracy: 0.5740\n",
      "Epoch 78/250\n",
      "5887/5887 [==============================] - 6s 966us/step - loss: 0.2420 - accuracy: 0.5731 - val_loss: 0.2398 - val_accuracy: 0.5754\n",
      "Epoch 79/250\n",
      "5887/5887 [==============================] - 6s 984us/step - loss: 0.2413 - accuracy: 0.5736 - val_loss: 0.2398 - val_accuracy: 0.5842\n",
      "Epoch 80/250\n",
      "5887/5887 [==============================] - 5s 927us/step - loss: 0.2420 - accuracy: 0.5697 - val_loss: 0.2398 - val_accuracy: 0.5802\n",
      "Epoch 81/250\n",
      "5887/5887 [==============================] - 6s 966us/step - loss: 0.2411 - accuracy: 0.5723 - val_loss: 0.2399 - val_accuracy: 0.5795\n",
      "Epoch 82/250\n",
      "5887/5887 [==============================] - 6s 1ms/step - loss: 0.2419 - accuracy: 0.5657 - val_loss: 0.2401 - val_accuracy: 0.5788\n",
      "Epoch 83/250\n",
      "5887/5887 [==============================] - 6s 993us/step - loss: 0.2417 - accuracy: 0.5704 - val_loss: 0.2397 - val_accuracy: 0.5720\n",
      "Epoch 84/250\n",
      "5887/5887 [==============================] - 6s 968us/step - loss: 0.2411 - accuracy: 0.5670 - val_loss: 0.2397 - val_accuracy: 0.5747\n",
      "Epoch 85/250\n",
      "5887/5887 [==============================] - 6s 945us/step - loss: 0.2419 - accuracy: 0.5736 - val_loss: 0.2400 - val_accuracy: 0.5849\n",
      "Epoch 86/250\n",
      "5887/5887 [==============================] - 5s 922us/step - loss: 0.2414 - accuracy: 0.5680 - val_loss: 0.2397 - val_accuracy: 0.5761\n",
      "Epoch 87/250\n",
      "5887/5887 [==============================] - 6s 941us/step - loss: 0.2412 - accuracy: 0.5662 - val_loss: 0.2399 - val_accuracy: 0.5849\n",
      "Epoch 88/250\n",
      "5887/5887 [==============================] - 5s 918us/step - loss: 0.2411 - accuracy: 0.5741 - val_loss: 0.2398 - val_accuracy: 0.5761\n",
      "Epoch 89/250\n",
      "5887/5887 [==============================] - 6s 939us/step - loss: 0.2407 - accuracy: 0.5697 - val_loss: 0.2397 - val_accuracy: 0.5849\n",
      "Epoch 90/250\n",
      "5887/5887 [==============================] - 6s 934us/step - loss: 0.2418 - accuracy: 0.5657 - val_loss: 0.2400 - val_accuracy: 0.5808\n",
      "Epoch 91/250\n",
      "5887/5887 [==============================] - 6s 962us/step - loss: 0.2415 - accuracy: 0.5709 - val_loss: 0.2399 - val_accuracy: 0.5734\n",
      "Epoch 92/250\n",
      "5887/5887 [==============================] - 6s 943us/step - loss: 0.2419 - accuracy: 0.5650 - val_loss: 0.2399 - val_accuracy: 0.5727\n",
      "Epoch 93/250\n",
      "5887/5887 [==============================] - 6s 1ms/step - loss: 0.2415 - accuracy: 0.5735 - val_loss: 0.2399 - val_accuracy: 0.5842\n",
      "Epoch 94/250\n",
      "5887/5887 [==============================] - 5s 931us/step - loss: 0.2410 - accuracy: 0.5707 - val_loss: 0.2403 - val_accuracy: 0.5734\n",
      "Epoch 95/250\n",
      "5887/5887 [==============================] - 6s 949us/step - loss: 0.2419 - accuracy: 0.5633 - val_loss: 0.2400 - val_accuracy: 0.5795\n",
      "Epoch 96/250\n",
      "5887/5887 [==============================] - 6s 949us/step - loss: 0.2418 - accuracy: 0.5689 - val_loss: 0.2399 - val_accuracy: 0.5815\n",
      "Epoch 97/250\n",
      "5887/5887 [==============================] - 6s 973us/step - loss: 0.2419 - accuracy: 0.5696 - val_loss: 0.2402 - val_accuracy: 0.5781\n",
      "Epoch 98/250\n",
      "5887/5887 [==============================] - 5s 910us/step - loss: 0.2422 - accuracy: 0.5694 - val_loss: 0.2400 - val_accuracy: 0.5836\n",
      "Epoch 99/250\n",
      "5887/5887 [==============================] - 5s 934us/step - loss: 0.2414 - accuracy: 0.5709 - val_loss: 0.2399 - val_accuracy: 0.5740\n",
      "Epoch 100/250\n",
      "5887/5887 [==============================] - 5s 912us/step - loss: 0.2413 - accuracy: 0.5701 - val_loss: 0.2399 - val_accuracy: 0.5754\n",
      "Epoch 101/250\n",
      "5887/5887 [==============================] - 6s 936us/step - loss: 0.2417 - accuracy: 0.5738 - val_loss: 0.2401 - val_accuracy: 0.5781\n",
      "Epoch 102/250\n",
      "5887/5887 [==============================] - 6s 950us/step - loss: 0.2416 - accuracy: 0.5665 - val_loss: 0.2398 - val_accuracy: 0.5720\n",
      "Epoch 103/250\n",
      "5887/5887 [==============================] - 6s 1ms/step - loss: 0.2413 - accuracy: 0.5762 - val_loss: 0.2399 - val_accuracy: 0.5836\n",
      "Epoch 104/250\n",
      "5887/5887 [==============================] - 6s 997us/step - loss: 0.2411 - accuracy: 0.5699 - val_loss: 0.2398 - val_accuracy: 0.5754\n",
      "Epoch 105/250\n",
      "5887/5887 [==============================] - 6s 957us/step - loss: 0.2414 - accuracy: 0.5680 - val_loss: 0.2398 - val_accuracy: 0.5734\n",
      "Epoch 106/250\n",
      "5887/5887 [==============================] - 6s 956us/step - loss: 0.2411 - accuracy: 0.5674 - val_loss: 0.2399 - val_accuracy: 0.5754\n",
      "Epoch 107/250\n",
      "5887/5887 [==============================] - 6s 936us/step - loss: 0.2414 - accuracy: 0.5706 - val_loss: 0.2401 - val_accuracy: 0.5815\n",
      "Epoch 108/250\n",
      "5887/5887 [==============================] - 6s 976us/step - loss: 0.2413 - accuracy: 0.5733 - val_loss: 0.2398 - val_accuracy: 0.5754\n",
      "Epoch 109/250\n",
      "5887/5887 [==============================] - 5s 921us/step - loss: 0.2412 - accuracy: 0.5730 - val_loss: 0.2403 - val_accuracy: 0.5781\n",
      "Epoch 110/250\n",
      "5887/5887 [==============================] - 6s 946us/step - loss: 0.2410 - accuracy: 0.5653 - val_loss: 0.2401 - val_accuracy: 0.5842\n",
      "Epoch 111/250\n",
      "5887/5887 [==============================] - 6s 942us/step - loss: 0.2413 - accuracy: 0.5691 - val_loss: 0.2398 - val_accuracy: 0.5747\n",
      "Epoch 112/250\n",
      "5887/5887 [==============================] - 5s 925us/step - loss: 0.2412 - accuracy: 0.5775 - val_loss: 0.2403 - val_accuracy: 0.5822\n",
      "Epoch 113/250\n",
      "5887/5887 [==============================] - 6s 941us/step - loss: 0.2414 - accuracy: 0.5716 - val_loss: 0.2402 - val_accuracy: 0.5788\n",
      "Epoch 114/250\n",
      "5887/5887 [==============================] - 6s 1ms/step - loss: 0.2414 - accuracy: 0.5704 - val_loss: 0.2399 - val_accuracy: 0.5727\n",
      "Epoch 115/250\n",
      "5887/5887 [==============================] - 6s 963us/step - loss: 0.2418 - accuracy: 0.5680 - val_loss: 0.2397 - val_accuracy: 0.5754\n",
      "Epoch 116/250\n",
      "5887/5887 [==============================] - 6s 960us/step - loss: 0.2416 - accuracy: 0.5714 - val_loss: 0.2400 - val_accuracy: 0.5842\n",
      "Epoch 117/250\n",
      "5887/5887 [==============================] - 5s 918us/step - loss: 0.2418 - accuracy: 0.5657 - val_loss: 0.2401 - val_accuracy: 0.5849\n",
      "Epoch 118/250\n",
      "5887/5887 [==============================] - 6s 939us/step - loss: 0.2409 - accuracy: 0.5711 - val_loss: 0.2402 - val_accuracy: 0.5781\n",
      "Epoch 119/250\n",
      "5887/5887 [==============================] - 6s 942us/step - loss: 0.2414 - accuracy: 0.5730 - val_loss: 0.2400 - val_accuracy: 0.5836\n",
      "Epoch 120/250\n",
      "5887/5887 [==============================] - 6s 983us/step - loss: 0.2415 - accuracy: 0.5718 - val_loss: 0.2402 - val_accuracy: 0.5788\n",
      "Epoch 121/250\n",
      "5887/5887 [==============================] - 5s 928us/step - loss: 0.2414 - accuracy: 0.5702 - val_loss: 0.2399 - val_accuracy: 0.5713\n",
      "Epoch 122/250\n",
      "5887/5887 [==============================] - 6s 945us/step - loss: 0.2404 - accuracy: 0.5731 - val_loss: 0.2399 - val_accuracy: 0.5747\n",
      "Epoch 123/250\n",
      "5887/5887 [==============================] - 5s 913us/step - loss: 0.2414 - accuracy: 0.5677 - val_loss: 0.2403 - val_accuracy: 0.5788\n",
      "Epoch 124/250\n",
      "5887/5887 [==============================] - 6s 951us/step - loss: 0.2418 - accuracy: 0.5662 - val_loss: 0.2400 - val_accuracy: 0.5768\n",
      "Epoch 125/250\n",
      "5887/5887 [==============================] - 6s 1ms/step - loss: 0.2412 - accuracy: 0.5757 - val_loss: 0.2401 - val_accuracy: 0.5795\n",
      "Epoch 126/250\n",
      "5887/5887 [==============================] - 6s 1ms/step - loss: 0.2410 - accuracy: 0.5758 - val_loss: 0.2400 - val_accuracy: 0.5836\n",
      "Epoch 127/250\n",
      "5887/5887 [==============================] - 6s 935us/step - loss: 0.2417 - accuracy: 0.5752 - val_loss: 0.2398 - val_accuracy: 0.5713\n",
      "Epoch 128/250\n",
      "5887/5887 [==============================] - 6s 941us/step - loss: 0.2411 - accuracy: 0.5736 - val_loss: 0.2399 - val_accuracy: 0.5788\n",
      "Epoch 129/250\n",
      "5887/5887 [==============================] - 5s 924us/step - loss: 0.2408 - accuracy: 0.5711 - val_loss: 0.2398 - val_accuracy: 0.5747\n",
      "Epoch 130/250\n",
      "5887/5887 [==============================] - 6s 958us/step - loss: 0.2411 - accuracy: 0.5726 - val_loss: 0.2400 - val_accuracy: 0.5836\n",
      "Epoch 131/250\n",
      "5887/5887 [==============================] - 5s 929us/step - loss: 0.2410 - accuracy: 0.5646 - val_loss: 0.2401 - val_accuracy: 0.5842\n",
      "Epoch 132/250\n",
      "5887/5887 [==============================] - 6s 946us/step - loss: 0.2414 - accuracy: 0.5731 - val_loss: 0.2398 - val_accuracy: 0.5754\n",
      "Epoch 133/250\n",
      "5887/5887 [==============================] - 6s 952us/step - loss: 0.2416 - accuracy: 0.5677 - val_loss: 0.2398 - val_accuracy: 0.5720\n",
      "Epoch 134/250\n",
      "5887/5887 [==============================] - 6s 939us/step - loss: 0.2409 - accuracy: 0.5733 - val_loss: 0.2401 - val_accuracy: 0.5842\n",
      "Epoch 135/250\n",
      "5887/5887 [==============================] - 6s 1ms/step - loss: 0.2416 - accuracy: 0.5682 - val_loss: 0.2400 - val_accuracy: 0.5747\n",
      "Epoch 136/250\n",
      "5887/5887 [==============================] - 6s 1ms/step - loss: 0.2415 - accuracy: 0.5699 - val_loss: 0.2400 - val_accuracy: 0.5774\n",
      "Epoch 137/250\n",
      "5887/5887 [==============================] - 6s 1ms/step - loss: 0.2406 - accuracy: 0.5724 - val_loss: 0.2399 - val_accuracy: 0.5740\n",
      "Epoch 138/250\n",
      "5887/5887 [==============================] - 6s 1ms/step - loss: 0.2413 - accuracy: 0.5663 - val_loss: 0.2400 - val_accuracy: 0.5761\n",
      "Epoch 139/250\n",
      "5887/5887 [==============================] - 5s 911us/step - loss: 0.2413 - accuracy: 0.5660 - val_loss: 0.2399 - val_accuracy: 0.5754\n",
      "Epoch 140/250\n",
      "5887/5887 [==============================] - 6s 941us/step - loss: 0.2413 - accuracy: 0.5728 - val_loss: 0.2400 - val_accuracy: 0.5727\n",
      "Epoch 141/250\n",
      "5887/5887 [==============================] - 5s 917us/step - loss: 0.2409 - accuracy: 0.5721 - val_loss: 0.2401 - val_accuracy: 0.5842\n",
      "Epoch 142/250\n",
      "5887/5887 [==============================] - 5s 932us/step - loss: 0.2411 - accuracy: 0.5733 - val_loss: 0.2399 - val_accuracy: 0.5747\n",
      "Epoch 143/250\n",
      "5887/5887 [==============================] - 5s 917us/step - loss: 0.2413 - accuracy: 0.5724 - val_loss: 0.2399 - val_accuracy: 0.5754\n",
      "Epoch 144/250\n",
      "5887/5887 [==============================] - 6s 979us/step - loss: 0.2412 - accuracy: 0.5677 - val_loss: 0.2400 - val_accuracy: 0.5727\n",
      "Epoch 145/250\n",
      "5887/5887 [==============================] - 5s 926us/step - loss: 0.2408 - accuracy: 0.5694 - val_loss: 0.2400 - val_accuracy: 0.5761\n",
      "Epoch 146/250\n",
      "5887/5887 [==============================] - 6s 1ms/step - loss: 0.2408 - accuracy: 0.5723 - val_loss: 0.2399 - val_accuracy: 0.5754\n",
      "Epoch 147/250\n",
      "5887/5887 [==============================] - 6s 999us/step - loss: 0.2415 - accuracy: 0.5682 - val_loss: 0.2400 - val_accuracy: 0.5836\n",
      "Epoch 148/250\n",
      "5887/5887 [==============================] - 6s 969us/step - loss: 0.2409 - accuracy: 0.5726 - val_loss: 0.2398 - val_accuracy: 0.5754\n",
      "Epoch 149/250\n",
      "5887/5887 [==============================] - 6s 941us/step - loss: 0.2409 - accuracy: 0.5707 - val_loss: 0.2400 - val_accuracy: 0.5768\n",
      "Epoch 150/250\n",
      "5887/5887 [==============================] - 6s 970us/step - loss: 0.2413 - accuracy: 0.5650 - val_loss: 0.2400 - val_accuracy: 0.5768\n",
      "Epoch 151/250\n",
      "5887/5887 [==============================] - 5s 918us/step - loss: 0.2407 - accuracy: 0.5692 - val_loss: 0.2400 - val_accuracy: 0.5754\n",
      "Epoch 152/250\n",
      "5887/5887 [==============================] - 5s 929us/step - loss: 0.2413 - accuracy: 0.5685 - val_loss: 0.2399 - val_accuracy: 0.5754\n",
      "Epoch 153/250\n",
      "5887/5887 [==============================] - 5s 929us/step - loss: 0.2410 - accuracy: 0.5711 - val_loss: 0.2399 - val_accuracy: 0.5754\n",
      "Epoch 154/250\n",
      "5887/5887 [==============================] - 5s 926us/step - loss: 0.2410 - accuracy: 0.5709 - val_loss: 0.2400 - val_accuracy: 0.5754\n",
      "Epoch 155/250\n",
      "5887/5887 [==============================] - 5s 920us/step - loss: 0.2411 - accuracy: 0.5696 - val_loss: 0.2403 - val_accuracy: 0.5781\n",
      "Epoch 156/250\n",
      "5887/5887 [==============================] - 6s 943us/step - loss: 0.2408 - accuracy: 0.5723 - val_loss: 0.2399 - val_accuracy: 0.5754\n",
      "Epoch 157/250\n",
      "5887/5887 [==============================] - 6s 1ms/step - loss: 0.2406 - accuracy: 0.5728 - val_loss: 0.2401 - val_accuracy: 0.5734\n",
      "Epoch 158/250\n",
      "1990/5887 [=========>....................] - ETA: 3s - loss: 0.2387 - accuracy: 0.5698"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train, shuffle=True,\n",
    "          batch_size=5,epochs=250,verbose=1,\n",
    "          validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49574897]\n",
      " [0.49379423]\n",
      " [0.49165258]\n",
      " ...\n",
      " [0.41656658]\n",
      " [0.41656658]\n",
      " [0.41656658]]\n"
     ]
    }
   ],
   "source": [
    "Y_out = model.predict(test_datum)\n",
    "print(Y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#We choose max_depth of 6 as our model\\nrf_model = RandomForestClassifier(max_depth=6,n_estimators=2500);\\nrf_model.fit(X_train,y_train);\\nY_pred = rf_model.predict(X_test);\\na = accuracy_score(y_test,Y_pred);\\nprint(i,a);\\n\\nY_out = rf_model.predict(test_datum)\\nprint(Y_out)\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We choose XXXX RF since best accuracy\n",
    "#print(np.mean(Y_pred))\n",
    "#print(np.mean(y_test))\n",
    "\"\"\"\n",
    "#We choose max_depth of 6 as our model\n",
    "rf_model = RandomForestClassifier(max_depth=6,n_estimators=2500);\n",
    "rf_model.fit(X_train,y_train);\n",
    "Y_pred = rf_model.predict(X_test);\n",
    "a = accuracy_score(y_test,Y_pred);\n",
    "print(i,a);\n",
    "\n",
    "Y_out = rf_model.predict(test_datum)\n",
    "print(Y_out)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is a simple algorithm to get you started...for each possible edge i-j in the test set\n",
    "# we will count the number of nodes in the training graph they have in common...if they have\n",
    "# one or more \"mutual friends\" then we will connect them (this will score about .67 acc on kaggle)\n",
    "\n",
    "alfa = 0;\n",
    "with open('/kaggle/working/submission.csv', 'w') as csvfile:\n",
    "    fieldnames = ['edge', 'label']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    with open(sample_sub) as csvfile2:\n",
    "        reader = csv.reader(csvfile2, delimiter=',')\n",
    "        for row in reader:\n",
    "            if row[1]=='1' or row[1]=='0':\n",
    "                edge= row[0].split('-')\n",
    "\n",
    "                i=int(edge[0])\n",
    "                j=int(edge[1])\n",
    "                \n",
    "                # here networkx has a common_neighbors function\n",
    "                num_com_neigh=len(sorted(nx.common_neighbors(G, i, j)))\n",
    "                y=0\n",
    "                #if num_com_neigh>0:\n",
    "                #    y=1\n",
    "\n",
    "                edg_out=str(i)+\"-\"+str(j) \n",
    "                y = Y_out[alfa];\n",
    "                alfa += 1;\n",
    "                writer.writerow({'edge': edg_out, 'label': y})\n",
    "                #print(edg_out, num_com_neigh, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "alfa = 0;\n",
    "with open('/kaggle/working/submission_alt.csv', 'w') as csvfile:\n",
    "    fieldnames = ['edge', 'label']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    with open(sample_sub) as csvfile2:\n",
    "        reader = csv.reader(csvfile2, delimiter=',')\n",
    "        for row in reader:\n",
    "            if row[1]=='1' or row[1]=='0':\n",
    "                edge= row[0].split('-')\n",
    "\n",
    "                i=int(edge[0])\n",
    "                j=int(edge[1])\n",
    "                \n",
    "                # here networkx has a common_neighbors function\n",
    "                num_com_neigh=len(sorted(nx.common_neighbors(G, i, j)))\n",
    "                y=0\n",
    "                #if num_com_neigh>0:\n",
    "                #    y=1\n",
    "\n",
    "                edg_out=str(i)+\"-\"+str(j) \n",
    "                if (Y_out[alfa] > .5):\n",
    "                    y = 1;\n",
    "                alfa += 1;\n",
    "                writer.writerow({'edge': edg_out, 'label': y})\n",
    "                #print(edg_out, num_com_neigh, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/iupui-link-prediction/features.csv\n",
      "/kaggle/input/iupui-link-prediction/train_edges.csv\n",
      "/kaggle/input/iupui-link-prediction/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, LeakyReLU, ReLU\n",
    "from keras.utils import np_utils,  to_categorical\n",
    "from keras import optimizers\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"/kaggle/input/iupui-link-prediction/train_edges.csv\";\n",
    "sample_sub = \"/kaggle/input/iupui-link-prediction/sample_submission.csv\"\n",
    "feat_file = \"/kaggle/input/iupui-link-prediction/features.csv\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_index(G):\n",
    "    \n",
    "    s = list(G.nodes);\n",
    "    num_nodes = max(s)+1;\n",
    "    \n",
    "    #Store it into a matrix for self preservation\n",
    "    #num_nodes = G.number_of_nodes();\n",
    "    common_neigh = np.zeros((num_nodes,num_nodes));\n",
    "    salton = common_neigh.copy();\n",
    "    jaccard = common_neigh.copy();\n",
    "    sorensen = common_neigh.copy();\n",
    "    hub_promoted = common_neigh.copy();\n",
    "    hub_depressed = common_neigh.copy();\n",
    "    leicht = common_neigh.copy();\n",
    "    pref = common_neigh.copy();\n",
    "    adamic = common_neigh.copy();\n",
    "    resource = common_neigh.copy();\n",
    "    \n",
    "    for k in s:\n",
    "        i = s.index(k);\n",
    "        if (i % 100 == 0):\n",
    "            print(i)\n",
    "        for l in s:\n",
    "            j = s.index(l);\n",
    "            if (i != j):\n",
    "                deg_i = G.degree[i];\n",
    "                deg_j = G.degree[j];\n",
    "                min_deg = deg_i;\n",
    "                max_deg = deg_j;\n",
    "                if (deg_j < min_deg):\n",
    "                    min_deg = deg_j;\n",
    "                    max_deg = deg_i;\n",
    "\n",
    "                com = sorted(nx.common_neighbors(G,i,j));\n",
    "                num_com = len(com);\n",
    "                common_neigh[i,j] = num_com;\n",
    "                preff = deg_i * deg_j;\n",
    "                \n",
    "                ada = 0;\n",
    "                res = 0;\n",
    "                \n",
    "                for k in com:\n",
    "                    ada += 1 / math.log(G.degree[k]);\n",
    "                    res += 1 / (G.degree[k]);\n",
    "                    \n",
    "                pref[i,j] = preff;\n",
    "                adamic[i,j] = ada;\n",
    "                resource[i,j] = res;\n",
    "\n",
    "                if (num_com > 0):\n",
    "                    salt = num_com / math.sqrt(deg_i * deg_j);\n",
    "                    jac = num_com / (deg_i + deg_j - num_com);\n",
    "                    sor = 2 * num_com / (deg_i + deg_j);\n",
    "                    hub_d = num_com / min_deg;\n",
    "                    hub_p = num_com / max_deg;\n",
    "                    lec = num_com / (deg_i * deg_j);\n",
    "\n",
    "                    salton[i,j] = salt;\n",
    "                    jaccard[i,j] = jac;\n",
    "                    sorensen[i,j] = sor;\n",
    "                    hub_promoted[i,j] = hub_p;\n",
    "                    hub_depressed[i,j] = hub_d;\n",
    "                    leicht[i,j] = lec;\n",
    "                \n",
    "    \n",
    "    return(common_neigh,salton,jaccard,sorensen,hub_promoted,hub_depressed,leicht,pref,adamic,resource);\n",
    "\n",
    "def calc_common_neigh(G):\n",
    "    #Store it into a matrix for self preservation\n",
    "    num_nodes = G.number_of_nodes();\n",
    "    common_neigh = np.zeros((num_nodes,num_nodes));\n",
    "    \n",
    "    for i in range(num_nodes):\n",
    "        if (i % 100 == 0):\n",
    "            print(i)\n",
    "        for j in range(num_nodes):\n",
    "            if (i != j):\n",
    "                num_com = len(sorted(nx.common_neighbors(G,i,j)));\n",
    "                common_neigh[i,j] = num_com\n",
    "    \n",
    "    return(common_neigh);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#Load the input files\n",
    "import csv\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "with open(train_file) as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in spamreader:\n",
    "        if row[1]=='1':\n",
    "            edge= row[0].split('-')\n",
    "            #print(int(edge[0]),int(edge[1]))\n",
    "            G.add_edge(int(edge[0]),int(edge[1]))\n",
    "            \n",
    "#s = sorted(G.nodes);\n",
    "s = G.nodes\n",
    "s = list(s)\n",
    "\n",
    "adj = nx.adjacency_matrix(G);\n",
    "print(adj[146,1356])\n",
    "print(adj[s.index(146),s.index(1356)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(s)\n",
    "#print(sorted(s))\n",
    "#print(type(s))\n",
    "#print(list(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2506\n"
     ]
    }
   ],
   "source": [
    "#Node info\n",
    "print(G.number_of_nodes());\n",
    "#nx.draw(G,node_size = 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2708\n"
     ]
    }
   ],
   "source": [
    "# next let's read in the possible edges we need to classify from the test file\n",
    "#FUCK THE WAY MOHLER DID THIS\n",
    "#It assumes all nodes are connected so fuck\n",
    "#But we will let it stand...\n",
    "\n",
    "Gsub = nx.Graph()\n",
    "\n",
    "with open(sample_sub) as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in spamreader:\n",
    "        if row[1]=='1' or row[1]=='0':\n",
    "            edge= row[0].split('-')\n",
    "            #print(edge[0],edge[1]);\n",
    "            Gsub.add_edge(int(edge[0]),int(edge[1]))\n",
    "            \n",
    "G.add_nodes_from(Gsub.nodes)\n",
    "print(G.number_of_nodes());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sorted(G.nodes))\n",
    "s = list(G.nodes);\n",
    "#print(s)\n",
    "#a = 0;\n",
    "#for i in range(len(s)-1):\n",
    "#    if(s[i+1] - s[i] != 1):\n",
    "#        a += 1;\n",
    "#print(a)\n",
    "#print(G.degree[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n"
     ]
    }
   ],
   "source": [
    "#n = calc_common_neigh(G)\n",
    "com,salt,jac,sor,hub_p,hub_d,lec,pref,ada,res = calc_index(G);\n",
    "\n",
    "\n",
    "#print(com,salt,jac,sor,hub_p,hub_d);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select only the training data\n",
    "train_df = pd.read_csv(train_file);\n",
    "train_df = train_df.values;\n",
    "sz = np.shape(train_df);\n",
    "train_data = np.zeros((sz[0],3));\n",
    "\n",
    "for i in range(sz[0]):\n",
    "    edge = train_df[i,0];\n",
    "    edge = edge.split('-');\n",
    "    train_data[i,0] = int(edge[0])\n",
    "    train_data[i,1] = int(edge[1]);\n",
    "    train_data[i,2] = int(train_df[i,1]);\n",
    "    \n",
    "#print(train_data)\n",
    "\n",
    "#Create a matrix of the data\n",
    "data_exist = np.zeros((len(s),len(s)));\n",
    "for i in range(sz[0]):\n",
    "    \n",
    "    j = int(train_data[i,0]);\n",
    "    k = int(train_data[i,1]);\n",
    "    j = s.index(j);\n",
    "    k = s.index(k);\n",
    "    \n",
    "    data_exist[j,k] = 1;\n",
    "    data_exist[k,j] = 1;\n",
    "\n",
    "def datum_in(i,j,df):\n",
    "    sz = np.shape(df);\n",
    "    for k in range(sz[0]):\n",
    "        if (df[k,0] == i):\n",
    "            if (df[k,1] == j):\n",
    "                return True;\n",
    "        if (df[k,0] == j):\n",
    "            if (df[k,1] == i):\n",
    "                return True;\n",
    "        \n",
    "    return False;\n",
    "\n",
    "def datum_in_mat(i,j,s,dat):\n",
    "    \n",
    "    if (data_exist[i,j] != 0):\n",
    "        return True;\n",
    "    else:\n",
    "        return False;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 462    0    0 ...    0    0    0]\n",
      " [1911    0    0 ...    0    0    0]\n",
      " [2002    0    0 ...    0    0    0]\n",
      " ...\n",
      " [2372    0    0 ...    0    0    0]\n",
      " [ 955    0    0 ...    0    0    0]\n",
      " [ 376    0    0 ...    0    0    0]]\n",
      "(2708, 1434)\n",
      "17\n",
      "-1429\n",
      "22\n",
      "-74\n"
     ]
    }
   ],
   "source": [
    "#Load the feature file\n",
    "feat = pd.read_csv(feat_file,header=None);\n",
    "feat = feat.values;\n",
    "print(feat)\n",
    "print(feat.shape)\n",
    "\n",
    "#feat = feat.sort(axis=0);\n",
    "#print(feat)\n",
    "\n",
    "def feat_find(x,feat):\n",
    "    y = feat[feat[:,0]==x][0][1:1433];\n",
    "    return y;\n",
    "\n",
    "y = feat_find(1911,feat)\n",
    "print(np.sum(y))\n",
    "print(np.sum(feat[0,:]) - 1911)\n",
    "y = feat_find(2002,feat)\n",
    "print(np.sum(y))\n",
    "print(np.sum(feat[1,:]) - 2002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "3\n",
      "2.23606797749979\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(3)\n",
    "print(a)\n",
    "print(np.sum(a))\n",
    "print(np.linalg.norm(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "(7388, 16)\n",
      "[[2.         0.10166571 0.01538462 ... 0.04364358 1.52713888 1.        ]\n",
      " [0.         0.         0.         ... 0.09759001 1.47305075 0.        ]\n",
      " [0.         0.         0.         ... 0.23809524 1.33039211 1.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         1.57079633 0.        ]\n",
      " [0.         0.         0.         ... 0.         1.57079633 0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Now I need to make this into something that I can feed into a neural net like nothing\n",
    "num_nodes = max(s)+1;\n",
    "adj = nx.adjacency_matrix(G);\n",
    "adj = adj.todense();\n",
    "\n",
    "sz_train = np.shape(train_df);\n",
    "datum = np.zeros((sz_train[0],16));\n",
    "ind = 0;\n",
    "\n",
    "for k in s:\n",
    "    i = s.index(k);\n",
    "    if (i % 100 == 0):\n",
    "            print(i)\n",
    "    for l in s:\n",
    "        j = s.index(l);\n",
    "        if (j > i):\n",
    "            if (datum_in_mat(i,j,s,data_exist)):\n",
    "\n",
    "                datum[ind,0] = com[i,j];\n",
    "                datum[ind,1] = salt[i,j];\n",
    "                datum[ind,2] = jac[i,j];\n",
    "                datum[ind,3] = sor[i,j];\n",
    "                datum[ind,4] = hub_p[i,j];\n",
    "                datum[ind,5] = hub_d[i,j];\n",
    "                datum[ind,6] = lec[i,j];\n",
    "                datum[ind,7] = pref[i,j];\n",
    "                datum[ind,8] = res[i,j];\n",
    "                datum[ind,9] = ada[i,j];\n",
    "\n",
    "                deg_i = G.degree[i];\n",
    "                deg_j = G.degree[j];\n",
    "                min_deg = deg_i;\n",
    "                max_deg = deg_j;\n",
    "                if (min_deg > max_deg):\n",
    "                    max_deg = deg_j;\n",
    "                    min_deg = deg_i;\n",
    "                datum[ind,10] = min_deg;\n",
    "                datum[ind,11] = max_deg;\n",
    "                \n",
    "                #print(ind,k,l)\n",
    "                y1 = feat_find(k,feat);\n",
    "                y2 = feat_find(l,feat);\n",
    "                \n",
    "                dp = np.dot(y1,y2);\n",
    "                n1 = np.linalg.norm(y1);\n",
    "                n2 = np.linalg.norm(y2);\n",
    "                dp_norm = dp / (n1*n2);\n",
    "                \n",
    "                c_ang = math.acos(dp_norm);\n",
    "                \n",
    "                datum[ind,12] = dp;\n",
    "                datum[ind,13] = dp_norm;\n",
    "                datum[ind,14] = c_ang;\n",
    "\n",
    "                #ind_i = s.index(i);\n",
    "                #ind_j = s.index(j);\n",
    "\n",
    "                datum[ind,-1] = adj[i,j];\n",
    "                ind += 1;\n",
    "        \n",
    "#remove the all zero rows\n",
    "#datum = datum[~np.all(datum == 0, axis=1)];\n",
    "#Append an all zero row\n",
    "#oof = np.zeros((1,13));\n",
    "#datum = np.append(datum,oof);\n",
    "#datum = datum.reshape(-1,13);\n",
    "print(np.shape(datum))\n",
    "print(datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#print(adj)\n",
    "\n",
    "\n",
    "print(adj[2250,985])\n",
    "print(adj[s.index(146),s.index(1356)])\n",
    "print(adj[s.index(1356),s.index(146)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 1.         0.03857584 1.53221092]\n",
      " [0.         0.         0.         ... 2.         0.18731716 1.38236608]\n",
      " [0.         0.         0.         ... 2.         0.09128709 1.47938197]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Get test data\n",
    "test_df = pd.read_csv(sample_sub);\n",
    "test_df = test_df.values;\n",
    "test_sz = np.shape(test_df);\n",
    "test_inp = np.zeros((test_sz[0],3));\n",
    "\n",
    "for i in range(test_sz[0]):\n",
    "    edge = test_df[i,0];\n",
    "    edge = edge.split('-');\n",
    "    test_inp[i,0] = int(edge[0])\n",
    "    test_inp[i,1] = int(edge[1]);\n",
    "    test_inp[i,2] = int(test_inp[i,1]);\n",
    "    \n",
    "#print(train_data)\n",
    "\n",
    "num_nodes = max(s)+1;\n",
    "\n",
    "sz_test = np.shape(train_df);\n",
    "test_datum = np.zeros((sz_test[0],15));\n",
    "ind = 0;\n",
    "\n",
    "for ind in range(test_sz[0]):\n",
    "\n",
    "    i = int(test_inp[ind,0]);\n",
    "    j = int(test_inp[ind,1]);\n",
    "    k = i;\n",
    "    l = j;\n",
    "    i = s.index(i);\n",
    "    j = s.index(j);\n",
    "    \n",
    "    test_datum[ind,0] = com[i,j];\n",
    "    test_datum[ind,1] = salt[i,j];\n",
    "    test_datum[ind,2] = jac[i,j];\n",
    "    test_datum[ind,3] = sor[i,j];\n",
    "    test_datum[ind,4] = hub_p[i,j];\n",
    "    test_datum[ind,5] = hub_d[i,j];\n",
    "    test_datum[ind,6] = lec[i,j];\n",
    "    test_datum[ind,7] = pref[i,j];\n",
    "    test_datum[ind,8] = res[i,j];\n",
    "    test_datum[ind,9] = ada[i,j];\n",
    "\n",
    "    deg_i = G.degree[i];\n",
    "    deg_j = G.degree[j];\n",
    "    min_deg = deg_i;\n",
    "    max_deg = deg_j;\n",
    "    if (min_deg > max_deg):\n",
    "        max_deg = deg_j;\n",
    "        min_deg = deg_i;\n",
    "    \n",
    "    test_datum[ind,10] = min_deg;\n",
    "    test_datum[ind,11] = max_deg;\n",
    "    \n",
    "    y1 = feat_find(k,feat);\n",
    "    y2 = feat_find(l,feat);\n",
    "\n",
    "    dp = np.dot(y1,y2);\n",
    "    n1 = np.linalg.norm(y1);\n",
    "    n2 = np.linalg.norm(y2);\n",
    "    dp_norm = dp / (n1*n2);\n",
    "\n",
    "    c_ang = math.acos(dp_norm);\n",
    "\n",
    "    test_datum[ind,12] = dp;\n",
    "    test_datum[ind,13] = dp_norm;\n",
    "    test_datum[ind,14] = c_ang;\n",
    "\n",
    "print(test_datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(datum);\n",
    "train_df.to_csv(\"train.csv\",header=None,index=None)\n",
    "test_df = pd.DataFrame(test_datum);\n",
    "test_df.to_csv(\"test.csv\",header=None,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization\n",
    "max_col = np.amax(datum,axis=0);\n",
    "datum /= max_col;\n",
    "test_datum /= max_col[:-1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I now have the training and testing data\n",
    "#Separate the training data into train and test\n",
    "Y = datum[:,-1];\n",
    "X = datum[:,:-1];\n",
    "\n",
    "#X = X.reshape(-1,12,1);\n",
    "#test_datum = test_datum.reshape(-1,12,1);\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size = .2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# create convolution neural network\\nmodel = Sequential()\\nmodel.add(Dense(512, input_shape=(12,1)))\\nmodel.add(LeakyReLU(alpha=0.05))\\nmodel.add(Dropout(0.33))\\n\\nmodel.add(Flatten())\\n\\nmodel.add(Dense(256))\\nmodel.add(LeakyReLU(alpha=0.05))\\nmodel.add(Dropout(0.33))\\n\\nmodel.add(Dense(128))\\nmodel.add(LeakyReLU(alpha=0.05))\\nmodel.add(Dropout(0.33))\\n\\nmodel.add(Dense(64))\\nmodel.add(LeakyReLU(alpha=0.05))\\nmodel.add(Dropout(0.33))\\n\\nmodel.add(Dense(32))\\nmodel.add(LeakyReLU(alpha=0.05))\\nmodel.add(Dropout(0.33))\\n\\nmodel.add(Dense(32))\\nmodel.add(LeakyReLU(alpha=0.05))\\nmodel.add(Dropout(0.25))\\n\\nmodel.add(Dense(1, activation='sigmoid'))\\n\\nmodel.summary();\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# create convolution neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(12,1)))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.33))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.33))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.33))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.33))\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.33))\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary();\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#Compile model\\n#sgd = optimizers.SGD(lr=.01);\\nmodel.compile(loss='MSE', \\n              optimizer='adagrad', \\n              metrics=['accuracy'])\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#Compile model\n",
    "#sgd = optimizers.SGD(lr=.01);\n",
    "model.compile(loss='MSE', \n",
    "              optimizer='adagrad', \n",
    "              metrics=['accuracy'])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nhistory = model.fit(X_train,y_train, shuffle=True,\\n          batch_size=5,epochs=250,verbose=1,\\n          validation_data=(X_test,y_test))\\n          '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "history = model.fit(X_train,y_train, shuffle=True,\n",
    "          batch_size=5,epochs=250,verbose=1,\n",
    "          validation_data=(X_test,y_test))\n",
    "          \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nY_out = model.predict(test_datum)\\nprint(Y_out)\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Y_out = model.predict(test_datum)\n",
    "print(Y_out)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.7334235453315291\n",
      "4 0.7374830852503383\n",
      "5 0.7395128552097429\n",
      "6 0.7395128552097429\n",
      "7 0.7395128552097429\n",
      "8 0.7354533152909337\n"
     ]
    }
   ],
   "source": [
    "#\"\"\"\n",
    "#Create and train the random forest classifier\n",
    "for i in [3,4,5,6,7,8]:\n",
    "    rf_model = RandomForestClassifier(max_depth=i,n_estimators=2500);\n",
    "    rf_model.fit(X_train,y_train);\n",
    "    Y_pred = rf_model.predict(X_test);\n",
    "    a = accuracy_score(y_test,Y_pred);\n",
    "    print(i,a);\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 0.7408660351826793\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "0.18854899837574446\n"
     ]
    }
   ],
   "source": [
    "#We choose XXXX RF since best accuracy\n",
    "#print(np.mean(Y_pred))\n",
    "#print(np.mean(y_test))\n",
    "#\"\"\"\n",
    "#We choose max_depth of 5 as our model\n",
    "rf_model = RandomForestClassifier(max_depth=5,n_estimators=2500);\n",
    "rf_model.fit(datum[:,:-1],datum[:,-1]);\n",
    "Y_pred = rf_model.predict(X_test);\n",
    "a = accuracy_score(y_test,Y_pred);\n",
    "print(i,a);\n",
    "\n",
    "Y_out = rf_model.predict(test_datum)\n",
    "print(Y_out)\n",
    "print(np.mean(Y_out))\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "# here is a simple algorithm to get you started...for each possible edge i-j in the test set\n",
    "# we will count the number of nodes in the training graph they have in common...if they have\n",
    "# one or more \"mutual friends\" then we will connect them (this will score about .67 acc on kaggle)\n",
    "\n",
    "alfa = 0;\n",
    "with open('/kaggle/working/submission.csv', 'w') as csvfile:\n",
    "    fieldnames = ['edge', 'label']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    with open(sample_sub) as csvfile2:\n",
    "        reader = csv.reader(csvfile2, delimiter=',')\n",
    "        for row in reader:\n",
    "            if row[1]=='1' or row[1]=='0':\n",
    "                edge= row[0].split('-')\n",
    "\n",
    "                i=int(edge[0])\n",
    "                j=int(edge[1])\n",
    "                \n",
    "                # here networkx has a common_neighbors function\n",
    "                num_com_neigh=len(sorted(nx.common_neighbors(G, i, j)))\n",
    "                y=0\n",
    "                #if num_com_neigh>0:\n",
    "                #    y=1\n",
    "\n",
    "                edg_out=str(i)+\"-\"+str(j) \n",
    "                y = Y_out[alfa];\n",
    "                alfa += 1;\n",
    "                writer.writerow({'edge': edg_out, 'label': y})\n",
    "                #print(edg_out, num_com_neigh, y)\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nalfa = 0;\\nwith open(\\'/kaggle/working/submission_alt.csv\\', \\'w\\') as csvfile:\\n    fieldnames = [\\'edge\\', \\'label\\']\\n    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\\n    writer.writeheader()\\n\\n    with open(sample_sub) as csvfile2:\\n        reader = csv.reader(csvfile2, delimiter=\\',\\')\\n        for row in reader:\\n            if row[1]==\\'1\\' or row[1]==\\'0\\':\\n                edge= row[0].split(\\'-\\')\\n\\n                i=int(edge[0])\\n                j=int(edge[1])\\n                \\n                # here networkx has a common_neighbors function\\n                num_com_neigh=len(sorted(nx.common_neighbors(G, i, j)))\\n                y=0\\n                #if num_com_neigh>0:\\n                #    y=1\\n\\n                edg_out=str(i)+\"-\"+str(j) \\n                if (Y_out[alfa] > .5):\\n                    y = 1;\\n                alfa += 1;\\n                writer.writerow({\\'edge\\': edg_out, \\'label\\': y})\\n                #print(edg_out, num_com_neigh, y)\\n\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "alfa = 0;\n",
    "with open('/kaggle/working/submission_alt.csv', 'w') as csvfile:\n",
    "    fieldnames = ['edge', 'label']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    with open(sample_sub) as csvfile2:\n",
    "        reader = csv.reader(csvfile2, delimiter=',')\n",
    "        for row in reader:\n",
    "            if row[1]=='1' or row[1]=='0':\n",
    "                edge= row[0].split('-')\n",
    "\n",
    "                i=int(edge[0])\n",
    "                j=int(edge[1])\n",
    "                \n",
    "                # here networkx has a common_neighbors function\n",
    "                num_com_neigh=len(sorted(nx.common_neighbors(G, i, j)))\n",
    "                y=0\n",
    "                #if num_com_neigh>0:\n",
    "                #    y=1\n",
    "\n",
    "                edg_out=str(i)+\"-\"+str(j) \n",
    "                if (Y_out[alfa] > .5):\n",
    "                    y = 1;\n",
    "                alfa += 1;\n",
    "                writer.writerow({'edge': edg_out, 'label': y})\n",
    "                #print(edg_out, num_com_neigh, y)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

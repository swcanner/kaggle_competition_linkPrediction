{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/iupui-link-prediction/features.csv\n",
      "/kaggle/input/iupui-link-prediction/sample_submission.csv\n",
      "/kaggle/input/iupui-link-prediction/train_edges.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, LeakyReLU, ReLU\n",
    "from keras.utils import np_utils,  to_categorical\n",
    "from keras import optimizers\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"/kaggle/input/iupui-link-prediction/train_edges.csv\";\n",
    "sample_sub = \"/kaggle/input/iupui-link-prediction/sample_submission.csv\"\n",
    "feat_file = \"/kaggle/input/iupui-link-prediction/features.csv\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_index(G):\n",
    "    \n",
    "    s = list(G.nodes);\n",
    "    num_nodes = max(s)+1;\n",
    "    \n",
    "    #Store it into a matrix for self preservation\n",
    "    #num_nodes = G.number_of_nodes();\n",
    "    common_neigh = np.zeros((num_nodes,num_nodes));\n",
    "    salton = common_neigh.copy();\n",
    "    jaccard = common_neigh.copy();\n",
    "    sorensen = common_neigh.copy();\n",
    "    hub_promoted = common_neigh.copy();\n",
    "    hub_depressed = common_neigh.copy();\n",
    "    leicht = common_neigh.copy();\n",
    "    pref = common_neigh.copy();\n",
    "    adamic = common_neigh.copy();\n",
    "    resource = common_neigh.copy();\n",
    "    \n",
    "    for k in s:\n",
    "        i = s.index(k);\n",
    "        if (i % 100 == 0):\n",
    "            print(i)\n",
    "        for l in s:\n",
    "            j = s.index(l);\n",
    "            if (i != j):\n",
    "                deg_i = G.degree[i];\n",
    "                deg_j = G.degree[j];\n",
    "                min_deg = deg_i;\n",
    "                max_deg = deg_j;\n",
    "                if (deg_j < min_deg):\n",
    "                    min_deg = deg_j;\n",
    "                    max_deg = deg_i;\n",
    "\n",
    "                com = sorted(nx.common_neighbors(G,i,j));\n",
    "                num_com = len(com);\n",
    "                common_neigh[i,j] = num_com;\n",
    "                preff = deg_i * deg_j;\n",
    "                \n",
    "                ada = 0;\n",
    "                res = 0;\n",
    "                \n",
    "                for k in com:\n",
    "                    ada += 1 / math.log(G.degree[k]);\n",
    "                    res += 1 / (G.degree[k]);\n",
    "                    \n",
    "                pref[i,j] = preff;\n",
    "                adamic[i,j] = ada;\n",
    "                resource[i,j] = res;\n",
    "\n",
    "                if (num_com > 0):\n",
    "                    salt = num_com / math.sqrt(deg_i * deg_j);\n",
    "                    jac = num_com / (deg_i + deg_j - num_com);\n",
    "                    sor = 2 * num_com / (deg_i + deg_j);\n",
    "                    hub_d = num_com / min_deg;\n",
    "                    hub_p = num_com / max_deg;\n",
    "                    lec = num_com / (deg_i * deg_j);\n",
    "\n",
    "                    salton[i,j] = salt;\n",
    "                    jaccard[i,j] = jac;\n",
    "                    sorensen[i,j] = sor;\n",
    "                    hub_promoted[i,j] = hub_p;\n",
    "                    hub_depressed[i,j] = hub_d;\n",
    "                    leicht[i,j] = lec;\n",
    "                \n",
    "    \n",
    "    return(common_neigh,salton,jaccard,sorensen,hub_promoted,hub_depressed,leicht,pref,adamic,resource);\n",
    "\n",
    "def calc_index_nodes(k,l,s,G):\n",
    "    \n",
    "    i = s.index(k);\n",
    "    j = s.index(l);\n",
    "    \n",
    "    deg_i = G.degree[i];\n",
    "    deg_j = G.degree[j];\n",
    "    min_deg = deg_i;\n",
    "    max_deg = deg_j;\n",
    "    if (deg_j < min_deg):\n",
    "        min_deg = deg_j;\n",
    "        max_deg = deg_i;\n",
    "\n",
    "    com = sorted(nx.common_neighbors(G,i,j));\n",
    "    num_com = len(com);\n",
    "    pref = deg_i * deg_j;\n",
    "\n",
    "    ada = 0;\n",
    "    res = 0;\n",
    "\n",
    "    for k in com:\n",
    "        ada += 1 / math.log(G.degree[k]);\n",
    "        res += 1 / (G.degree[k]);\n",
    "\n",
    "    salt = 0;\n",
    "    jac = 0;\n",
    "    sor = 0;\n",
    "    hub_d = 0;\n",
    "    hub_p = 0;\n",
    "    lec = 0;\n",
    "        \n",
    "    if (num_com > 0):\n",
    "        salt = num_com / math.sqrt(deg_i * deg_j);\n",
    "        jac = num_com / (deg_i + deg_j - num_com);\n",
    "        sor = 2 * num_com / (deg_i + deg_j);\n",
    "        hub_d = num_com / min_deg;\n",
    "        hub_p = num_com / max_deg;\n",
    "        lec = num_com / (deg_i * deg_j);                \n",
    "    \n",
    "    return(num_com,salt,jac,sor,hub_p,hub_d,lec,pref,ada,res);\n",
    "\n",
    "def calc_common_neigh(G):\n",
    "    #Store it into a matrix for self preservation\n",
    "    num_nodes = G.number_of_nodes();\n",
    "    common_neigh = np.zeros((num_nodes,num_nodes));\n",
    "    \n",
    "    for i in range(num_nodes):\n",
    "        if (i % 100 == 0):\n",
    "            print(i)\n",
    "        for j in range(num_nodes):\n",
    "            if (i != j):\n",
    "                num_com = len(sorted(nx.common_neighbors(G,i,j)));\n",
    "                common_neigh[i,j] = num_com\n",
    "    \n",
    "    return(common_neigh);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#Load the input files\n",
    "import csv\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "with open(train_file) as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in spamreader:\n",
    "        if row[1]=='1':\n",
    "            edge= row[0].split('-')\n",
    "            #print(int(edge[0]),int(edge[1]))\n",
    "            G.add_edge(int(edge[0]),int(edge[1]))\n",
    "            \n",
    "#s = sorted(G.nodes);\n",
    "s = G.nodes\n",
    "s = list(s)\n",
    "\n",
    "adj = nx.adjacency_matrix(G);\n",
    "print(adj[146,1356])\n",
    "print(adj[s.index(146),s.index(1356)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2708\n"
     ]
    }
   ],
   "source": [
    "# next let's read in the possible edges we need to classify from the test file\n",
    "#FUCK THE WAY MOHLER DID THIS\n",
    "#It assumes all nodes are connected so fuck\n",
    "#But we will let it stand...\n",
    "\n",
    "Gsub = nx.Graph()\n",
    "\n",
    "with open(sample_sub) as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in spamreader:\n",
    "        if row[1]=='1' or row[1]=='0':\n",
    "            edge= row[0].split('-')\n",
    "            #print(edge[0],edge[1]);\n",
    "            Gsub.add_edge(int(edge[0]),int(edge[1]))\n",
    "            \n",
    "G.add_nodes_from(Gsub.nodes)\n",
    "print(G.number_of_nodes());\n",
    "s = list(G.nodes);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select only the training data\n",
    "train_df = pd.read_csv(train_file);\n",
    "train_df = train_df.values;\n",
    "sz = np.shape(train_df);\n",
    "train_data = np.zeros((sz[0],3));\n",
    "\n",
    "for i in range(sz[0]):\n",
    "    edge = train_df[i,0];\n",
    "    edge = edge.split('-');\n",
    "    train_data[i,0] = int(edge[0])\n",
    "    train_data[i,1] = int(edge[1]);\n",
    "    train_data[i,2] = int(train_df[i,1]);\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 462    0    0 ...    0    0    0]\n",
      " [1911    0    0 ...    0    0    0]\n",
      " [2002    0    0 ...    0    0    0]\n",
      " ...\n",
      " [2372    0    0 ...    0    0    0]\n",
      " [ 955    0    0 ...    0    0    0]\n",
      " [ 376    0    0 ...    0    0    0]]\n",
      "(2708, 1434)\n",
      "17\n",
      "482\n",
      "22\n",
      "1928\n"
     ]
    }
   ],
   "source": [
    "#Load the feature file\n",
    "feat = pd.read_csv(feat_file,header=None);\n",
    "feat = feat.values;\n",
    "print(feat)\n",
    "print(feat.shape)\n",
    "\n",
    "#feat = feat.sort(axis=0);\n",
    "#print(feat)\n",
    "\n",
    "def feat_find(x,feat):\n",
    "    y = feat[feat[:,0]==x][0][1:1433];\n",
    "    return y;\n",
    "\n",
    "y = feat_find(1911,feat)\n",
    "print(np.sum(y))\n",
    "print(np.sum(feat[0,:]))\n",
    "y = feat_find(2002,feat)\n",
    "print(np.sum(y))\n",
    "print(np.sum(feat[1,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24 18 18 ... 14 23  2]\n"
     ]
    }
   ],
   "source": [
    "#Run K-means on the raw data\n",
    "km_k_1 = 25;\n",
    "km_k_2 = 10;\n",
    "feat_data = feat[:,1:1433];\n",
    "feat_ind = feat[:,0];\n",
    "feat_ind = feat_ind.tolist()\n",
    "\n",
    "km_1 = KMeans(n_clusters=km_k_1, random_state=0).fit(feat_data)\n",
    "km_1_lab = km_1.labels_;\n",
    "km_2 = KMeans(n_clusters=km_k_2, random_state=0).fit(feat_data)\n",
    "km_2_lab = km_2.labels_;\n",
    "\n",
    "print(km_1.labels_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28.61206931 27.26813755 25.87107964 25.15121463 23.54647124 21.39481863\n",
      " 19.90519528 19.31194525 18.89723153 18.33969828 18.05846835 17.83362693\n",
      " 17.11809814 16.93300092 16.8071248  16.69262164 16.52078844 16.29253721\n",
      " 15.89277723 15.82629542 15.68529524 15.51854285 15.3663168  15.28055615\n",
      " 15.11490981 15.04455479 14.8817165  14.62175703 14.49790702 14.42120255\n",
      " 14.40376359 14.1908171  14.08916846 13.95304846 13.93504741 13.851285\n",
      " 13.73887393 13.65861627 13.62391349 13.47851668 13.43624489 13.31623762\n",
      " 13.18422773 13.08929589 13.06151362 12.9835771  12.91093343 12.87168234\n",
      " 12.73517102 12.65581377]\n",
      "(2708, 50)\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components = 50);\n",
    "feat_pca = pca.fit_transform(feat_data)\n",
    "print(pca.singular_values_)\n",
    "print(np.shape(feat_pca))\n",
    "#print(feat_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57.98869193 28.06106211 26.38354815 25.24221479 23.83808358 22.14033389\n",
      " 21.06337249 19.41407071 18.98746452 18.35584903 18.10183269 17.88867761\n",
      " 17.6055499  17.10510647 16.93320787 16.80631596 16.6921565  16.30243008\n",
      " 15.89373125 15.82896717 15.76646632 15.55468423 15.40369101 15.28760072\n",
      " 15.1621917  15.04766364 14.89578648 14.71406208 14.53643347 14.48211731\n",
      " 14.41763929 14.21069041 14.09894683 14.0139846  13.96775518 13.89236661\n",
      " 13.79989858 13.71455299 13.6450032  13.54488142 13.52702257 13.45681756\n",
      " 13.32378067 13.2132904  13.17325318 13.14694598 13.09770372 13.00223527\n",
      " 12.87393354 12.80284622]\n",
      "(2708, 50)\n"
     ]
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components = 50, n_iter=20, random_state=42);\n",
    "feat_svd = svd.fit_transform(feat_data)\n",
    "print(svd.singular_values_)\n",
    "print(np.shape(feat_svd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "(14776, 294)\n",
      "[[ 0.          0.          0.         ... -0.00718503 -0.29170672\n",
      "   0.        ]\n",
      " [ 2.          0.10166571  0.01538462 ...  0.08148147  0.07736458\n",
      "   1.        ]\n",
      " [ 5.          0.30429031  0.14705882 ... -0.26652173 -0.08860407\n",
      "   1.        ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.10072617  0.09468894\n",
      "   1.        ]\n",
      " [ 0.          0.          0.         ... -0.39492834  0.34602869\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.28412727 -0.13233154\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Now I need to make this into something that I can feed into a neural net like nothing\n",
    "num_nodes = max(s)+1;\n",
    "adj = nx.adjacency_matrix(G);\n",
    "adj = adj.todense();\n",
    "\n",
    "sz_train = np.shape(train_df);\n",
    "sz_pca = np.shape(feat_pca);\n",
    "sz_svd = np.shape(feat_svd);\n",
    "datum = np.zeros((sz_train[0],24+km_k_1*2+km_k_2*2+sz_pca[1]*2+sz_svd[1]*2));\n",
    "datum_1 = datum.copy();\n",
    "ind = 0;\n",
    "\n",
    "\n",
    "\n",
    "eps = .0001;\n",
    "\n",
    "for ind in range(sz_train[0]):\n",
    "    \n",
    "    if (ind % 1000 == 0):\n",
    "        print(ind)\n",
    "\n",
    "    k = train_data[ind,0];\n",
    "    l = train_data[ind,1];\n",
    "    i = s.index(k);\n",
    "    j = s.index(l);\n",
    "    \n",
    "    com, salt, jac, sor, hub_p, hub_d, lec, pref, res, ada = calc_index_nodes(k,l,s,G);\n",
    "    \n",
    "    datum[ind,0] = com;\n",
    "    datum[ind,1] = salt;\n",
    "    datum[ind,2] = jac;\n",
    "    datum[ind,3] = sor;\n",
    "    datum[ind,4] = hub_p;\n",
    "    datum[ind,5] = hub_d;\n",
    "    datum[ind,6] = lec;\n",
    "    datum[ind,7] = pref;\n",
    "    datum[ind,8] = res;\n",
    "    datum[ind,9] = ada;\n",
    "\n",
    "    deg_i = G.degree[i];\n",
    "    deg_j = G.degree[j];\n",
    "    min_deg = deg_i;\n",
    "    max_deg = deg_j;\n",
    "    if (min_deg > max_deg):\n",
    "        max_deg = deg_j;\n",
    "        min_deg = deg_i;\n",
    "    datum[ind,10] = min_deg;\n",
    "    datum[ind,11] = max_deg;\n",
    "\n",
    "    #print(ind,k,l)\n",
    "    y1 = feat_find(k,feat);\n",
    "    y2 = feat_find(l,feat);\n",
    "\n",
    "    dp = np.dot(y1,y2);\n",
    "    n1 = np.linalg.norm(y1);\n",
    "    n2 = np.linalg.norm(y2);\n",
    "    dp_norm = dp / (n1*n2);\n",
    "\n",
    "    c_ang = math.acos(dp_norm);\n",
    "\n",
    "    datum[ind,12] = dp;\n",
    "    datum[ind,13] = dp_norm;\n",
    "    datum[ind,14] = c_ang;\n",
    "\n",
    "    #Covariance stuff\n",
    "    #Use the cov and inv cov\n",
    "\n",
    "    #Covariance of the features\n",
    "    c_cov = np.cov(y1,y2);\n",
    "    new_cov = c_cov.copy();\n",
    "\n",
    "    for i in range(np.shape(new_cov)[0]):\n",
    "        new_cov[i,i] += eps\n",
    "\n",
    "    new_cov = np.linalg.inv(new_cov);\n",
    "\n",
    "    c_cov = c_cov.flatten();\n",
    "    new_cov = new_cov.flatten();\n",
    "\n",
    "    datum[ind,15:19] = c_cov;\n",
    "    datum[ind,19:23] = new_cov;\n",
    "    \n",
    "    #Kmeans\n",
    "    i_f_ind = feat_ind.index(k);\n",
    "    j_f_ind = feat_ind.index(l);\n",
    "    \n",
    "    k_c1 = km_1_lab[i_f_ind];\n",
    "    k_c2 = km_1_lab[j_f_ind];\n",
    "    \n",
    "    \n",
    "    datum_1[ind,:23] = datum[ind,:23];\n",
    "    \n",
    "    datum[ind,23+k_c1] = 1;\n",
    "    datum[ind,23+km_k_1+k_c2] = 1;\n",
    "    datum_1[ind,23+k_c2] = 1;\n",
    "    datum_1[ind,23+km_k_1+k_c1] = 1;\n",
    "    \n",
    "    \n",
    "    k_c1 = km_2_lab[i_f_ind];\n",
    "    k_c2 = km_2_lab[j_f_ind];\n",
    "    datum[ind,23+km_k_1*2+k_c1] = 1;\n",
    "    datum[ind,23+km_k_1*2+km_k_2+k_c2] = 1;\n",
    "    datum_1[ind,23+k_c2] = 1;\n",
    "    datum_1[ind,23+km_k_1*2+km_k_2+k_c1] = 1;\n",
    "\n",
    "    #PCA\n",
    "    p_c1 = feat_pca[i_f_ind,:];\n",
    "    p_c2 = feat_pca[j_f_ind,:];\n",
    "    new_st = 23+km_k_1*2+km_k_2*2;\n",
    "    \n",
    "    datum[ind,new_st:new_st+sz_pca[1]] = p_c1;\n",
    "    datum[ind,new_st+sz_pca[1]:new_st+sz_pca[1]*2] = p_c2;\n",
    "    datum_1[ind,new_st:new_st+sz_pca[1]] = p_c2;\n",
    "    datum_1[ind,new_st+sz_pca[1]:new_st+sz_pca[1]*2] = p_c1;\n",
    "    \n",
    "    #SVD\n",
    "    s_c1 = feat_svd[i_f_ind,:];\n",
    "    s_c2 = feat_svd[j_f_ind,:];\n",
    "    #print(s_c1)\n",
    "    new_st = 23+km_k_1*2+km_k_2*2+sz_pca[1]*2;\n",
    "    #print(new_st,new_st+sz_svd[1])\n",
    "    \n",
    "    datum[ind,new_st:new_st+sz_svd[1]] = s_c1;\n",
    "    datum[ind,new_st+sz_svd[1]:new_st+sz_svd[1]*2] = s_c2;\n",
    "    datum_1[ind,new_st:new_st+sz_svd[1]] = s_c2;\n",
    "    datum_1[ind,new_st+sz_svd[1]:new_st+sz_svd[1]*2] = s_c1;\n",
    "    \n",
    "    \n",
    "    datum[ind,-1] = train_data[ind,2];\n",
    "    datum_1[ind,-1] = train_data[ind,2];\n",
    "        \n",
    "\n",
    "#Combine our datums\n",
    "datum = np.concatenate((datum,datum_1),axis=0);    \n",
    "    \n",
    "print(np.shape(datum))\n",
    "print(datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "[[ 0.          0.          0.         ... -0.1189455  -0.01883812\n",
      "  -0.32427948]\n",
      " [ 0.          0.          0.         ...  0.07919397 -0.11346237\n",
      "  -0.26149187]\n",
      " [ 0.          0.          0.         ...  0.55638111 -0.34469456\n",
      "  -0.00997553]\n",
      " ...\n",
      " [ 0.          0.          0.         ... -0.00363146 -0.0605926\n",
      "   0.02013961]\n",
      " [ 0.          0.          0.         ... -0.0641351   0.44628184\n",
      "   0.49173668]\n",
      " [ 0.          0.          0.         ...  0.07332682  0.17217887\n",
      "  -0.01377703]]\n"
     ]
    }
   ],
   "source": [
    "#Get test data\n",
    "test_df = pd.read_csv(sample_sub);\n",
    "test_df = test_df.values;\n",
    "test_sz = np.shape(test_df);\n",
    "test_inp = np.zeros((test_sz[0],3));\n",
    "\n",
    "for i in range(test_sz[0]):\n",
    "    edge = test_df[i,0];\n",
    "    edge = edge.split('-');\n",
    "    test_inp[i,0] = int(edge[0])\n",
    "    test_inp[i,1] = int(edge[1]);\n",
    "    test_inp[i,2] = int(test_inp[i,1]);\n",
    "    \n",
    "#print(train_data)\n",
    "\n",
    "num_nodes = max(s)+1;\n",
    "\n",
    "test_datum = np.zeros((test_sz[0],23+km_k_1*2+km_k_2*2+sz_pca[1]*2+sz_svd[1]*2));\n",
    "test_datum_1 = test_datum.copy();\n",
    "ind = 0;\n",
    "\n",
    "for ind in range(test_sz[0]):\n",
    "    \n",
    "    if (ind % 1000 == 0):\n",
    "        print(ind)\n",
    "\n",
    "    k = int(test_inp[ind,0]);\n",
    "    l = int(test_inp[ind,1]);\n",
    "    i = s.index(k);\n",
    "    j = s.index(l);\n",
    "    \n",
    "    com, salt, jac, sor, hub_p, hub_d, lec, pref, res, ada = calc_index_nodes(k,l,s,G);\n",
    "    \n",
    "    test_datum[ind,0] = com;\n",
    "    test_datum[ind,1] = salt;\n",
    "    test_datum[ind,2] = jac;\n",
    "    test_datum[ind,3] = sor;\n",
    "    test_datum[ind,4] = hub_p;\n",
    "    test_datum[ind,5] = hub_d;\n",
    "    test_datum[ind,6] = lec;\n",
    "    test_datum[ind,7] = pref;\n",
    "    test_datum[ind,8] = res;\n",
    "    test_datum[ind,9] = ada;\n",
    "\n",
    "    deg_i = G.degree[i];\n",
    "    deg_j = G.degree[j];\n",
    "    min_deg = deg_i;\n",
    "    max_deg = deg_j;\n",
    "    if (min_deg > max_deg):\n",
    "        max_deg = deg_j;\n",
    "        min_deg = deg_i;\n",
    "    \n",
    "    test_datum[ind,10] = min_deg;\n",
    "    test_datum[ind,11] = max_deg;\n",
    "    \n",
    "    y1 = feat_find(k,feat);\n",
    "    y2 = feat_find(l,feat);\n",
    "\n",
    "    dp = np.dot(y1,y2);\n",
    "    n1 = np.linalg.norm(y1);\n",
    "    n2 = np.linalg.norm(y2);\n",
    "    dp_norm = dp / (n1*n2);\n",
    "\n",
    "    c_ang = math.acos(dp_norm);\n",
    "\n",
    "    test_datum[ind,12] = dp;\n",
    "    test_datum[ind,13] = dp_norm;\n",
    "    test_datum[ind,14] = c_ang;\n",
    "    \n",
    "    #Covariance of the features\n",
    "    c_cov = np.cov(y1,y2);\n",
    "    new_cov = c_cov.copy();\n",
    "\n",
    "    for i in range(np.shape(new_cov)[0]):\n",
    "        new_cov[i,i] += eps\n",
    "\n",
    "    new_cov = np.linalg.inv(new_cov);\n",
    "\n",
    "    c_cov = c_cov.flatten();\n",
    "    new_cov = new_cov.flatten();\n",
    "\n",
    "    test_datum[ind,15:19] = c_cov;\n",
    "    test_datum[ind,19:23] = new_cov;\n",
    "    \n",
    "    \n",
    "    #Kmeans\n",
    "    i_f_ind = feat_ind.index(k);\n",
    "    j_f_ind = feat_ind.index(l);\n",
    "    \n",
    "    k_c1 = km_1_lab[i_f_ind];\n",
    "    k_c2 = km_1_lab[j_f_ind];\n",
    "    \n",
    "    test_datum_1[ind,:23] = test_datum[ind,:23];\n",
    "    \n",
    "    test_datum[ind,23+k_c1] = 1;\n",
    "    test_datum[ind,23+km_k_1+k_c2] = 1;\n",
    "    \n",
    "    test_datum_1[ind,23+k_c2] = 1;\n",
    "    test_datum_1[ind,23+km_k_1+k_c1] = 1;\n",
    "    \n",
    "    k_c1 = km_2_lab[i_f_ind];\n",
    "    k_c2 = km_2_lab[j_f_ind];\n",
    "    \n",
    "    test_datum[ind,23+km_k_1*2+k_c1] = 1;\n",
    "    test_datum[ind,23+km_k_1*2+km_k_2+k_c2] = 1;\n",
    "    \n",
    "    test_datum_1[ind,23+k_c2] = 1;\n",
    "    test_datum_1[ind,23+km_k_1*2+km_k_2+k_c1] = 1;\n",
    "    \n",
    "    #PCA\n",
    "    p_c1 = feat_pca[i_f_ind,:];\n",
    "    p_c2 = feat_pca[j_f_ind,:];\n",
    "    new_st = 23+km_k_1*2+km_k_2*2;\n",
    "    \n",
    "    test_datum[ind,new_st:new_st+sz_pca[1]] = p_c1;\n",
    "    test_datum[ind,new_st+sz_pca[1]:new_st+sz_pca[1]*2] = p_c2;\n",
    "    test_datum_1[ind,new_st:new_st+sz_pca[1]] = p_c2;\n",
    "    test_datum_1[ind,new_st+sz_pca[1]:new_st+sz_pca[1]*2] = p_c1;\n",
    "    \n",
    "    #SVD\n",
    "    s_c1 = feat_svd[i_f_ind,:];\n",
    "    s_c2 = feat_svd[j_f_ind,:];\n",
    "    #print(s_c1)\n",
    "    new_st = 23+km_k_1*2+km_k_2*2+sz_pca[1]*2;\n",
    "    #print(new_st,new_st+sz_svd[1])\n",
    "    \n",
    "    test_datum[ind,new_st:new_st+sz_svd[1]] = s_c1;\n",
    "    test_datum[ind,new_st+sz_svd[1]:new_st+sz_svd[1]*2] = s_c2;\n",
    "    test_datum_1[ind,new_st:new_st+sz_svd[1]] = s_c2;\n",
    "    test_datum_1[ind,new_st+sz_svd[1]:new_st+sz_svd[1]*2] = s_c1;\n",
    "\n",
    "\n",
    "print(test_datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(datum);\n",
    "train_df.to_csv(\"train.csv\",header=None,index=None)\n",
    "test_df = pd.DataFrame(test_datum);\n",
    "test_df.to_csv(\"test.csv\",header=None,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization\n",
    "max_col = np.amax(datum,axis=0);\n",
    "datum /= max_col;\n",
    "test_datum /= max_col[:-1];\n",
    "test_datum_1 /= max_col[:-1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I now have the training and testing data\n",
    "#Separate the training data into train and test\n",
    "Y = datum[:,-1];\n",
    "X = datum[:,:-1];\n",
    "\n",
    "sz_X = np.shape(X);\n",
    "\n",
    "#\"\"\"\n",
    "X = X.reshape(-1,sz_X[1],1);\n",
    "test_datum = test_datum.reshape(-1,sz_X[1],1);\n",
    "test_datum_1 = test_datum_1.reshape(-1,sz_X[1],1);\n",
    "#\"\"\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size = .25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 293, 512)          1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 293, 512)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 293, 512)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 150016)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               38404352  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 38,466,209\n",
      "Trainable params: 38,466,209\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#\"\"\"\n",
    "#create convolution neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(sz_X[1],1)))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.33))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.33))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.33))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.33))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.33))\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.33))\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary();\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11082 samples, validate on 3694 samples\n",
      "Epoch 1/250\n",
      "11082/11082 [==============================] - 11s 983us/step - loss: 0.1999 - accuracy: 0.7056 - val_loss: 0.1874 - val_accuracy: 0.7128\n",
      "Epoch 2/250\n",
      "11082/11082 [==============================] - 10s 879us/step - loss: 0.1823 - accuracy: 0.7424 - val_loss: 0.1761 - val_accuracy: 0.7426\n",
      "Epoch 3/250\n",
      "11082/11082 [==============================] - 10s 873us/step - loss: 0.1777 - accuracy: 0.7445 - val_loss: 0.1842 - val_accuracy: 0.7285\n",
      "Epoch 4/250\n",
      "11082/11082 [==============================] - 10s 857us/step - loss: 0.1705 - accuracy: 0.7578 - val_loss: 0.1717 - val_accuracy: 0.7501\n",
      "Epoch 5/250\n",
      "11082/11082 [==============================] - 9s 853us/step - loss: 0.1651 - accuracy: 0.7679 - val_loss: 0.1664 - val_accuracy: 0.7604\n",
      "Epoch 6/250\n",
      "11082/11082 [==============================] - 9s 856us/step - loss: 0.1555 - accuracy: 0.7814 - val_loss: 0.1659 - val_accuracy: 0.7620\n",
      "Epoch 7/250\n",
      "11082/11082 [==============================] - 9s 856us/step - loss: 0.1500 - accuracy: 0.7919 - val_loss: 0.1631 - val_accuracy: 0.7637\n",
      "Epoch 8/250\n",
      "11082/11082 [==============================] - 10s 869us/step - loss: 0.1418 - accuracy: 0.8041 - val_loss: 0.1594 - val_accuracy: 0.7734\n",
      "Epoch 9/250\n",
      "11082/11082 [==============================] - 10s 868us/step - loss: 0.1373 - accuracy: 0.8107 - val_loss: 0.1600 - val_accuracy: 0.7767\n",
      "Epoch 10/250\n",
      "11082/11082 [==============================] - 9s 852us/step - loss: 0.1296 - accuracy: 0.8235 - val_loss: 0.1589 - val_accuracy: 0.7767\n",
      "Epoch 11/250\n",
      "11082/11082 [==============================] - 9s 857us/step - loss: 0.1230 - accuracy: 0.8372 - val_loss: 0.1557 - val_accuracy: 0.7737\n",
      "Epoch 12/250\n",
      "11082/11082 [==============================] - 9s 849us/step - loss: 0.1175 - accuracy: 0.8415 - val_loss: 0.1592 - val_accuracy: 0.7753\n",
      "Epoch 13/250\n",
      "11082/11082 [==============================] - 9s 852us/step - loss: 0.1136 - accuracy: 0.8452 - val_loss: 0.1535 - val_accuracy: 0.7802\n",
      "Epoch 14/250\n",
      "11082/11082 [==============================] - 9s 853us/step - loss: 0.1074 - accuracy: 0.8569 - val_loss: 0.1597 - val_accuracy: 0.7791\n",
      "Epoch 15/250\n",
      "11082/11082 [==============================] - 10s 892us/step - loss: 0.1008 - accuracy: 0.8665 - val_loss: 0.1680 - val_accuracy: 0.7683\n",
      "Epoch 16/250\n",
      "11082/11082 [==============================] - 10s 862us/step - loss: 0.0985 - accuracy: 0.8689 - val_loss: 0.1630 - val_accuracy: 0.7764\n",
      "Epoch 17/250\n",
      "11082/11082 [==============================] - 10s 863us/step - loss: 0.0949 - accuracy: 0.8763 - val_loss: 0.1643 - val_accuracy: 0.7775\n",
      "Epoch 18/250\n",
      "11082/11082 [==============================] - 9s 856us/step - loss: 0.0900 - accuracy: 0.8815 - val_loss: 0.1691 - val_accuracy: 0.7726\n",
      "Epoch 19/250\n",
      "11082/11082 [==============================] - 9s 848us/step - loss: 0.0883 - accuracy: 0.8855 - val_loss: 0.1675 - val_accuracy: 0.7775\n",
      "Epoch 20/250\n",
      "11082/11082 [==============================] - 9s 850us/step - loss: 0.0851 - accuracy: 0.8912 - val_loss: 0.1689 - val_accuracy: 0.7799\n",
      "Epoch 21/250\n",
      "11082/11082 [==============================] - 10s 894us/step - loss: 0.0815 - accuracy: 0.8945 - val_loss: 0.1723 - val_accuracy: 0.7672\n",
      "Epoch 22/250\n",
      "11082/11082 [==============================] - 9s 857us/step - loss: 0.0791 - accuracy: 0.9005 - val_loss: 0.1656 - val_accuracy: 0.7767\n",
      "Epoch 23/250\n",
      "11082/11082 [==============================] - 9s 851us/step - loss: 0.0756 - accuracy: 0.9040 - val_loss: 0.1730 - val_accuracy: 0.7731\n",
      "Epoch 24/250\n",
      "11082/11082 [==============================] - 9s 849us/step - loss: 0.0750 - accuracy: 0.9029 - val_loss: 0.1695 - val_accuracy: 0.7764\n",
      "Epoch 25/250\n",
      "11082/11082 [==============================] - 10s 858us/step - loss: 0.0717 - accuracy: 0.9073 - val_loss: 0.1696 - val_accuracy: 0.7829\n",
      "Epoch 26/250\n",
      "11082/11082 [==============================] - 9s 851us/step - loss: 0.0687 - accuracy: 0.9109 - val_loss: 0.1761 - val_accuracy: 0.7756\n",
      "Epoch 27/250\n",
      "11082/11082 [==============================] - 10s 877us/step - loss: 0.0686 - accuracy: 0.9145 - val_loss: 0.1807 - val_accuracy: 0.7737\n",
      "Epoch 28/250\n",
      "11082/11082 [==============================] - 10s 866us/step - loss: 0.0675 - accuracy: 0.9163 - val_loss: 0.1740 - val_accuracy: 0.7799\n",
      "Epoch 29/250\n",
      "11082/11082 [==============================] - 10s 860us/step - loss: 0.0628 - accuracy: 0.9238 - val_loss: 0.1743 - val_accuracy: 0.7753\n",
      "Epoch 30/250\n",
      "11082/11082 [==============================] - 9s 847us/step - loss: 0.0620 - accuracy: 0.9219 - val_loss: 0.1739 - val_accuracy: 0.7777\n",
      "Epoch 31/250\n",
      "11082/11082 [==============================] - 9s 848us/step - loss: 0.0595 - accuracy: 0.9245 - val_loss: 0.1778 - val_accuracy: 0.7748\n",
      "Epoch 32/250\n",
      "11082/11082 [==============================] - 9s 856us/step - loss: 0.0579 - accuracy: 0.9289 - val_loss: 0.1698 - val_accuracy: 0.7810\n",
      "Epoch 33/250\n",
      "11082/11082 [==============================] - 10s 859us/step - loss: 0.0586 - accuracy: 0.9271 - val_loss: 0.1769 - val_accuracy: 0.7707\n",
      "Epoch 34/250\n",
      "11082/11082 [==============================] - 9s 856us/step - loss: 0.0265 - accuracy: 0.9671 - val_loss: 0.1947 - val_accuracy: 0.7837\n",
      "Epoch 99/250\n",
      "11082/11082 [==============================] - 9s 854us/step - loss: 0.0254 - accuracy: 0.9692 - val_loss: 0.2047 - val_accuracy: 0.7694\n",
      "Epoch 100/250\n",
      "11082/11082 [==============================] - 9s 851us/step - loss: 0.0269 - accuracy: 0.9663 - val_loss: 0.1911 - val_accuracy: 0.7788\n",
      "Epoch 101/250\n",
      "11082/11082 [==============================] - 9s 853us/step - loss: 0.0259 - accuracy: 0.9706 - val_loss: 0.1916 - val_accuracy: 0.7756\n",
      "Epoch 102/250\n",
      "11082/11082 [==============================] - 9s 852us/step - loss: 0.0244 - accuracy: 0.9704 - val_loss: 0.1949 - val_accuracy: 0.7807\n",
      "Epoch 103/250\n",
      "11082/11082 [==============================] - 10s 884us/step - loss: 0.0240 - accuracy: 0.9722 - val_loss: 0.2026 - val_accuracy: 0.7710\n",
      "Epoch 104/250\n",
      "11082/11082 [==============================] - 9s 854us/step - loss: 0.0263 - accuracy: 0.9684 - val_loss: 0.1883 - val_accuracy: 0.7837\n",
      "Epoch 105/250\n",
      "11082/11082 [==============================] - 10s 858us/step - loss: 0.0237 - accuracy: 0.9721 - val_loss: 0.1990 - val_accuracy: 0.7775\n",
      "Epoch 106/250\n",
      "11082/11082 [==============================] - 9s 846us/step - loss: 0.0285 - accuracy: 0.9650 - val_loss: 0.1988 - val_accuracy: 0.7807\n",
      "Epoch 107/250\n",
      "11082/11082 [==============================] - 9s 851us/step - loss: 0.0268 - accuracy: 0.9694 - val_loss: 0.1952 - val_accuracy: 0.7799\n",
      "Epoch 108/250\n",
      "11082/11082 [==============================] - 9s 854us/step - loss: 0.0287 - accuracy: 0.9660 - val_loss: 0.1939 - val_accuracy: 0.7796\n",
      "Epoch 109/250\n",
      "11082/11082 [==============================] - 10s 878us/step - loss: 0.0269 - accuracy: 0.9690 - val_loss: 0.1951 - val_accuracy: 0.7807\n",
      "Epoch 110/250\n",
      "11082/11082 [==============================] - 10s 864us/step - loss: 0.0297 - accuracy: 0.9660 - val_loss: 0.1947 - val_accuracy: 0.7802\n",
      "Epoch 111/250\n",
      "11082/11082 [==============================] - 9s 852us/step - loss: 0.0256 - accuracy: 0.9702 - val_loss: 0.2018 - val_accuracy: 0.7818\n",
      "Epoch 112/250\n",
      "11082/11082 [==============================] - 9s 851us/step - loss: 0.0267 - accuracy: 0.9684 - val_loss: 0.1917 - val_accuracy: 0.7823\n",
      "Epoch 113/250\n",
      "11082/11082 [==============================] - 10s 859us/step - loss: 0.0254 - accuracy: 0.9694 - val_loss: 0.1856 - val_accuracy: 0.7845\n",
      "Epoch 114/250\n",
      "11082/11082 [==============================] - 10s 859us/step - loss: 0.0250 - accuracy: 0.9700 - val_loss: 0.1910 - val_accuracy: 0.7786\n",
      "Epoch 115/250\n",
      "11082/11082 [==============================] - 10s 870us/step - loss: 0.0233 - accuracy: 0.9724 - val_loss: 0.1937 - val_accuracy: 0.7848\n",
      "Epoch 116/250\n",
      "11082/11082 [==============================] - 10s 881us/step - loss: 0.0240 - accuracy: 0.9718 - val_loss: 0.1912 - val_accuracy: 0.7867\n",
      "Epoch 117/250\n",
      "11082/11082 [==============================] - 9s 850us/step - loss: 0.0247 - accuracy: 0.9707 - val_loss: 0.1944 - val_accuracy: 0.7791\n",
      "Epoch 118/250\n",
      "11082/11082 [==============================] - 9s 850us/step - loss: 0.0216 - accuracy: 0.9743 - val_loss: 0.1950 - val_accuracy: 0.7767\n",
      "Epoch 119/250\n",
      "11082/11082 [==============================] - 9s 851us/step - loss: 0.0240 - accuracy: 0.9718 - val_loss: 0.1967 - val_accuracy: 0.7759\n",
      "Epoch 120/250\n",
      "11082/11082 [==============================] - 9s 857us/step - loss: 0.0222 - accuracy: 0.9740 - val_loss: 0.1923 - val_accuracy: 0.7786\n",
      "Epoch 121/250\n",
      "11082/11082 [==============================] - 9s 848us/step - loss: 0.0235 - accuracy: 0.9720 - val_loss: 0.2004 - val_accuracy: 0.7745\n",
      "Epoch 122/250\n",
      "11082/11082 [==============================] - 10s 887us/step - loss: 0.0277 - accuracy: 0.9675 - val_loss: 0.1995 - val_accuracy: 0.7775\n",
      "Epoch 123/250\n",
      "11082/11082 [==============================] - 9s 857us/step - loss: 0.0231 - accuracy: 0.9730 - val_loss: 0.1960 - val_accuracy: 0.7799\n",
      "Epoch 124/250\n",
      "11082/11082 [==============================] - 9s 856us/step - loss: 0.0239 - accuracy: 0.9716 - val_loss: 0.1918 - val_accuracy: 0.7829\n",
      "Epoch 125/250\n",
      "11082/11082 [==============================] - 9s 850us/step - loss: 0.0232 - accuracy: 0.9727 - val_loss: 0.1938 - val_accuracy: 0.7807\n",
      "Epoch 126/250\n",
      "11082/11082 [==============================] - 9s 848us/step - loss: 0.0220 - accuracy: 0.9737 - val_loss: 0.1963 - val_accuracy: 0.7815\n",
      "Epoch 127/250\n",
      "11082/11082 [==============================] - 9s 853us/step - loss: 0.0229 - accuracy: 0.9729 - val_loss: 0.2030 - val_accuracy: 0.7731\n",
      "Epoch 128/250\n",
      "11082/11082 [==============================] - 10s 880us/step - loss: 0.0253 - accuracy: 0.9698 - val_loss: 0.1893 - val_accuracy: 0.7823\n",
      "Epoch 129/250\n",
      "11082/11082 [==============================] - 10s 858us/step - loss: 0.0247 - accuracy: 0.9710 - val_loss: 0.1909 - val_accuracy: 0.7750\n",
      "Epoch 130/250\n",
      "11082/11082 [==============================] - 9s 852us/step - loss: 0.0236 - accuracy: 0.9718 - val_loss: 0.1899 - val_accuracy: 0.7823\n",
      "Epoch 131/250\n",
      "11082/11082 [==============================] - 9s 855us/step - loss: 0.0207 - accuracy: 0.9752 - val_loss: 0.1919 - val_accuracy: 0.7851\n",
      "Epoch 132/250\n",
      "11082/11082 [==============================] - 9s 849us/step - loss: 0.0206 - accuracy: 0.9764 - val_loss: 0.1973 - val_accuracy: 0.7832\n",
      "Epoch 133/250\n",
      "11082/11082 [==============================] - 9s 852us/step - loss: 0.0223 - accuracy: 0.9725 - val_loss: 0.2025 - val_accuracy: 0.7769\n",
      "Epoch 134/250\n",
      "11082/11082 [==============================] - 10s 869us/step - loss: 0.0277 - accuracy: 0.9684 - val_loss: 0.1956 - val_accuracy: 0.7864\n",
      "Epoch 135/250\n",
      "11082/11082 [==============================] - 9s 850us/step - loss: 0.0226 - accuracy: 0.9728 - val_loss: 0.1996 - val_accuracy: 0.7851\n",
      "Epoch 204/250\n",
      "11082/11082 [==============================] - 10s 888us/step - loss: 0.0244 - accuracy: 0.9721 - val_loss: 0.2008 - val_accuracy: 0.7813\n",
      "Epoch 205/250\n",
      "11082/11082 [==============================] - 10s 860us/step - loss: 0.0233 - accuracy: 0.9735 - val_loss: 0.1981 - val_accuracy: 0.7848\n",
      "Epoch 206/250\n",
      "11082/11082 [==============================] - 9s 852us/step - loss: 0.0240 - accuracy: 0.9724 - val_loss: 0.1942 - val_accuracy: 0.7867\n",
      "Epoch 207/250\n",
      "11082/11082 [==============================] - 9s 850us/step - loss: 0.0208 - accuracy: 0.9750 - val_loss: 0.1860 - val_accuracy: 0.7891\n",
      "Epoch 208/250\n",
      "11082/11082 [==============================] - 9s 854us/step - loss: 0.0181 - accuracy: 0.9794 - val_loss: 0.1985 - val_accuracy: 0.7870\n",
      "Epoch 209/250\n",
      "11082/11082 [==============================] - 10s 860us/step - loss: 0.0211 - accuracy: 0.9750 - val_loss: 0.1967 - val_accuracy: 0.7845\n",
      "Epoch 210/250\n",
      "11082/11082 [==============================] - 10s 878us/step - loss: 0.0242 - accuracy: 0.9724 - val_loss: 0.1956 - val_accuracy: 0.7842\n",
      "Epoch 211/250\n",
      "11082/11082 [==============================] - 10s 858us/step - loss: 0.0232 - accuracy: 0.9727 - val_loss: 0.1987 - val_accuracy: 0.7883\n",
      "Epoch 212/250\n",
      "11082/11082 [==============================] - 10s 864us/step - loss: 0.0231 - accuracy: 0.9733 - val_loss: 0.2003 - val_accuracy: 0.7704\n",
      "Epoch 213/250\n",
      "11082/11082 [==============================] - 9s 856us/step - loss: 0.0215 - accuracy: 0.9752 - val_loss: 0.1994 - val_accuracy: 0.7826\n",
      "Epoch 214/250\n",
      "11082/11082 [==============================] - 9s 845us/step - loss: 0.0211 - accuracy: 0.9753 - val_loss: 0.1925 - val_accuracy: 0.7891\n",
      "Epoch 215/250\n",
      "11082/11082 [==============================] - 9s 854us/step - loss: 0.0209 - accuracy: 0.9755 - val_loss: 0.1921 - val_accuracy: 0.7907\n",
      "Epoch 216/250\n",
      "11082/11082 [==============================] - 10s 871us/step - loss: 0.0252 - accuracy: 0.9721 - val_loss: 0.2060 - val_accuracy: 0.7818\n",
      "Epoch 217/250\n",
      "11082/11082 [==============================] - 10s 875us/step - loss: 0.0238 - accuracy: 0.9728 - val_loss: 0.2016 - val_accuracy: 0.7834\n",
      "Epoch 218/250\n",
      "11082/11082 [==============================] - 9s 853us/step - loss: 0.0224 - accuracy: 0.9727 - val_loss: 0.1970 - val_accuracy: 0.7856\n",
      "Epoch 219/250\n",
      "11082/11082 [==============================] - 9s 850us/step - loss: 0.0229 - accuracy: 0.9733 - val_loss: 0.1961 - val_accuracy: 0.7821\n",
      "Epoch 220/250\n",
      "11082/11082 [==============================] - 9s 854us/step - loss: 0.0228 - accuracy: 0.9736 - val_loss: 0.1958 - val_accuracy: 0.7856\n",
      "Epoch 221/250\n",
      "11082/11082 [==============================] - 9s 849us/step - loss: 0.0201 - accuracy: 0.9765 - val_loss: 0.1917 - val_accuracy: 0.7913\n",
      "Epoch 222/250\n",
      "11082/11082 [==============================] - 10s 880us/step - loss: 0.0205 - accuracy: 0.9762 - val_loss: 0.1995 - val_accuracy: 0.7840\n",
      "Epoch 223/250\n",
      "10680/11082 [===========================>..] - ETA: 0s - loss: 0.0224 - accuracy: 0.9729"
     ]
    }
   ],
   "source": [
    "#\"\"\"\n",
    "#Compile model\n",
    "sgd = optimizers.SGD(lr=.01);\n",
    "model.compile(loss='MSE', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train,y_train, shuffle=True,\n",
    "          batch_size=20,epochs=250,verbose=1,\n",
    "          validation_data=(X_test,y_test))\n",
    "\n",
    "Y_out_0 = model.predict(test_datum)\n",
    "Y_out_1 = model.predict(test_datum_1);\n",
    "\n",
    "Y_out = ( Y_out_0 + Y_out_1 ) / 2;\n",
    "\n",
    "print(Y_out)\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmax_num = 0;\\nmax_acc = 0;\\n#Create and train the random forest classifier\\nfor i in [20,30,40,50,60,70,80]:\\n#for i in [5]:\\n    rf_model = RandomForestClassifier(max_depth=i,n_estimators=2500);\\n    rf_model.fit(X_train,y_train);\\n    Y_pred = rf_model.predict(X_test);\\n    a = accuracy_score(y_test,Y_pred);\\n    print(i,a);\\n    if (a > max_acc):\\n        max_acc = a;\\n        max_num = i;\\n\\n#We choose XXXX RF since best accuracy\\n#print(np.mean(Y_pred))\\n#print(np.mean(y_test))\\n#We choose max_depth of 5 as our model\\nrf_model = RandomForestClassifier(max_depth=max_num,n_estimators=2500);\\nrf_model.fit(X_train,y_train);\\nY_pred = rf_model.predict(X_test);\\na = accuracy_score(y_test,Y_pred);\\nprint(i,a);\\n\\nY_out_0 = rf_model.predict_proba(test_datum)\\nY_out_1 = rf_model.predict_proba(test_datum_1);\\n\\nY_out = ( Y_out_0 + Y_out_1 ) / 2;\\nY_out = Y_out[:,1]\\n\\nprint(Y_out)\\nprint(np.mean(Y_out))\\n#'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "max_num = 0;\n",
    "max_acc = 0;\n",
    "#Create and train the random forest classifier\n",
    "for i in [20,30,40,50,60,70,80]:\n",
    "#for i in [5]:\n",
    "    rf_model = RandomForestClassifier(max_depth=i,n_estimators=2500);\n",
    "    rf_model.fit(X_train,y_train);\n",
    "    Y_pred = rf_model.predict(X_test);\n",
    "    a = accuracy_score(y_test,Y_pred);\n",
    "    print(i,a);\n",
    "    if (a > max_acc):\n",
    "        max_acc = a;\n",
    "        max_num = i;\n",
    "\n",
    "#We choose XXXX RF since best accuracy\n",
    "#print(np.mean(Y_pred))\n",
    "#print(np.mean(y_test))\n",
    "#We choose max_depth of 5 as our model\n",
    "rf_model = RandomForestClassifier(max_depth=max_num,n_estimators=2500);\n",
    "rf_model.fit(X_train,y_train);\n",
    "Y_pred = rf_model.predict(X_test);\n",
    "a = accuracy_score(y_test,Y_pred);\n",
    "print(i,a);\n",
    "\n",
    "Y_out_0 = rf_model.predict_proba(test_datum)\n",
    "Y_out_1 = rf_model.predict_proba(test_datum_1);\n",
    "\n",
    "Y_out = ( Y_out_0 + Y_out_1 ) / 2;\n",
    "Y_out = Y_out[:,1]\n",
    "\n",
    "print(Y_out)\n",
    "print(np.mean(Y_out))\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#Create an SVM\\n\\n#Create a simple one\\nlin_svc = svm.SVC(kernel='linear');\\nlin_svc.fit(X_train,y_train);\\nY_lin = lin_svc.predict(X_test);\\nprint(accuracy_score(Y_lin,y_test))\\n\\n#RBF Kernel\\nrbf_svc = svm.SVC(kernel='rbf');\\nrbf_svc.fit(X_train,y_train);\\nY_rbf = rbf_svc.predict(X_test);\\nprint(accuracy_score(Y_rbf,y_test))\\n\\nY_lin = lin_svc.predict(test_datum);\\nY_rbf = rbf_svc.predict(test_datum);\\nY_out = Y_lin.copy();\\n#\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#Create an SVM\n",
    "\n",
    "#Create a simple one\n",
    "lin_svc = svm.SVC(kernel='linear');\n",
    "lin_svc.fit(X_train,y_train);\n",
    "Y_lin = lin_svc.predict(X_test);\n",
    "print(accuracy_score(Y_lin,y_test))\n",
    "\n",
    "#RBF Kernel\n",
    "rbf_svc = svm.SVC(kernel='rbf');\n",
    "rbf_svc.fit(X_train,y_train);\n",
    "Y_rbf = rbf_svc.predict(X_test);\n",
    "print(accuracy_score(Y_rbf,y_test))\n",
    "\n",
    "Y_lin = lin_svc.predict(test_datum);\n",
    "Y_rbf = rbf_svc.predict(test_datum);\n",
    "Y_out = Y_lin.copy();\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "# here is a simple algorithm to get you started...for each possible edge i-j in the test set\n",
    "# we will count the number of nodes in the training graph they have in common...if they have\n",
    "# one or more \"mutual friends\" then we will connect them (this will score about .67 acc on kaggle)\n",
    "\n",
    "\n",
    "cutoff = .45;\n",
    "\n",
    "alfa = 0;\n",
    "with open('/kaggle/working/sub.csv', 'w') as csvfile:\n",
    "    fieldnames = ['edge', 'label']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    with open(sample_sub) as csvfile2:\n",
    "        reader = csv.reader(csvfile2, delimiter=',')\n",
    "        for row in reader:\n",
    "            if row[1]=='1' or row[1]=='0':\n",
    "                edge= row[0].split('-')\n",
    "\n",
    "                i=int(edge[0])\n",
    "                j=int(edge[1])\n",
    "                \n",
    "                # here networkx has a common_neighbors function\n",
    "                num_com_neigh=len(sorted(nx.common_neighbors(G, i, j)))\n",
    "                y=0\n",
    "                #if num_com_neigh>0:\n",
    "                #    y=1\n",
    "\n",
    "                edg_out=str(i)+\"-\"+str(j) \n",
    "                if (Y_out[alfa] > cutoff):\n",
    "                    y = 1\n",
    "                alfa += 1;\n",
    "                writer.writerow({'edge': edg_out, 'label': y})\n",
    "                #print(edg_out, num_com_neigh, y)\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.mean(Y_out > .45))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

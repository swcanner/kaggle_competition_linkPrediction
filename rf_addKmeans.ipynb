{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/iupui-link-prediction/features.csv\n",
      "/kaggle/input/iupui-link-prediction/sample_submission.csv\n",
      "/kaggle/input/iupui-link-prediction/train_edges.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, LeakyReLU, ReLU\n",
    "from keras.utils import np_utils,  to_categorical\n",
    "from keras import optimizers\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"/kaggle/input/iupui-link-prediction/train_edges.csv\";\n",
    "sample_sub = \"/kaggle/input/iupui-link-prediction/sample_submission.csv\"\n",
    "feat_file = \"/kaggle/input/iupui-link-prediction/features.csv\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_index(G):\n",
    "    \n",
    "    s = list(G.nodes);\n",
    "    num_nodes = max(s)+1;\n",
    "    \n",
    "    #Store it into a matrix for self preservation\n",
    "    #num_nodes = G.number_of_nodes();\n",
    "    common_neigh = np.zeros((num_nodes,num_nodes));\n",
    "    salton = common_neigh.copy();\n",
    "    jaccard = common_neigh.copy();\n",
    "    sorensen = common_neigh.copy();\n",
    "    hub_promoted = common_neigh.copy();\n",
    "    hub_depressed = common_neigh.copy();\n",
    "    leicht = common_neigh.copy();\n",
    "    pref = common_neigh.copy();\n",
    "    adamic = common_neigh.copy();\n",
    "    resource = common_neigh.copy();\n",
    "    \n",
    "    for k in s:\n",
    "        i = s.index(k);\n",
    "        if (i % 100 == 0):\n",
    "            print(i)\n",
    "        for l in s:\n",
    "            j = s.index(l);\n",
    "            if (i != j):\n",
    "                deg_i = G.degree[i];\n",
    "                deg_j = G.degree[j];\n",
    "                min_deg = deg_i;\n",
    "                max_deg = deg_j;\n",
    "                if (deg_j < min_deg):\n",
    "                    min_deg = deg_j;\n",
    "                    max_deg = deg_i;\n",
    "\n",
    "                com = sorted(nx.common_neighbors(G,i,j));\n",
    "                num_com = len(com);\n",
    "                common_neigh[i,j] = num_com;\n",
    "                preff = deg_i * deg_j;\n",
    "                \n",
    "                ada = 0;\n",
    "                res = 0;\n",
    "                \n",
    "                for k in com:\n",
    "                    ada += 1 / math.log(G.degree[k]);\n",
    "                    res += 1 / (G.degree[k]);\n",
    "                    \n",
    "                pref[i,j] = preff;\n",
    "                adamic[i,j] = ada;\n",
    "                resource[i,j] = res;\n",
    "\n",
    "                if (num_com > 0):\n",
    "                    salt = num_com / math.sqrt(deg_i * deg_j);\n",
    "                    jac = num_com / (deg_i + deg_j - num_com);\n",
    "                    sor = 2 * num_com / (deg_i + deg_j);\n",
    "                    hub_d = num_com / min_deg;\n",
    "                    hub_p = num_com / max_deg;\n",
    "                    lec = num_com / (deg_i * deg_j);\n",
    "\n",
    "                    salton[i,j] = salt;\n",
    "                    jaccard[i,j] = jac;\n",
    "                    sorensen[i,j] = sor;\n",
    "                    hub_promoted[i,j] = hub_p;\n",
    "                    hub_depressed[i,j] = hub_d;\n",
    "                    leicht[i,j] = lec;\n",
    "                \n",
    "    \n",
    "    return(common_neigh,salton,jaccard,sorensen,hub_promoted,hub_depressed,leicht,pref,adamic,resource);\n",
    "\n",
    "def calc_common_neigh(G):\n",
    "    #Store it into a matrix for self preservation\n",
    "    num_nodes = G.number_of_nodes();\n",
    "    common_neigh = np.zeros((num_nodes,num_nodes));\n",
    "    \n",
    "    for i in range(num_nodes):\n",
    "        if (i % 100 == 0):\n",
    "            print(i)\n",
    "        for j in range(num_nodes):\n",
    "            if (i != j):\n",
    "                num_com = len(sorted(nx.common_neighbors(G,i,j)));\n",
    "                common_neigh[i,j] = num_com\n",
    "    \n",
    "    return(common_neigh);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#Load the input files\n",
    "import csv\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "with open(train_file) as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in spamreader:\n",
    "        if row[1]=='1':\n",
    "            edge= row[0].split('-')\n",
    "            #print(int(edge[0]),int(edge[1]))\n",
    "            G.add_edge(int(edge[0]),int(edge[1]))\n",
    "            \n",
    "#s = sorted(G.nodes);\n",
    "s = G.nodes\n",
    "s = list(s)\n",
    "\n",
    "adj = nx.adjacency_matrix(G);\n",
    "print(adj[146,1356])\n",
    "print(adj[s.index(146),s.index(1356)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(s)\n",
    "#print(sorted(s))\n",
    "#print(type(s))\n",
    "#print(list(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2506\n"
     ]
    }
   ],
   "source": [
    "#Node info\n",
    "print(G.number_of_nodes());\n",
    "#nx.draw(G,node_size = 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2708\n"
     ]
    }
   ],
   "source": [
    "# next let's read in the possible edges we need to classify from the test file\n",
    "#FUCK THE WAY MOHLER DID THIS\n",
    "#It assumes all nodes are connected so fuck\n",
    "#But we will let it stand...\n",
    "\n",
    "Gsub = nx.Graph()\n",
    "\n",
    "with open(sample_sub) as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in spamreader:\n",
    "        if row[1]=='1' or row[1]=='0':\n",
    "            edge= row[0].split('-')\n",
    "            #print(edge[0],edge[1]);\n",
    "            Gsub.add_edge(int(edge[0]),int(edge[1]))\n",
    "            \n",
    "G.add_nodes_from(Gsub.nodes)\n",
    "print(G.number_of_nodes());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sorted(G.nodes))\n",
    "s = list(G.nodes);\n",
    "#print(s)\n",
    "#a = 0;\n",
    "#for i in range(len(s)-1):\n",
    "#    if(s[i+1] - s[i] != 1):\n",
    "#        a += 1;\n",
    "#print(a)\n",
    "#print(G.degree[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n"
     ]
    }
   ],
   "source": [
    "#n = calc_common_neigh(G)\n",
    "com,salt,jac,sor,hub_p,hub_d,lec,pref,ada,res = calc_index(G);\n",
    "\n",
    "\n",
    "#print(com,salt,jac,sor,hub_p,hub_d);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select only the training data\n",
    "train_df = pd.read_csv(train_file);\n",
    "train_df = train_df.values;\n",
    "sz = np.shape(train_df);\n",
    "train_data = np.zeros((sz[0],3));\n",
    "\n",
    "for i in range(sz[0]):\n",
    "    edge = train_df[i,0];\n",
    "    edge = edge.split('-');\n",
    "    train_data[i,0] = int(edge[0])\n",
    "    train_data[i,1] = int(edge[1]);\n",
    "    train_data[i,2] = int(train_df[i,1]);\n",
    "    \n",
    "#print(train_data)\n",
    "\n",
    "#Create a matrix of the data\n",
    "data_exist = np.zeros((len(s),len(s)));\n",
    "for i in range(sz[0]):\n",
    "    \n",
    "    j = int(train_data[i,0]);\n",
    "    k = int(train_data[i,1]);\n",
    "    j = s.index(j);\n",
    "    k = s.index(k);\n",
    "    \n",
    "    data_exist[j,k] = 1;\n",
    "    data_exist[k,j] = 1;\n",
    "\n",
    "def datum_in(i,j,df):\n",
    "    sz = np.shape(df);\n",
    "    for k in range(sz[0]):\n",
    "        if (df[k,0] == i):\n",
    "            if (df[k,1] == j):\n",
    "                return True;\n",
    "        if (df[k,0] == j):\n",
    "            if (df[k,1] == i):\n",
    "                return True;\n",
    "        \n",
    "    return False;\n",
    "\n",
    "def datum_in_mat(i,j,s,dat):\n",
    "    \n",
    "    if (data_exist[i,j] != 0):\n",
    "        return True;\n",
    "    else:\n",
    "        return False;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 462    0    0 ...    0    0    0]\n",
      " [1911    0    0 ...    0    0    0]\n",
      " [2002    0    0 ...    0    0    0]\n",
      " ...\n",
      " [2372    0    0 ...    0    0    0]\n",
      " [ 955    0    0 ...    0    0    0]\n",
      " [ 376    0    0 ...    0    0    0]]\n",
      "(2708, 1434)\n",
      "17\n",
      "482\n",
      "22\n",
      "1928\n"
     ]
    }
   ],
   "source": [
    "#Load the feature file\n",
    "feat = pd.read_csv(feat_file,header=None);\n",
    "feat = feat.values;\n",
    "print(feat)\n",
    "print(feat.shape)\n",
    "\n",
    "#feat = feat.sort(axis=0);\n",
    "#print(feat)\n",
    "\n",
    "def feat_find(x,feat):\n",
    "    y = feat[feat[:,0]==x][0][1:1433];\n",
    "    return y;\n",
    "\n",
    "y = feat_find(1911,feat)\n",
    "print(np.sum(y))\n",
    "print(np.sum(feat[0,:]))\n",
    "y = feat_find(2002,feat)\n",
    "print(np.sum(y))\n",
    "print(np.sum(feat[1,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01173877 0.00191393]\n",
      " [0.00191393 0.01513767]]\n",
      "[[79.92466636 -9.47905051]\n",
      " [-9.47905051 63.09101342]]\n",
      "[79.92466636 -9.47905051 -9.47905051 63.09101342]\n"
     ]
    }
   ],
   "source": [
    "#Covariance of the features\n",
    "uh_cov = np.cov(feat_find(1911,feat),feat_find(2002,feat));\n",
    "print(uh_cov)\n",
    "new_cov = uh_cov.copy();\n",
    "\n",
    "eps = .001;\n",
    "for i in range(np.shape(new_cov)[0]):\n",
    "    new_cov[i,i] += eps\n",
    "#print(new_cov)\n",
    "new_cov = np.linalg.inv(new_cov);\n",
    "print(new_cov)\n",
    "\n",
    "print(new_cov.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run K-means on the raw data\n",
    "km_k = 25;\n",
    "feat_data = feat[:,1:1433];\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "3\n",
      "2.23606797749979\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(3)\n",
    "print(a)\n",
    "print(np.sum(a))\n",
    "print(np.linalg.norm(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "(7388, 24)\n",
      "[[ 2.00000000e+00  1.01665714e-01  1.53846154e-02 ... -1.76210697e+00\n",
      "   5.79648082e+01  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ... -5.61475702e+00\n",
      "   6.91380260e+01  1.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ... -2.28379362e+00\n",
      "   6.87579030e+01  1.00000000e+00]\n",
      " ...\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "#Now I need to make this into something that I can feed into a neural net like nothing\n",
    "num_nodes = max(s)+1;\n",
    "adj = nx.adjacency_matrix(G);\n",
    "adj = adj.todense();\n",
    "\n",
    "sz_train = np.shape(train_df);\n",
    "datum = np.zeros((sz_train[0],24));\n",
    "ind = 0;\n",
    "\n",
    "eps = .0001;\n",
    "\n",
    "for k in s:\n",
    "    i = s.index(k);\n",
    "    if (i % 100 == 0):\n",
    "            print(i)\n",
    "    for l in s:\n",
    "        j = s.index(l);\n",
    "        if (j > i):\n",
    "            if (datum_in_mat(i,j,s,data_exist)):\n",
    "\n",
    "                datum[ind,0] = com[i,j];\n",
    "                datum[ind,1] = salt[i,j];\n",
    "                datum[ind,2] = jac[i,j];\n",
    "                datum[ind,3] = sor[i,j];\n",
    "                datum[ind,4] = hub_p[i,j];\n",
    "                datum[ind,5] = hub_d[i,j];\n",
    "                datum[ind,6] = lec[i,j];\n",
    "                datum[ind,7] = pref[i,j];\n",
    "                datum[ind,8] = res[i,j];\n",
    "                datum[ind,9] = ada[i,j];\n",
    "\n",
    "                deg_i = G.degree[i];\n",
    "                deg_j = G.degree[j];\n",
    "                min_deg = deg_i;\n",
    "                max_deg = deg_j;\n",
    "                if (min_deg > max_deg):\n",
    "                    max_deg = deg_j;\n",
    "                    min_deg = deg_i;\n",
    "                datum[ind,10] = min_deg;\n",
    "                datum[ind,11] = max_deg;\n",
    "                \n",
    "                #print(ind,k,l)\n",
    "                y1 = feat_find(k,feat);\n",
    "                y2 = feat_find(l,feat);\n",
    "                \n",
    "                dp = np.dot(y1,y2);\n",
    "                n1 = np.linalg.norm(y1);\n",
    "                n2 = np.linalg.norm(y2);\n",
    "                dp_norm = dp / (n1*n2);\n",
    "                \n",
    "                c_ang = math.acos(dp_norm);\n",
    "                \n",
    "                datum[ind,12] = dp;\n",
    "                datum[ind,13] = dp_norm;\n",
    "                datum[ind,14] = c_ang;\n",
    "\n",
    "                #Covariance stuff\n",
    "                #Use the cov and inv cov\n",
    "                \n",
    "                #Covariance of the features\n",
    "                c_cov = np.cov(y1,y2);\n",
    "                new_cov = c_cov.copy();\n",
    "\n",
    "                for i in range(np.shape(new_cov)[0]):\n",
    "                    new_cov[i,i] += eps\n",
    "\n",
    "                new_cov = np.linalg.inv(new_cov);\n",
    "                \n",
    "                c_cov = c_cov.flatten();\n",
    "                new_cov = new_cov.flatten();\n",
    "                                \n",
    "                datum[ind,15:19] = c_cov;\n",
    "                datum[ind,19:23] = new_cov;\n",
    "                \n",
    "                #ind_i = s.index(i);\n",
    "                #ind_j = s.index(j);\n",
    "\n",
    "                datum[ind,-1] = adj[i,j];\n",
    "                ind += 1;\n",
    "        \n",
    "#remove the all zero rows\n",
    "#datum = datum[~np.all(datum == 0, axis=1)];\n",
    "#Append an all zero row\n",
    "#oof = np.zeros((1,13));\n",
    "#datum = np.append(datum,oof);\n",
    "#datum = datum.reshape(-1,13);\n",
    "print(np.shape(datum))\n",
    "print(datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  9.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  3.00000000e+00  3.00000000e+00\n",
      "  5.00000000e+00  2.18217890e-01  1.35080835e+00  1.71653022e-02\n",
      "  3.23786156e-03  3.23786156e-03  1.44598456e-02  6.04402821e+01\n",
      " -1.34408888e+01 -1.34408888e+01  7.16710715e+01  1.00000000e+00]\n",
      "[0.0171653  0.00323786 0.00323786 0.01445985]\n",
      "0.01716530222643852\n",
      "[ 60.4402821  -13.44088878 -13.44088878  71.67107146]\n",
      "60.44028209876607\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(datum[4,:])\n",
    "print(datum[4,15:19])\n",
    "print(datum[4,15])\n",
    "print(datum[4,19:23])\n",
    "print(datum[4,19])\n",
    "print(datum[4,23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#print(adj)\n",
    "\n",
    "\n",
    "print(adj[2250,985])\n",
    "print(adj[s.index(146),s.index(1356)])\n",
    "print(adj[s.index(1356),s.index(146)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.           0.           0.         ...  -1.15973927  -1.15973927\n",
      "   60.30174057]\n",
      " [  0.           0.           0.         ... -24.56131147 -24.56131147\n",
      "  241.61113215]\n",
      " [  0.           0.           0.         ...  -5.08164811  -5.08164811\n",
      "   72.46660237]\n",
      " ...\n",
      " [  0.           0.           0.         ...   0.           0.\n",
      "    0.        ]\n",
      " [  0.           0.           0.         ...   0.           0.\n",
      "    0.        ]\n",
      " [  0.           0.           0.         ...   0.           0.\n",
      "    0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Get test data\n",
    "test_df = pd.read_csv(sample_sub);\n",
    "test_df = test_df.values;\n",
    "test_sz = np.shape(test_df);\n",
    "test_inp = np.zeros((test_sz[0],3));\n",
    "\n",
    "for i in range(test_sz[0]):\n",
    "    edge = test_df[i,0];\n",
    "    edge = edge.split('-');\n",
    "    test_inp[i,0] = int(edge[0])\n",
    "    test_inp[i,1] = int(edge[1]);\n",
    "    test_inp[i,2] = int(test_inp[i,1]);\n",
    "    \n",
    "#print(train_data)\n",
    "\n",
    "num_nodes = max(s)+1;\n",
    "\n",
    "sz_test = np.shape(train_df);\n",
    "test_datum = np.zeros((sz_test[0],23));\n",
    "ind = 0;\n",
    "\n",
    "for ind in range(test_sz[0]):\n",
    "\n",
    "    i = int(test_inp[ind,0]);\n",
    "    j = int(test_inp[ind,1]);\n",
    "    k = i;\n",
    "    l = j;\n",
    "    i = s.index(i);\n",
    "    j = s.index(j);\n",
    "    \n",
    "    test_datum[ind,0] = com[i,j];\n",
    "    test_datum[ind,1] = salt[i,j];\n",
    "    test_datum[ind,2] = jac[i,j];\n",
    "    test_datum[ind,3] = sor[i,j];\n",
    "    test_datum[ind,4] = hub_p[i,j];\n",
    "    test_datum[ind,5] = hub_d[i,j];\n",
    "    test_datum[ind,6] = lec[i,j];\n",
    "    test_datum[ind,7] = pref[i,j];\n",
    "    test_datum[ind,8] = res[i,j];\n",
    "    test_datum[ind,9] = ada[i,j];\n",
    "\n",
    "    deg_i = G.degree[i];\n",
    "    deg_j = G.degree[j];\n",
    "    min_deg = deg_i;\n",
    "    max_deg = deg_j;\n",
    "    if (min_deg > max_deg):\n",
    "        max_deg = deg_j;\n",
    "        min_deg = deg_i;\n",
    "    \n",
    "    test_datum[ind,10] = min_deg;\n",
    "    test_datum[ind,11] = max_deg;\n",
    "    \n",
    "    y1 = feat_find(k,feat);\n",
    "    y2 = feat_find(l,feat);\n",
    "\n",
    "    dp = np.dot(y1,y2);\n",
    "    n1 = np.linalg.norm(y1);\n",
    "    n2 = np.linalg.norm(y2);\n",
    "    dp_norm = dp / (n1*n2);\n",
    "\n",
    "    c_ang = math.acos(dp_norm);\n",
    "\n",
    "    test_datum[ind,12] = dp;\n",
    "    test_datum[ind,13] = dp_norm;\n",
    "    test_datum[ind,14] = c_ang;\n",
    "    \n",
    "    #Covariance of the features\n",
    "    c_cov = np.cov(y1,y2);\n",
    "    new_cov = c_cov.copy();\n",
    "\n",
    "    for i in range(np.shape(new_cov)[0]):\n",
    "        new_cov[i,i] += eps\n",
    "\n",
    "    new_cov = np.linalg.inv(new_cov);\n",
    "\n",
    "    c_cov = c_cov.flatten();\n",
    "    new_cov = new_cov.flatten();\n",
    "\n",
    "    test_datum[ind,15:19] = c_cov;\n",
    "    test_datum[ind,19:23] = new_cov;\n",
    "\n",
    "\n",
    "print(test_datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(datum);\n",
    "train_df.to_csv(\"train.csv\",header=None,index=None)\n",
    "test_df = pd.DataFrame(test_datum);\n",
    "test_df.to_csv(\"test.csv\",header=None,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization\n",
    "max_col = np.amax(datum,axis=0);\n",
    "datum /= max_col;\n",
    "test_datum /= max_col[:-1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I now have the training and testing data\n",
    "#Separate the training data into train and test\n",
    "Y = datum[:,-1];\n",
    "X = datum[:,:-1];\n",
    "\n",
    "X = X.reshape(-1,23,1);\n",
    "#test_datum = test_datum.reshape(-1,12,1);\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size = .25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 23, 512)           1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 23, 512)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 23, 512)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 11776)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               3014912   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 3,076,769\n",
      "Trainable params: 3,076,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#\"\"\"\n",
    "# create convolution neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(23,1)))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.33))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.33))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.33))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.33))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.33))\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.33))\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary();\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "#Compile model\n",
    "sgd = optimizers.SGD(lr=.01);\n",
    "model.compile(loss='MSE', \n",
    "              optimizer='adagrad', \n",
    "              metrics=['accuracy'])\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5541 samples, validate on 1847 samples\n",
      "Epoch 1/250\n",
      "5541/5541 [==============================] - 20s 4ms/step - loss: 0.1308 - accuracy: 0.7838 - val_loss: 0.1188 - val_accuracy: 0.7834\n",
      "Epoch 2/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.1118 - accuracy: 0.8208 - val_loss: 0.1057 - val_accuracy: 0.8295\n",
      "Epoch 3/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.1022 - accuracy: 0.8415 - val_loss: 0.1029 - val_accuracy: 0.8246\n",
      "Epoch 4/250\n",
      "5541/5541 [==============================] - 18s 3ms/step - loss: 0.1000 - accuracy: 0.8464 - val_loss: 0.0984 - val_accuracy: 0.8452\n",
      "Epoch 5/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.0967 - accuracy: 0.8524 - val_loss: 0.0990 - val_accuracy: 0.8435\n",
      "Epoch 6/250\n",
      "5541/5541 [==============================] - 20s 4ms/step - loss: 0.0935 - accuracy: 0.8594 - val_loss: 0.0922 - val_accuracy: 0.8592\n",
      "Epoch 7/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.0909 - accuracy: 0.8670 - val_loss: 0.0872 - val_accuracy: 0.8755\n",
      "Epoch 8/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.0900 - accuracy: 0.8697 - val_loss: 0.0861 - val_accuracy: 0.8733\n",
      "Epoch 9/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.0895 - accuracy: 0.8729 - val_loss: 0.0821 - val_accuracy: 0.8852\n",
      "Epoch 10/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.0876 - accuracy: 0.8731 - val_loss: 0.0798 - val_accuracy: 0.8906\n",
      "Epoch 11/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.0879 - accuracy: 0.8729 - val_loss: 0.0850 - val_accuracy: 0.8766\n",
      "Epoch 12/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.0855 - accuracy: 0.8738 - val_loss: 0.0875 - val_accuracy: 0.8706\n",
      "Epoch 13/250\n",
      "5541/5541 [==============================] - 21s 4ms/step - loss: 0.0847 - accuracy: 0.8798 - val_loss: 0.0879 - val_accuracy: 0.8728\n",
      "Epoch 14/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.0865 - accuracy: 0.8744 - val_loss: 0.0784 - val_accuracy: 0.8933\n",
      "Epoch 15/250\n",
      "5541/5541 [==============================] - 19s 4ms/step - loss: 0.0853 - accuracy: 0.8764 - val_loss: 0.0758 - val_accuracy: 0.8998\n",
      "Epoch 16/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.0827 - accuracy: 0.8820 - val_loss: 0.0940 - val_accuracy: 0.8484\n",
      "Epoch 17/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.0820 - accuracy: 0.8814 - val_loss: 0.0826 - val_accuracy: 0.8803\n",
      "Epoch 18/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.0813 - accuracy: 0.8856 - val_loss: 0.0804 - val_accuracy: 0.8847\n",
      "Epoch 20/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.0812 - accuracy: 0.8849 - val_loss: 0.0789 - val_accuracy: 0.8885\n",
      "Epoch 21/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.0809 - accuracy: 0.8849 - val_loss: 0.0829 - val_accuracy: 0.8766\n",
      "Epoch 22/250\n",
      "5541/5541 [==============================] - 20s 4ms/step - loss: 0.0796 - accuracy: 0.8845 - val_loss: 0.0812 - val_accuracy: 0.8836\n",
      "Epoch 23/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.0788 - accuracy: 0.8881 - val_loss: 0.0803 - val_accuracy: 0.8852\n",
      "Epoch 24/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.0805 - accuracy: 0.8868 - val_loss: 0.0780 - val_accuracy: 0.8863\n",
      "Epoch 25/250\n",
      "5541/5541 [==============================] - 20s 4ms/step - loss: 0.0775 - accuracy: 0.8906 - val_loss: 0.0776 - val_accuracy: 0.8917\n",
      "Epoch 26/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.0802 - accuracy: 0.8852 - val_loss: 0.0797 - val_accuracy: 0.8863\n",
      "Epoch 27/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.0786 - accuracy: 0.8901 - val_loss: 0.0791 - val_accuracy: 0.8858\n",
      "Epoch 28/250\n",
      "5541/5541 [==============================] - 20s 4ms/step - loss: 0.0781 - accuracy: 0.8877 - val_loss: 0.0772 - val_accuracy: 0.8868\n",
      "Epoch 29/250\n",
      "5541/5541 [==============================] - 20s 4ms/step - loss: 0.0789 - accuracy: 0.8852 - val_loss: 0.0790 - val_accuracy: 0.8858\n",
      "Epoch 30/250\n",
      "5541/5541 [==============================] - 39s 7ms/step - loss: 0.0778 - accuracy: 0.8906 - val_loss: 0.0730 - val_accuracy: 0.8988\n",
      "Epoch 31/250\n",
      "5541/5541 [==============================] - 20s 4ms/step - loss: 0.0774 - accuracy: 0.8905 - val_loss: 0.0771 - val_accuracy: 0.8906\n",
      "Epoch 32/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.0778 - accuracy: 0.8912 - val_loss: 0.1664 - val_accuracy: 0.7910\n",
      "Epoch 33/250\n",
      "5541/5541 [==============================] - 20s 4ms/step - loss: 0.0786 - accuracy: 0.8923 - val_loss: 0.0758 - val_accuracy: 0.8928\n",
      "Epoch 34/250\n",
      "5541/5541 [==============================] - 19s 4ms/step - loss: 0.0771 - accuracy: 0.8928 - val_loss: 0.0758 - val_accuracy: 0.8939\n",
      "Epoch 35/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.0771 - accuracy: 0.8919 - val_loss: 0.0737 - val_accuracy: 0.8982\n",
      "Epoch 36/250\n",
      "5541/5541 [==============================] - 19s 4ms/step - loss: 0.0770 - accuracy: 0.8923 - val_loss: 0.0762 - val_accuracy: 0.8917\n",
      "Epoch 37/250\n",
      "5541/5541 [==============================] - 20s 4ms/step - loss: 0.0768 - accuracy: 0.8932 - val_loss: 0.0775 - val_accuracy: 0.8874\n",
      "Epoch 38/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.0765 - accuracy: 0.8917 - val_loss: 0.0770 - val_accuracy: 0.8906\n",
      "Epoch 39/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.0760 - accuracy: 0.8948 - val_loss: 0.0731 - val_accuracy: 0.9025\n",
      "Epoch 40/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.0755 - accuracy: 0.8914 - val_loss: 0.0728 - val_accuracy: 0.9020\n",
      "Epoch 41/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.0764 - accuracy: 0.8915 - val_loss: 0.0777 - val_accuracy: 0.8901\n",
      "Epoch 42/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.0765 - accuracy: 0.8901 - val_loss: 0.0726 - val_accuracy: 0.8993\n",
      "Epoch 43/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.0745 - accuracy: 0.8944 - val_loss: 0.0700 - val_accuracy: 0.9063\n",
      "Epoch 44/250\n",
      "5541/5541 [==============================] - 18s 3ms/step - loss: 0.0664 - accuracy: 0.9076 - val_loss: 0.0611 - val_accuracy: 0.9161\n",
      "Epoch 123/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.0666 - accuracy: 0.9076 - val_loss: 0.0626 - val_accuracy: 0.9155\n",
      "Epoch 124/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.0595 - accuracy: 0.9172 - val_loss: 0.0538 - val_accuracy: 0.9285\n",
      "Epoch 150/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.0620 - accuracy: 0.9159 - val_loss: 0.0564 - val_accuracy: 0.9264\n",
      "Epoch 151/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.1729 - accuracy: 0.8239 - val_loss: 0.1762 - val_accuracy: 0.8202\n",
      "Epoch 176/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.1726 - accuracy: 0.8240 - val_loss: 0.1711 - val_accuracy: 0.8273\n",
      "Epoch 177/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.1726 - accuracy: 0.8233 - val_loss: 0.1706 - val_accuracy: 0.8284\n",
      "Epoch 178/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.1732 - accuracy: 0.8235 - val_loss: 0.1727 - val_accuracy: 0.8267\n",
      "Epoch 179/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.1729 - accuracy: 0.8253 - val_loss: 0.1712 - val_accuracy: 0.8273\n",
      "Epoch 180/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.1715 - accuracy: 0.8275 - val_loss: 0.1702 - val_accuracy: 0.8284\n",
      "Epoch 181/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.1720 - accuracy: 0.8257 - val_loss: 0.1708 - val_accuracy: 0.8273\n",
      "Epoch 182/250\n",
      "5541/5541 [==============================] - 19s 3ms/step - loss: 0.1727 - accuracy: 0.8239 - val_loss: 0.1701 - val_accuracy: 0.8289\n",
      "Epoch 183/250\n",
      " 290/5541 [>.............................] - ETA: 16s - loss: 0.1522 - accuracy: 0.8448"
     ]
    }
   ],
   "source": [
    "#\"\"\"\n",
    "history = model.fit(X_train,y_train, shuffle=True,\n",
    "          batch_size=10,epochs=250,verbose=1,\n",
    "          validation_data=(X_test,y_test))\n",
    "#          \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_1_input to have 3 dimensions, but got array with shape (7388, 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-78614eebfc2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mY_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_datum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \u001b[0;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1441\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1442\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    133\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    136\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_1_input to have 3 dimensions, but got array with shape (7388, 23)"
     ]
    }
   ],
   "source": [
    "#\"\"\"\n",
    "Y_out = model.predict(test_datum)\n",
    "print(Y_out)\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Create and train the random forest classifier\\nfor i in [8,9,10,11,12,13]:\\n    rf_model = RandomForestClassifier(max_depth=i,n_estimators=2500);\\n    rf_model.fit(X_train,y_train);\\n    Y_pred = rf_model.predict(X_test);\\n    a = accuracy_score(y_test,Y_pred);\\n    print(i,a);\\n#'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#Create and train the random forest classifier\n",
    "for i in [8,9,10,11,12,13]:\n",
    "    rf_model = RandomForestClassifier(max_depth=i,n_estimators=2500);\n",
    "    rf_model.fit(X_train,y_train);\n",
    "    Y_pred = rf_model.predict(X_test);\n",
    "    a = accuracy_score(y_test,Y_pred);\n",
    "    print(i,a);\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#We choose XXXX RF since best accuracy\\n#print(np.mean(Y_pred))\\n#print(np.mean(y_test))\\n#We choose max_depth of 5 as our model\\nrf_model = RandomForestClassifier(max_depth=12,n_estimators=2500);\\nrf_model.fit(datum[:,:-1],datum[:,-1]);\\nY_pred = rf_model.predict(X_test);\\na = accuracy_score(y_test,Y_pred);\\nprint(i,a);\\n\\nY_out = rf_model.predict(test_datum)\\nprint(Y_out)\\nprint(np.mean(Y_out))\\n#'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#We choose XXXX RF since best accuracy\n",
    "#print(np.mean(Y_pred))\n",
    "#print(np.mean(y_test))\n",
    "#We choose max_depth of 5 as our model\n",
    "rf_model = RandomForestClassifier(max_depth=12,n_estimators=2500);\n",
    "rf_model.fit(datum[:,:-1],datum[:,-1]);\n",
    "Y_pred = rf_model.predict(X_test);\n",
    "a = accuracy_score(y_test,Y_pred);\n",
    "print(i,a);\n",
    "\n",
    "Y_out = rf_model.predict(test_datum)\n",
    "print(Y_out)\n",
    "print(np.mean(Y_out))\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-d59c7ef28882>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0medg_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mY_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malfa\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0malfa\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Y_out' is not defined"
     ]
    }
   ],
   "source": [
    "#\"\"\"\n",
    "# here is a simple algorithm to get you started...for each possible edge i-j in the test set\n",
    "# we will count the number of nodes in the training graph they have in common...if they have\n",
    "# one or more \"mutual friends\" then we will connect them (this will score about .67 acc on kaggle)\n",
    "\n",
    "alfa = 0;\n",
    "with open('/kaggle/working/submission.csv', 'w') as csvfile:\n",
    "    fieldnames = ['edge', 'label']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    with open(sample_sub) as csvfile2:\n",
    "        reader = csv.reader(csvfile2, delimiter=',')\n",
    "        for row in reader:\n",
    "            if row[1]=='1' or row[1]=='0':\n",
    "                edge= row[0].split('-')\n",
    "\n",
    "                i=int(edge[0])\n",
    "                j=int(edge[1])\n",
    "                \n",
    "                # here networkx has a common_neighbors function\n",
    "                num_com_neigh=len(sorted(nx.common_neighbors(G, i, j)))\n",
    "                y=0\n",
    "                #if num_com_neigh>0:\n",
    "                #    y=1\n",
    "\n",
    "                edg_out=str(i)+\"-\"+str(j) \n",
    "                if (Y_out[alfa] > .5):\n",
    "                    y = 1\n",
    "                alfa += 1;\n",
    "                writer.writerow({'edge': edg_out, 'label': y})\n",
    "                #print(edg_out, num_com_neigh, y)\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-df0c67f399d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0medg_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mY_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malfa\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0malfa\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Y_out' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "alfa = 0;\n",
    "with open('/kaggle/working/submission_alt.csv', 'w') as csvfile:\n",
    "    fieldnames = ['edge', 'label']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    with open(sample_sub) as csvfile2:\n",
    "        reader = csv.reader(csvfile2, delimiter=',')\n",
    "        for row in reader:\n",
    "            if row[1]=='1' or row[1]=='0':\n",
    "                edge= row[0].split('-')\n",
    "\n",
    "                i=int(edge[0])\n",
    "                j=int(edge[1])\n",
    "                \n",
    "                # here networkx has a common_neighbors function\n",
    "                num_com_neigh=len(sorted(nx.common_neighbors(G, i, j)))\n",
    "                y=0\n",
    "                #if num_com_neigh>0:\n",
    "                #    y=1\n",
    "\n",
    "                edg_out=str(i)+\"-\"+str(j) \n",
    "                if (Y_out[alfa] < .5):\n",
    "                    y = 1;\n",
    "                alfa += 1;\n",
    "                writer.writerow({'edge': edg_out, 'label': y})\n",
    "                #print(edg_out, num_com_neigh, y)\n",
    "\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
